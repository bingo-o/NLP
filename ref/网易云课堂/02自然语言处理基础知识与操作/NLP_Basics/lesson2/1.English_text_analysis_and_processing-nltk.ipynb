{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](../img/NLP_banner.jpg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 英文文本处理与[NLTK](https://www.nltk.org/)\n",
    "#### \\[稀牛学院 x 网易云课程\\]《AI工程师(自然语言处理方向)》课程资料 by [@寒小阳](https://blog.csdn.net/han_xiaoyang)\n",
    "\n",
    "[NLTK](https://www.nltk.org/)，全称Natural Language Toolkit，自然语言处理工具包，是NLP研究领域常用的一个Python库，由宾夕法尼亚大学的Steven Bird和Edward Loper在Python的基础上开发的一个模块，至今已有超过十万行的代码。这是一个开源项目，包含数据集、Python模块、教程等；NLTK是最常用的英文自然语言处理python基础库之一。\n",
    "\n",
    "![](../img/L2_NLTK.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.英文Tokenization(标记化/分词)\n",
    "##### \\[稀牛学院 x 网易云课程\\]《AI工程师(自然语言处理方向)》课程资料 by [@寒小阳](https://blog.csdn.net/han_xiaoyang)\n",
    ">文本是不能成段送入模型中进行分析的，我们通常会把文本切成有独立含义的字、词或者短语，这个过程叫做tokenization，这通常是大家解决自然语言处理问题的第一步。在NLTK中提供了2种不同方式的tokenization，sentence tokenization 和 word tokenization，前者把文本进行“断句”，后者对文本进行“分词”。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk import word_tokenize, sent_tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "corpus的数据类型为: <class 'str'>\n"
     ]
    }
   ],
   "source": [
    "# 读入数据\n",
    "corpus = open('./data/text.txt','r').read()\n",
    "# 查看类型\n",
    "print(\"corpus的数据类型为:\",type(corpus))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[\"A ``knowledge engineer'' interviews experts in a certain domain and tries to embody their knowledge in a computer program for carrying out some task.\",\n",
       " 'How well this works depends on whether the intellectual mechanisms required for the task are within the present state of AI.',\n",
       " 'When this turned out not to be so, there were many disappointing results.',\n",
       " 'One of the first expert systems was MYCIN in 1974, which diagnosed bacterial infections of the blood and suggested treatments.',\n",
       " 'It did better than medical students or practicing doctors, provided its limitations were observed.',\n",
       " 'Namely, its ontology included bacteria, symptoms, and treatments and did not include patients, doctors, hospitals, death, recovery, and events occurring in time.',\n",
       " 'Its interactions depended on a single patient being considered.',\n",
       " 'Since the experts consulted by the knowledge engineers knew about patients, doctors, death, recovery, etc., it is clear that the knowledge engineers forced what the experts told them into a predetermined framework.',\n",
       " 'In the present state of AI, this has to be true.',\n",
       " 'The usefulness of current expert systems depends on their users having common sense.']"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 断句\n",
    "sentences = sent_tokenize(corpus)\n",
    "sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['A',\n",
       " '``',\n",
       " 'knowledge',\n",
       " 'engineer',\n",
       " \"''\",\n",
       " 'interviews',\n",
       " 'experts',\n",
       " 'in',\n",
       " 'a',\n",
       " 'certain',\n",
       " 'domain',\n",
       " 'and',\n",
       " 'tries',\n",
       " 'to',\n",
       " 'embody',\n",
       " 'their',\n",
       " 'knowledge',\n",
       " 'in',\n",
       " 'a',\n",
       " 'computer']"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 分词\n",
    "words = word_tokenize(corpus)\n",
    "words[:20]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.停用词\n",
    "##### \\[稀牛学院 x 网易云课程\\]《AI工程师(自然语言处理方向)》课程资料 by [@寒小阳]\n",
    "> 在自然语言处理的很多任务中，我们处理的主体“文本”中有一些功能词经常出现，然而对于最后的任务目标并没有帮助，甚至会对统计方法带来一些干扰，我们把这类词叫做**停用词**，通常我们会用一个停用词表把它们过滤出来。比如英语当中的**定冠词/不定冠词**(a,an,the等)。\n",
    "\n",
    ">关于机器学习中停用词的产出与收集方法，大家可以参见知乎讨论[机器学习中如何收集停用词](https://www.zhihu.com/question/34939177)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 导入内置停用词\n",
    "from nltk.corpus import stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['i', 'me', 'my', 'myself', 'we', 'our', 'ours', 'ourselves', 'you', 'your']"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stop_words = stopwords.words('english')\n",
    "#看头10个\n",
    "stop_words[0:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['A',\n",
       " '``',\n",
       " 'knowledge',\n",
       " 'engineer',\n",
       " \"''\",\n",
       " 'interviews',\n",
       " 'experts',\n",
       " 'certain',\n",
       " 'domain',\n",
       " 'tries',\n",
       " 'embody',\n",
       " 'knowledge',\n",
       " 'computer',\n",
       " 'program',\n",
       " 'carrying',\n",
       " 'task',\n",
       " '.',\n",
       " 'How',\n",
       " 'well',\n",
       " 'works']"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 使用列表推导式去掉停用词\n",
    "filtered_corpus = [w for w in words if not w in stop_words]\n",
    "filtered_corpus[:20]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "我们总共剔除的停用词数量为： 72\n"
     ]
    }
   ],
   "source": [
    "# 查看停用词数量\n",
    "print(\"我们总共剔除的停用词数量为：\", len(words)-len(filtered_corpus))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 词性标注\n",
    "##### \\[稀牛学院 x 网易云课程\\]《AI工程师(自然语言处理方向)》课程资料 by [@寒小阳]\n",
    "\n",
    ">词性（part-of-speech）是词汇基本的语法属性，通常也称为词性。\n",
    "\n",
    ">词性标注（part-of-speech tagging）,又称为词类标注或者简称标注，是指为分词结果中的每个单词标注一个正确的词性的程序，也即确定每个词是名词、动词、形容词或者其他词性的过程。\n",
    "\n",
    ">词性标注是很多NLP任务的预处理步骤，如句法分析，经过词性标注后的文本会带来很大的便利性，但也不是不可或缺的步骤。\n",
    ">词性标注的最简单做法是选取最高频词性，主流的做法可以分为基于规则和基于统计的方法，包括：\n",
    "* 基于最大熵的词性标注\n",
    "* 基于统计最大概率输出词性\n",
    "* 基于HMM的词性标注"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('A', 'DT'),\n",
       " ('``', '``'),\n",
       " ('knowledge', 'NN'),\n",
       " ('engineer', 'NN'),\n",
       " (\"''\", \"''\"),\n",
       " ('interviews', 'NNS'),\n",
       " ('experts', 'NNS'),\n",
       " ('certain', 'JJ'),\n",
       " ('domain', 'NN'),\n",
       " ('tries', 'NNS'),\n",
       " ('embody', 'VBP'),\n",
       " ('knowledge', 'JJ'),\n",
       " ('computer', 'NN'),\n",
       " ('program', 'NN'),\n",
       " ('carrying', 'NN'),\n",
       " ('task', 'NN'),\n",
       " ('.', '.'),\n",
       " ('How', 'WRB'),\n",
       " ('well', 'RB'),\n",
       " ('works', 'VBZ')]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 词性标注\n",
    "from nltk import pos_tag\n",
    "tags = pos_tag(filtered_corpus)\n",
    "tags[:20]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "具体的词性标注编码和含义见如下对应表：\n",
    "\n",
    "| POS Tag | Description | Example |\n",
    "| --- | --- | --- |\n",
    "| CC | coordinating conjunction | and |\n",
    "| CD | cardinal number | 1, third |\n",
    "| DT | determiner | the |\n",
    "| EX | existential there | there, is |\n",
    "| FW | foreign word | d’hoevre |\n",
    "| IN | preposition or subordinating conjunction | in, of, like |\n",
    "| JJ | adjective | big |\n",
    "| JJR | adjective, comparative | bigger |\n",
    "| JJS | adjective, superlative | biggest |\n",
    "| LS | list marker | 1) |\n",
    "| MD | modal | could, will |\n",
    "| NN | noun, singular or mass | door |\n",
    "| NNS | noun plural | doors |\n",
    "| NNP | proper noun, singular | John |\n",
    "| NNPS | proper noun, plural | Vikings |\n",
    "| PDT | predeterminer | both the boys |\n",
    "| POS | possessive ending | friend‘s |\n",
    "| PRP | personal pronoun | I, he, it |\n",
    "| PRP$ | possessive pronoun | my, his |\n",
    "| RB | adverb | however, usually, naturally, here, good |\n",
    "| RBR | adverb, comparative | better |\n",
    "| RBS | adverb, superlative | best |\n",
    "| RP | particle | give up |\n",
    "| TO | to | to go, to him |\n",
    "| UH | interjection | uhhuhhuhh |\n",
    "| VB | verb, base form | take |\n",
    "| VBD | verb, past tense | took |\n",
    "| VBG | verb, gerund or present participle | taking |\n",
    "| VBN | verb, past participle | taken |\n",
    "| VBP | verb, sing. present, non-3d | take |\n",
    "| VBZ | verb, 3rd person sing. present | takes |\n",
    "| WDT | wh-determiner | which |\n",
    "| WP | wh-pronoun | who, what |\n",
    "| WP\\$ | possessive wh-pronoun | whose |\n",
    "| WRB | wh-abverb | where, when |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.chunking/组块分析\n",
    "##### \\[稀牛学院 x 网易云课程\\]《AI工程师(自然语言处理方向)》课程资料 by [@寒小阳]\n",
    "\n",
    "分块是命名实体识别的基础，词性给出来的句子成分的属性，但有时候，更多的信息(比如句子句法结构)可以帮助我们对句子中的模式挖掘更充分。举个例子，”古天乐赞助了很多小学“中的头部古天乐是一个人名(命名实体)\n",
    "\n",
    "组块分析是一个非常有用的从文本抽取信息的方法，提取组块需要用到正则表达式："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.chunk import RegexpParser\n",
    "from nltk import sent_tokenize,word_tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 写一个匹配名词的模式\n",
    "pattern = \"\"\"\n",
    "    NP: {<JJ>*<NN>+}\n",
    "    {<JJ>*<NN><CC>*<NN>+}\n",
    "    \"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 定义组块分析器\n",
    "chunker = RegexpParser(pattern)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 一段文本\n",
    "text = \"\"\"\n",
    "he National Wrestling Association was an early professional wrestling sanctioning body created in 1930 by \n",
    "the National Boxing Association (NBA) (now the World Boxing Association, WBA) as an attempt to create\n",
    "a governing body for professional wrestling in the United States. The group created a number of \"World\" level \n",
    "championships as an attempt to clear up the professional wrestling rankings which at the time saw a number of \n",
    "different championships promoted as the \"true world championship\". The National Wrestling Association's NWA \n",
    "World Heavyweight Championship was later considered part of the historical lineage of the National Wrestling \n",
    "Alliance's NWA World Heavyweight Championship when then National Wrestling Association champion Lou Thesz \n",
    "won the National Wrestling Alliance championship, folding the original championship into one title in 1949.\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 分句\n",
    "tokenized_sentence = nltk.sent_tokenize(text)\n",
    "# 分词\n",
    "tokenized_words = [nltk.word_tokenize(sentence) for sentence in tokenized_sentence]\n",
    "# 词性标注\n",
    "tagged_words = [nltk.pos_tag(word) for word in tokenized_words]\n",
    "# 识别NP组块\n",
    "word_tree = [chunker.parse(word) for word in tagged_words]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_tree[0].draw() # 会跳出弹窗，显示如下的解析图"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](../img/L2_NLTK_parse.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.命名实体识别\n",
    "##### \\[稀牛学院 x 网易云课程\\]《AI工程师(自然语言处理方向)》课程资料 by [@寒小阳]\n",
    "\n",
    "命名实体识别（Named Entity Recognition，简称NER），又称作“专名识别”，是指识别文本中具有特定意义的实体，主要包括人名、地名、机构名、专有名词等。通常包括两部分：1) 实体边界识别；2) 确定实体类别（人名、地名、机构名或其他）。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(S\n",
      "  (PERSON John/NNP)\n",
      "  studies/NNS\n",
      "  at/IN\n",
      "  (ORGANIZATION Stanford/NNP University/NNP)\n",
      "  ./.)\n"
     ]
    }
   ],
   "source": [
    "from nltk import ne_chunk, pos_tag,  word_tokenize\n",
    "sentence = \"John studies at Stanford University.\"\n",
    "print(ne_chunk(pos_tag(word_tokenize(sentence))))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "命名实体识别也非常推荐大家使用 <a href=\"https://stanfordnlp.github.io/CoreNLP/\">stanford core nlp modules</a> 作为nltk的NER工具库，通常来说它速度更快，而且有更改的识别准确度。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.Stemming和Lemmatizing \n",
    "##### \\[稀牛学院 x 网易云课程\\]《AI工程师(自然语言处理方向)》课程资料 by [@寒小阳]\n",
    "\n",
    "很多时候我们需要对英文当中的时态语态等做归一化，这个时候我们就需要stemming和lemmatizing这样的操作了。比如\"running\"是进行时，但是这个词表征的含义和\"run\"是一致的，我们在识别语义的时候，希望能消除这种差异化。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'run'"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 可以用PorterStemmer\n",
    "from nltk.stem import PorterStemmer\n",
    "stemmer = PorterStemmer()\n",
    "stemmer.stem(\"running\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'make'"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stemmer.stem(\"makes\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'swim'"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stemmer.stem(\"swimming\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'grow'"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 也可以用\n",
    "from nltk.stem import SnowballStemmer\n",
    "stemmer2 = SnowballStemmer(\"english\")\n",
    "stemmer2.stem(\"growing\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'make'"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Lemmatization和Stemmer很类似，不同的地方在于它还考虑了词义关联等信息\n",
    "# Stemmer的速度更快，但是它通常只是一系列的规则\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "lemmatizer.lemmatize(\"makes\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6.WordNet与词义解析\n",
    "##### \\[稀牛学院 x 网易云课程\\]《AI工程师(自然语言处理方向)》课程资料 by [@寒小阳]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Synset('man.n.01'),\n",
       " Synset('serviceman.n.01'),\n",
       " Synset('man.n.03'),\n",
       " Synset('homo.n.02'),\n",
       " Synset('man.n.05'),\n",
       " Synset('man.n.06'),\n",
       " Synset('valet.n.01'),\n",
       " Synset('man.n.08'),\n",
       " Synset('man.n.09'),\n",
       " Synset('man.n.10'),\n",
       " Synset('world.n.08'),\n",
       " Synset('man.v.01'),\n",
       " Synset('man.v.02')]"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.corpus import wordnet as wn\n",
    "wn.synsets('man')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'an adult person who is male (as opposed to a woman)'"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 第一种词义\n",
    "wn.synsets('man')[0].definition()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'someone who serves in the armed forces; a member of a military force'"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 第二种词义\n",
    "wn.synsets('man')[1].definition()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Synset('dog.n.01'),\n",
       " Synset('frump.n.01'),\n",
       " Synset('dog.n.03'),\n",
       " Synset('cad.n.01'),\n",
       " Synset('frank.n.02'),\n",
       " Synset('pawl.n.01'),\n",
       " Synset('andiron.n.01'),\n",
       " Synset('chase.v.01')]"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wn.synsets('dog')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'a member of the genus Canis (probably descended from the common wolf) that has been domesticated by man since prehistoric times; occurs in many breeds'"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 查词义\n",
    "wn.synsets('dog')[0].definition()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'the dog barked all night'"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 造句\n",
    "dog = wn.synset('dog.n.01')\n",
    "dog.examples()[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Synset('canine.n.02'), Synset('domestic_animal.n.01')]"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 上位词\n",
    "dog.hypernyms()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](../img/xiniu_neteasy.png)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
