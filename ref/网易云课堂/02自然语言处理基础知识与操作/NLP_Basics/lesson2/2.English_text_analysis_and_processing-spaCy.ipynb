{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](../img/NLP_banner.jpg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 英文文本处理与[spaCy](https://spacy.io/)\n",
    "#### \\[稀牛学院 x 网易云课程\\]《AI工程师(自然语言处理方向)》课程资料 by [@寒小阳](https://blog.csdn.net/han_xiaoyang)\n",
    "\n",
    "[spaCy](https://spacy.io/)是Python和Cython中的高级自然语言处理库，它建立在最新的研究基础之上，从一开始就设计用于实际产品。spaCy 带有预先训练的统计模型和单词向量，目前支持 20 多种语言的标记。它具有世界上速度最快的句法分析器，用于标签的卷积神经网络模型，解析和命名实体识别以及与深度学习整合。\n",
    "\n",
    "![](../img/L2_spaCy.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 0.安装与配置\n",
    "##### \\[稀牛学院 x 网易云课程\\]《AI工程师(自然语言处理方向)》课程资料 by [@寒小阳]\n",
    "大家可以使用pip轻松安装spaCy工具库，注意到使用spaCy用到的模型(比如英文模型)需要手动下载。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Looking in indexes: https://pypi.tuna.tsinghua.edu.cn/simple\n",
      "Collecting spaCy\n",
      "\u001b[?25l  Downloading https://pypi.tuna.tsinghua.edu.cn/packages/da/de/8644f90ad938d1a8c42d30be176b5a97f785c3cb829a7ec9e1bee857c395/spacy-2.0.17.tar.gz (23.0MB)\n",
      "\u001b[K    100% |################################| 23.0MB 1.7MB/s eta 0:00:01    71% |######################          | 16.3MB 36.1MB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: numpy>=1.15.0 in /usr/local/lib/python3.5/dist-packages (from spaCy) (1.15.2)\n",
      "Collecting msgpack-numpy<0.4.4 (from spaCy)\n",
      "  Downloading https://pypi.tuna.tsinghua.edu.cn/packages/ad/45/464be6da85b5ca893cfcbd5de3b31a6710f636ccb8521b17bd4110a08d94/msgpack_numpy-0.4.3.2-py2.py3-none-any.whl\n",
      "Collecting murmurhash<1.1.0,>=0.28.0 (from spaCy)\n",
      "  Downloading https://pypi.tuna.tsinghua.edu.cn/packages/e9/08/f40e2023f53e859f5b48001ccb57c569a0d4a699fb935cabb4e17f3ab290/murmurhash-1.0.1-cp35-cp35m-manylinux1_x86_64.whl\n",
      "Collecting cymem<2.1.0,>=2.0.2 (from spaCy)\n",
      "  Downloading https://pypi.tuna.tsinghua.edu.cn/packages/6b/d5/c1583c90023608e71ee35b6943d2a5dc488d463b84ecd1c0fddbf23eed44/cymem-2.0.2-cp35-cp35m-manylinux1_x86_64.whl\n",
      "Collecting preshed<2.1.0,>=2.0.1 (from spaCy)\n",
      "\u001b[?25l  Downloading https://pypi.tuna.tsinghua.edu.cn/packages/c5/e5/705d59b08fa3e5ba33d2371f6d52c1993539d5b3beb4130c40bb4e540706/preshed-2.0.1-cp35-cp35m-manylinux1_x86_64.whl (82kB)\n",
      "\u001b[K    100% |################################| 92kB 22.5MB/s ta 0:00:01\n",
      "\u001b[?25hCollecting thinc<6.13.0,>=6.12.0 (from spaCy)\n",
      "\u001b[?25l  Downloading https://pypi.tuna.tsinghua.edu.cn/packages/e3/41/c072537c49900db582d0b4fab943d819b420db891d767c858a0aa36ee169/thinc-6.12.0-cp35-cp35m-manylinux1_x86_64.whl (1.9MB)\n",
      "\u001b[K    100% |################################| 1.9MB 10.9MB/s ta 0:00:01\n",
      "\u001b[?25hCollecting plac<1.0.0,>=0.9.6 (from spaCy)\n",
      "  Downloading https://pypi.tuna.tsinghua.edu.cn/packages/9e/9b/62c60d2f5bc135d2aa1d8c8a86aaf84edb719a59c7f11a4316259e61a298/plac-0.9.6-py2.py3-none-any.whl\n",
      "Collecting ujson>=1.35 (from spaCy)\n",
      "\u001b[?25l  Downloading https://pypi.tuna.tsinghua.edu.cn/packages/16/c4/79f3409bc710559015464e5f49b9879430d8f87498ecdc335899732e5377/ujson-1.35.tar.gz (192kB)\n",
      "\u001b[K    100% |################################| 194kB 3.3MB/s ta 0:00:01\n",
      "\u001b[?25hCollecting dill<0.3,>=0.2 (from spaCy)\n",
      "\u001b[?25l  Downloading https://pypi.tuna.tsinghua.edu.cn/packages/6f/78/8b96476f4ae426db71c6e86a8e6a81407f015b34547e442291cd397b18f3/dill-0.2.8.2.tar.gz (150kB)\n",
      "\u001b[K    100% |################################| 153kB 31.0MB/s ta 0:00:01\n",
      "\u001b[?25hCollecting regex<2017.12.1,>=2017.4.5 (from spaCy)\n",
      "\u001b[?25l  Downloading https://pypi.tuna.tsinghua.edu.cn/packages/b4/fb/e894cf877cc3ad788892899c23e2a73e3e54524c4ec6e5d3826fe153af80/regex-2017.11.09.tar.gz (608kB)\n",
      "\u001b[K    100% |################################| 614kB 5.0MB/s ta 0:00:011\n",
      "\u001b[?25hCollecting requests<3.0.0,>=2.13.0 (from spaCy)\n",
      "\u001b[?25l  Downloading https://pypi.tuna.tsinghua.edu.cn/packages/ff/17/5cbb026005115301a8fb2f9b0e3e8d32313142fe8b617070e7baad20554f/requests-2.20.1-py2.py3-none-any.whl (57kB)\n",
      "\u001b[K    100% |################################| 61kB 20.3MB/s ta 0:00:01\n",
      "\u001b[?25hCollecting msgpack>=0.3.0 (from msgpack-numpy<0.4.4->spaCy)\n",
      "\u001b[?25l  Downloading https://pypi.tuna.tsinghua.edu.cn/packages/08/72/5a01d2a6a894e7f6966b0038445c748d7a16754cceb0e988699269d8152a/msgpack-0.5.6-cp35-cp35m-manylinux1_x86_64.whl (309kB)\n",
      "\u001b[K    100% |################################| 317kB 13.6MB/s ta 0:00:01\n",
      "\u001b[?25hCollecting wrapt<1.11.0,>=1.10.0 (from thinc<6.13.0,>=6.12.0->spaCy)\n",
      "  Downloading https://pypi.tuna.tsinghua.edu.cn/packages/a0/47/66897906448185fcb77fc3c2b1bc20ed0ecca81a0f2f88eda3fc5a34fc3d/wrapt-1.10.11.tar.gz\n",
      "Collecting cytoolz<0.10,>=0.9.0 (from thinc<6.13.0,>=6.12.0->spaCy)\n",
      "\u001b[?25l  Downloading https://pypi.tuna.tsinghua.edu.cn/packages/36/f4/9728ba01ccb2f55df9a5af029b48ba0aaca1081bbd7823ea2ee223ba7a42/cytoolz-0.9.0.1.tar.gz (443kB)\n",
      "\u001b[K    100% |################################| 450kB 19.9MB/s ta 0:00:01\n",
      "\u001b[?25hCollecting tqdm<5.0.0,>=4.10.0 (from thinc<6.13.0,>=6.12.0->spaCy)\n",
      "\u001b[?25l  Downloading https://pypi.tuna.tsinghua.edu.cn/packages/91/55/8cb23a97301b177e9c8e3226dba45bb454411de2cbd25746763267f226c2/tqdm-4.28.1-py2.py3-none-any.whl (45kB)\n",
      "\u001b[K    100% |################################| 51kB 19.2MB/s ta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: six<2.0.0,>=1.10.0 in /usr/local/lib/python3.5/dist-packages (from thinc<6.13.0,>=6.12.0->spaCy) (1.11.0)\n",
      "Collecting certifi>=2017.4.17 (from requests<3.0.0,>=2.13.0->spaCy)\n",
      "\u001b[?25l  Downloading https://pypi.tuna.tsinghua.edu.cn/packages/56/9d/1d02dd80bc4cd955f98980f28c5ee2200e1209292d5f9e9cc8d030d18655/certifi-2018.10.15-py2.py3-none-any.whl (146kB)\n",
      "\u001b[K    100% |################################| 153kB 27.7MB/s ta 0:00:01\n",
      "\u001b[?25hCollecting idna<2.8,>=2.5 (from requests<3.0.0,>=2.13.0->spaCy)\n",
      "\u001b[?25l  Downloading https://pypi.tuna.tsinghua.edu.cn/packages/4b/2a/0276479a4b3caeb8a8c1af2f8e4355746a97fab05a372e4a2c6a6b876165/idna-2.7-py2.py3-none-any.whl (58kB)\n",
      "\u001b[K    100% |################################| 61kB 18.3MB/s ta 0:00:01\n",
      "\u001b[?25hCollecting urllib3<1.25,>=1.21.1 (from requests<3.0.0,>=2.13.0->spaCy)\n",
      "\u001b[?25l  Downloading https://pypi.tuna.tsinghua.edu.cn/packages/62/00/ee1d7de624db8ba7090d1226aebefab96a2c71cd5cfa7629d6ad3f61b79e/urllib3-1.24.1-py2.py3-none-any.whl (118kB)\n",
      "\u001b[K    100% |################################| 122kB 9.4MB/s ta 0:00:01\n",
      "\u001b[?25hCollecting chardet<3.1.0,>=3.0.2 (from requests<3.0.0,>=2.13.0->spaCy)\n",
      "\u001b[?25l  Downloading https://pypi.tuna.tsinghua.edu.cn/packages/bc/a9/01ffebfb562e4274b6487b4bb1ddec7ca55ec7510b22e4c51f14098443b8/chardet-3.0.4-py2.py3-none-any.whl (133kB)\n",
      "\u001b[K    100% |################################| 143kB 34.3MB/s ta 0:00:01\n",
      "\u001b[?25hCollecting toolz>=0.8.0 (from cytoolz<0.10,>=0.9.0->thinc<6.13.0,>=6.12.0->spaCy)\n",
      "\u001b[?25l  Downloading https://pypi.tuna.tsinghua.edu.cn/packages/14/d0/a73c15bbeda3d2e7b381a36afb0d9cd770a9f4adc5d1532691013ba881db/toolz-0.9.0.tar.gz (45kB)\n",
      "\u001b[K    100% |################################| 51kB 19.7MB/s ta 0:00:01\n",
      "\u001b[?25hBuilding wheels for collected packages: spaCy, ujson, dill, regex, wrapt, cytoolz, toolz\n",
      "  Running setup.py bdist_wheel for spaCy ... \u001b[?25ldone\n",
      "\u001b[?25h  Stored in directory: /root/.cache/pip/wheels/9c/e7/21/cbc4c6785b42abce4894b64e0c23a8626c9ed559efebfaaf5e\n",
      "  Running setup.py bdist_wheel for ujson ... \u001b[?25ldone\n",
      "\u001b[?25h  Stored in directory: /root/.cache/pip/wheels/16/3f/9c/4a195876807205a6b102be3ece9238c3dfd01e8c407d53edc7\n",
      "  Running setup.py bdist_wheel for dill ... \u001b[?25ldone\n",
      "\u001b[?25h  Stored in directory: /root/.cache/pip/wheels/b9/2a/cb/969e3cb8b66ad09871d62c9f792c8887047b06dc08b4565bf4\n",
      "  Running setup.py bdist_wheel for regex ... \u001b[?25ldone\n",
      "\u001b[?25h  Stored in directory: /root/.cache/pip/wheels/ed/22/b9/370a12ee3dd298cb01abf29e4dcd976c58d925d1e60fa237ac\n",
      "  Running setup.py bdist_wheel for wrapt ... \u001b[?25ldone\n",
      "\u001b[?25h  Stored in directory: /root/.cache/pip/wheels/3c/4e/54/df3b6755e5a65ad444c874ec18e66f0b89a7fd15b77e475f5a\n",
      "  Running setup.py bdist_wheel for cytoolz ... \u001b[?25ldone\n",
      "\u001b[?25h  Stored in directory: /root/.cache/pip/wheels/ea/db/8b/e8ac820349f9c2b8411c5e822cae2bb3941a079bb034db0692\n",
      "  Running setup.py bdist_wheel for toolz ... \u001b[?25ldone\n",
      "\u001b[?25h  Stored in directory: /root/.cache/pip/wheels/73/ad/e1/f8fe78eeb9e2b31ea8396419d92adc107c553ff7eb47ad12d9\n",
      "Successfully built spaCy ujson dill regex wrapt cytoolz toolz\n",
      "Installing collected packages: msgpack, msgpack-numpy, murmurhash, cymem, preshed, wrapt, dill, toolz, cytoolz, tqdm, plac, thinc, ujson, regex, certifi, idna, urllib3, chardet, requests, spaCy\n",
      "\u001b[33m  The script tqdm is installed in '/root/.local/bin' which is not on PATH.\n",
      "  Consider adding this directory to PATH or, if you prefer to suppress this warning, use --no-warn-script-location.\u001b[0m\n",
      "\u001b[33m  The script chardetect is installed in '/root/.local/bin' which is not on PATH.\n",
      "  Consider adding this directory to PATH or, if you prefer to suppress this warning, use --no-warn-script-location.\u001b[0m\n",
      "Successfully installed certifi-2018.10.15 chardet-3.0.4 cymem-2.0.2 cytoolz-0.9.0.1 dill-0.2.8.2 idna-2.7 msgpack-0.5.6 msgpack-numpy-0.4.3.2 murmurhash-1.0.1 plac-0.9.6 preshed-2.0.1 regex-2017.11.9 requests-2.20.1 spaCy-2.0.17 thinc-6.12.0 toolz-0.9.0 tqdm-4.28.1 ujson-1.35 urllib3-1.24.1 wrapt-1.10.11\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[33mYou are using pip version 18.0, however version 18.1 is available.\r\n",
      "You should consider upgrading via the 'pip install --upgrade pip' command.\u001b[0m\r\n"
     ]
    }
   ],
   "source": [
    "!pip install -i https://pypi.tuna.tsinghua.edu.cn/simple spaCy --user"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting en_core_web_sm==2.0.0 from https://github.com/explosion/spacy-models/releases/download/en_core_web_sm-2.0.0/en_core_web_sm-2.0.0.tar.gz#egg=en_core_web_sm==2.0.0\n",
      "\u001b[?25l  Downloading https://github.com/explosion/spacy-models/releases/download/en_core_web_sm-2.0.0/en_core_web_sm-2.0.0.tar.gz (37.4MB)\n",
      "\u001b[K    100% |################################| 37.4MB 72kB/s ta 0:00:010    71% |#######################         | 26.9MB 82kB/s eta 0:02:08\n",
      "\u001b[?25hInstalling collected packages: en-core-web-sm\n",
      "  Running setup.py install for en-core-web-sm ... \u001b[?25ldone\n",
      "\u001b[?25hSuccessfully installed en-core-web-sm-2.0.0\n",
      "\n",
      "\u001b[93m    Linking successful\u001b[0m\n",
      "    /usr/local/lib/python3.5/dist-packages/en_core_web_sm -->\n",
      "    /root/.local/lib/python3.5/site-packages/spacy/data/en\n",
      "\n",
      "    You can now load the model via spacy.load('en')\n",
      "\n"
     ]
    }
   ],
   "source": [
    "!python3 -m spacy download en"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.英文Tokenization(标记化/分词)\n",
    "##### \\[稀牛学院 x 网易云课程\\]《AI工程师(自然语言处理方向)》课程资料 by [@寒小阳]\n",
    ">文本是不能成段送入模型中进行分析的，我们通常会把文本切成有独立含义的字、词或者短语，这个过程叫做tokenization，这通常是大家解决自然语言处理问题的第一步。在spaCY中同样可以很方便地完成Tokenization。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\"Hello\"\n",
      "\"World\"\n",
      "\"!\"\n",
      "\"My\"\n",
      "\"name\"\n",
      "\"is\"\n",
      "\"HanXiaoyang\"\n"
     ]
    }
   ],
   "source": [
    "import spacy\n",
    "nlp = spacy.load('en')\n",
    "doc = nlp('Hello World! My name is HanXiaoyang')\n",
    "for token in doc:\n",
    "    print('\"' + token.text + '\"')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "每个token对象有着非常丰富的属性，如下的方式可以取出其中的部分属性。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Next\t0\tnext\tFalse\tFalse\tXxxx\tADJ\tJJ\n",
      "week\t5\tweek\tFalse\tFalse\txxxx\tNOUN\tNN\n",
      "I\t10\t-PRON-\tFalse\tFalse\tX\tPRON\tPRP\n",
      "'ll\t11\twill\tFalse\tFalse\t'xx\tVERB\tMD\n",
      "  \t15\t  \tFalse\tTrue\t  \tSPACE\t_SP\n",
      "be\t17\tbe\tFalse\tFalse\txx\tVERB\tVB\n",
      "in\t20\tin\tFalse\tFalse\txx\tADP\tIN\n",
      "Shanghai\t23\tshanghai\tFalse\tFalse\tXxxxx\tPROPN\tNNP\n",
      ".\t31\t.\tTrue\tFalse\t.\tPUNCT\t.\n"
     ]
    }
   ],
   "source": [
    "doc = nlp(\"Next week I'll   be in Shanghai.\")\n",
    "for token in doc:\n",
    "    print(\"{0}\\t{1}\\t{2}\\t{3}\\t{4}\\t{5}\\t{6}\\t{7}\".format(\n",
    "        token.text,\n",
    "        token.idx,\n",
    "        token.lemma_,\n",
    "        token.is_punct,\n",
    "        token.is_space,\n",
    "        token.shape_,\n",
    "        token.pos_,\n",
    "        token.tag_\n",
    "    ))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "断句功能在spaCy中也有体现，如下"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hello World!\n",
      "My name is HanXiaoyang\n"
     ]
    }
   ],
   "source": [
    "# 断句\n",
    "doc = nlp(\"Hello World! My name is HanXiaoyang\")\n",
    "for sent in doc.sents:\n",
    "    print(sent)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.词性标注\n",
    "##### \\[稀牛学院 x 网易云课程\\]《AI工程师(自然语言处理方向)》课程资料 by [@寒小阳]\n",
    ">词性（part-of-speech）是词汇基本的语法属性，通常也称为词性。\n",
    "\n",
    ">词性标注（part-of-speech tagging）,又称为词类标注或者简称标注，是指为分词结果中的每个单词标注一个正确的词性的程序，也即确定每个词是名词、动词、形容词或者其他词性的过程。\n",
    "\n",
    ">词性标注是很多NLP任务的预处理步骤，如句法分析，经过词性标注后的文本会带来很大的便利性，但也不是不可或缺的步骤。\n",
    ">词性标注的最简单做法是选取最高频词性，主流的做法可以分为基于规则和基于统计的方法，包括：\n",
    "* 基于最大熵的词性标注\n",
    "* 基于统计最大概率输出词性\n",
    "* 基于HMM的词性标注"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('Next', 'JJ'), ('week', 'NN'), ('I', 'PRP'), (\"'ll\", 'MD'), ('be', 'VB'), ('in', 'IN'), ('Shanghai', 'NNP'), ('.', '.')]\n"
     ]
    }
   ],
   "source": [
    "# 词性标注\n",
    "doc = nlp(\"Next week I'll be in Shanghai.\")\n",
    "print([(token.text, token.tag_) for token in doc])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "具体的词性标注编码和含义见如下对应表：\n",
    "\n",
    "| POS Tag | Description | Example |\n",
    "| --- | --- | --- |\n",
    "| CC | coordinating conjunction | and |\n",
    "| CD | cardinal number | 1, third |\n",
    "| DT | determiner | the |\n",
    "| EX | existential there | there, is |\n",
    "| FW | foreign word | d’hoevre |\n",
    "| IN | preposition or subordinating conjunction | in, of, like |\n",
    "| JJ | adjective | big |\n",
    "| JJR | adjective, comparative | bigger |\n",
    "| JJS | adjective, superlative | biggest |\n",
    "| LS | list marker | 1) |\n",
    "| MD | modal | could, will |\n",
    "| NN | noun, singular or mass | door |\n",
    "| NNS | noun plural | doors |\n",
    "| NNP | proper noun, singular | John |\n",
    "| NNPS | proper noun, plural | Vikings |\n",
    "| PDT | predeterminer | both the boys |\n",
    "| POS | possessive ending | friend‘s |\n",
    "| PRP | personal pronoun | I, he, it |\n",
    "| PRP$ | possessive pronoun | my, his |\n",
    "| RB | adverb | however, usually, naturally, here, good |\n",
    "| RBR | adverb, comparative | better |\n",
    "| RBS | adverb, superlative | best |\n",
    "| RP | particle | give up |\n",
    "| TO | to | to go, to him |\n",
    "| UH | interjection | uhhuhhuhh |\n",
    "| VB | verb, base form | take |\n",
    "| VBD | verb, past tense | took |\n",
    "| VBG | verb, gerund or present participle | taking |\n",
    "| VBN | verb, past participle | taken |\n",
    "| VBP | verb, sing. present, non-3d | take |\n",
    "| VBZ | verb, 3rd person sing. present | takes |\n",
    "| WDT | wh-determiner | which |\n",
    "| WP | wh-pronoun | who, what |\n",
    "| WP\\$ | possessive wh-pronoun | whose |\n",
    "| WRB | wh-abverb | where, when |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.命名实体识别\n",
    "##### \\[稀牛学院 x 网易云课程\\]《AI工程师(自然语言处理方向)》课程资料 by [@寒小阳]\n",
    "命名实体识别（Named Entity Recognition，简称NER），又称作“专名识别”，是指识别文本中具有特定意义的实体，主要包括人名、地名、机构名、专有名词等。通常包括两部分：1) 实体边界识别；2) 确定实体类别（人名、地名、机构名或其他）。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Next week DATE\n",
      "Shanghai GPE\n"
     ]
    }
   ],
   "source": [
    "doc = nlp(\"Next week I'll be in Shanghai.\")\n",
    "for ent in doc.ents:\n",
    "    print(ent.text, ent.label_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('Next', 'JJ', 'B-DATE'), ('week', 'NN', 'I-DATE'), ('I', 'PRP', 'O'), (\"'ll\", 'MD', 'O'), ('be', 'VB', 'O'), ('in', 'IN', 'O'), ('Shanghai', 'NNP', 'B-GPE'), ('.', '.', 'O')]\n",
      "(S\n",
      "  (DATE Next/JJ week/NN)\n",
      "  I/PRP\n",
      "  'll/MD\n",
      "  be/VB\n",
      "  in/IN\n",
      "  (GPE Shanghai/NNP)\n",
      "  ./.)\n"
     ]
    }
   ],
   "source": [
    "from nltk.chunk import conlltags2tree\n",
    "\n",
    "doc = nlp(\"Next week I'll be in Shanghai.\")\n",
    "iob_tagged = [\n",
    "    (\n",
    "        token.text, \n",
    "        token.tag_, \n",
    "        \"{0}-{1}\".format(token.ent_iob_, token.ent_type_) if token.ent_iob_ != 'O' else token.ent_iob_\n",
    "    ) for token in doc\n",
    "]\n",
    " \n",
    "print(iob_tagged)\n",
    "# 按照nltk.Tree的格式显示\n",
    "print(conlltags2tree(iob_tagged))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "spaCy中包含的命名实体非常丰富，如下例所示："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2 CARDINAL\n",
      "9 a.m. TIME\n",
      "30% PERCENT\n",
      "just 2 days DATE\n",
      "WSJ ORG\n"
     ]
    }
   ],
   "source": [
    "doc = nlp(\"I just bought 2 shares at 9 a.m. because the stock went up 30% in just 2 days according to the WSJ\")\n",
    "for ent in doc.ents:\n",
    "    print(ent.text, ent.label_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "还可以用非常漂亮的可视化做显示："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div class=\"entities\" style=\"line-height: 2.5\">I just bought \n",
       "<mark class=\"entity\" style=\"background: #e4e7d2; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em; box-decoration-break: clone; -webkit-box-decoration-break: clone\">\n",
       "    2\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; text-transform: uppercase; vertical-align: middle; margin-left: 0.5rem\">CARDINAL</span>\n",
       "</mark>\n",
       " shares at \n",
       "<mark class=\"entity\" style=\"background: #bfe1d9; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em; box-decoration-break: clone; -webkit-box-decoration-break: clone\">\n",
       "    9 a.m.\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; text-transform: uppercase; vertical-align: middle; margin-left: 0.5rem\">TIME</span>\n",
       "</mark>\n",
       " because the stock went up \n",
       "<mark class=\"entity\" style=\"background: #e4e7d2; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em; box-decoration-break: clone; -webkit-box-decoration-break: clone\">\n",
       "    30%\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; text-transform: uppercase; vertical-align: middle; margin-left: 0.5rem\">PERCENT</span>\n",
       "</mark>\n",
       " in \n",
       "<mark class=\"entity\" style=\"background: #bfe1d9; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em; box-decoration-break: clone; -webkit-box-decoration-break: clone\">\n",
       "    just 2 days\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; text-transform: uppercase; vertical-align: middle; margin-left: 0.5rem\">DATE</span>\n",
       "</mark>\n",
       " according to the \n",
       "<mark class=\"entity\" style=\"background: #7aecec; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em; box-decoration-break: clone; -webkit-box-decoration-break: clone\">\n",
       "    WSJ\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; text-transform: uppercase; vertical-align: middle; margin-left: 0.5rem\">ORG</span>\n",
       "</mark>\n",
       "</div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from spacy import displacy\n",
    " \n",
    "doc = nlp('I just bought 2 shares at 9 a.m. because the stock went up 30% in just 2 days according to the WSJ')\n",
    "displacy.render(doc, style='ent', jupyter=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.chunking/组块分析\n",
    "##### \\[稀牛学院 x 网易云课程\\]《AI工程师(自然语言处理方向)》课程资料 by [@寒小阳]\n",
    "spaCy可以自动检测名词短语，并输出根(root)词，比如下面的\"Journal\",\"piece\",\"currencies\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall Street Journal NP Journal\n",
      "an interesting piece NP piece\n",
      "crypto currencies NP currencies\n"
     ]
    }
   ],
   "source": [
    "doc = nlp(\"Wall Street Journal just published an interesting piece on crypto currencies\")\n",
    "for chunk in doc.noun_chunks:\n",
    "    print(chunk.text, chunk.label_, chunk.root.text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.句法依存解析\n",
    "##### \\[稀牛学院 x 网易云课程\\]《AI工程师(自然语言处理方向)》课程资料 by [@寒小阳]\n",
    "spaCy有着非常强大的句法依存解析功能，可以试试对句子进行解析。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall/NNP <--compound-- Street/NNP\n",
      "Street/NNP <--compound-- Journal/NNP\n",
      "Journal/NNP <--nsubj-- published/VBD\n",
      "just/RB <--advmod-- published/VBD\n",
      "published/VBD <--ROOT-- published/VBD\n",
      "an/DT <--det-- piece/NN\n",
      "interesting/JJ <--amod-- piece/NN\n",
      "piece/NN <--dobj-- published/VBD\n",
      "on/IN <--prep-- piece/NN\n",
      "crypto/JJ <--compound-- currencies/NNS\n",
      "currencies/NNS <--pobj-- on/IN\n"
     ]
    }
   ],
   "source": [
    "doc = nlp('Wall Street Journal just published an interesting piece on crypto currencies')\n",
    " \n",
    "for token in doc:\n",
    "    print(\"{0}/{1} <--{2}-- {3}/{4}\".format(\n",
    "        token.text, token.tag_, token.dep_, token.head.text, token.head.tag_))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<svg xmlns=\"http://www.w3.org/2000/svg\" xmlns:xlink=\"http://www.w3.org/1999/xlink\" id=\"0\" class=\"displacy\" width=\"1040\" height=\"272.0\" style=\"max-width: none; height: 272.0px; color: #000000; background: #ffffff; font-family: Arial\">\n",
       "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"182.0\">\n",
       "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"50\">Wall</tspan>\n",
       "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"50\">PROPN</tspan>\n",
       "</text>\n",
       "\n",
       "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"182.0\">\n",
       "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"140\">Street</tspan>\n",
       "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"140\">PROPN</tspan>\n",
       "</text>\n",
       "\n",
       "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"182.0\">\n",
       "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"230\">Journal</tspan>\n",
       "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"230\">PROPN</tspan>\n",
       "</text>\n",
       "\n",
       "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"182.0\">\n",
       "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"320\">just</tspan>\n",
       "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"320\">ADV</tspan>\n",
       "</text>\n",
       "\n",
       "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"182.0\">\n",
       "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"410\">published</tspan>\n",
       "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"410\">VERB</tspan>\n",
       "</text>\n",
       "\n",
       "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"182.0\">\n",
       "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"500\">an</tspan>\n",
       "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"500\">DET</tspan>\n",
       "</text>\n",
       "\n",
       "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"182.0\">\n",
       "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"590\">interesting</tspan>\n",
       "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"590\">ADJ</tspan>\n",
       "</text>\n",
       "\n",
       "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"182.0\">\n",
       "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"680\">piece</tspan>\n",
       "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"680\">NOUN</tspan>\n",
       "</text>\n",
       "\n",
       "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"182.0\">\n",
       "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"770\">on</tspan>\n",
       "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"770\">ADP</tspan>\n",
       "</text>\n",
       "\n",
       "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"182.0\">\n",
       "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"860\">crypto</tspan>\n",
       "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"860\">ADJ</tspan>\n",
       "</text>\n",
       "\n",
       "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"182.0\">\n",
       "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"950\">currencies</tspan>\n",
       "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"950\">NOUN</tspan>\n",
       "</text>\n",
       "\n",
       "<g class=\"displacy-arrow\">\n",
       "    <path class=\"displacy-arc\" id=\"arrow-0-0\" stroke-width=\"2px\" d=\"M70,137.0 C70,92.0 130.0,92.0 130.0,137.0\" fill=\"none\" stroke=\"currentColor\"/>\n",
       "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
       "        <textPath xlink:href=\"#arrow-0-0\" class=\"displacy-label\" startOffset=\"50%\" fill=\"currentColor\" text-anchor=\"middle\">compound</textPath>\n",
       "    </text>\n",
       "    <path class=\"displacy-arrowhead\" d=\"M70,139.0 L62,127.0 78,127.0\" fill=\"currentColor\"/>\n",
       "</g>\n",
       "\n",
       "<g class=\"displacy-arrow\">\n",
       "    <path class=\"displacy-arc\" id=\"arrow-0-1\" stroke-width=\"2px\" d=\"M160,137.0 C160,92.0 220.0,92.0 220.0,137.0\" fill=\"none\" stroke=\"currentColor\"/>\n",
       "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
       "        <textPath xlink:href=\"#arrow-0-1\" class=\"displacy-label\" startOffset=\"50%\" fill=\"currentColor\" text-anchor=\"middle\">compound</textPath>\n",
       "    </text>\n",
       "    <path class=\"displacy-arrowhead\" d=\"M160,139.0 L152,127.0 168,127.0\" fill=\"currentColor\"/>\n",
       "</g>\n",
       "\n",
       "<g class=\"displacy-arrow\">\n",
       "    <path class=\"displacy-arc\" id=\"arrow-0-2\" stroke-width=\"2px\" d=\"M250,137.0 C250,47.0 405.0,47.0 405.0,137.0\" fill=\"none\" stroke=\"currentColor\"/>\n",
       "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
       "        <textPath xlink:href=\"#arrow-0-2\" class=\"displacy-label\" startOffset=\"50%\" fill=\"currentColor\" text-anchor=\"middle\">nsubj</textPath>\n",
       "    </text>\n",
       "    <path class=\"displacy-arrowhead\" d=\"M250,139.0 L242,127.0 258,127.0\" fill=\"currentColor\"/>\n",
       "</g>\n",
       "\n",
       "<g class=\"displacy-arrow\">\n",
       "    <path class=\"displacy-arc\" id=\"arrow-0-3\" stroke-width=\"2px\" d=\"M340,137.0 C340,92.0 400.0,92.0 400.0,137.0\" fill=\"none\" stroke=\"currentColor\"/>\n",
       "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
       "        <textPath xlink:href=\"#arrow-0-3\" class=\"displacy-label\" startOffset=\"50%\" fill=\"currentColor\" text-anchor=\"middle\">advmod</textPath>\n",
       "    </text>\n",
       "    <path class=\"displacy-arrowhead\" d=\"M340,139.0 L332,127.0 348,127.0\" fill=\"currentColor\"/>\n",
       "</g>\n",
       "\n",
       "<g class=\"displacy-arrow\">\n",
       "    <path class=\"displacy-arc\" id=\"arrow-0-4\" stroke-width=\"2px\" d=\"M520,137.0 C520,47.0 675.0,47.0 675.0,137.0\" fill=\"none\" stroke=\"currentColor\"/>\n",
       "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
       "        <textPath xlink:href=\"#arrow-0-4\" class=\"displacy-label\" startOffset=\"50%\" fill=\"currentColor\" text-anchor=\"middle\">det</textPath>\n",
       "    </text>\n",
       "    <path class=\"displacy-arrowhead\" d=\"M520,139.0 L512,127.0 528,127.0\" fill=\"currentColor\"/>\n",
       "</g>\n",
       "\n",
       "<g class=\"displacy-arrow\">\n",
       "    <path class=\"displacy-arc\" id=\"arrow-0-5\" stroke-width=\"2px\" d=\"M610,137.0 C610,92.0 670.0,92.0 670.0,137.0\" fill=\"none\" stroke=\"currentColor\"/>\n",
       "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
       "        <textPath xlink:href=\"#arrow-0-5\" class=\"displacy-label\" startOffset=\"50%\" fill=\"currentColor\" text-anchor=\"middle\">amod</textPath>\n",
       "    </text>\n",
       "    <path class=\"displacy-arrowhead\" d=\"M610,139.0 L602,127.0 618,127.0\" fill=\"currentColor\"/>\n",
       "</g>\n",
       "\n",
       "<g class=\"displacy-arrow\">\n",
       "    <path class=\"displacy-arc\" id=\"arrow-0-6\" stroke-width=\"2px\" d=\"M430,137.0 C430,2.0 680.0,2.0 680.0,137.0\" fill=\"none\" stroke=\"currentColor\"/>\n",
       "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
       "        <textPath xlink:href=\"#arrow-0-6\" class=\"displacy-label\" startOffset=\"50%\" fill=\"currentColor\" text-anchor=\"middle\">dobj</textPath>\n",
       "    </text>\n",
       "    <path class=\"displacy-arrowhead\" d=\"M680.0,139.0 L688.0,127.0 672.0,127.0\" fill=\"currentColor\"/>\n",
       "</g>\n",
       "\n",
       "<g class=\"displacy-arrow\">\n",
       "    <path class=\"displacy-arc\" id=\"arrow-0-7\" stroke-width=\"2px\" d=\"M700,137.0 C700,92.0 760.0,92.0 760.0,137.0\" fill=\"none\" stroke=\"currentColor\"/>\n",
       "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
       "        <textPath xlink:href=\"#arrow-0-7\" class=\"displacy-label\" startOffset=\"50%\" fill=\"currentColor\" text-anchor=\"middle\">prep</textPath>\n",
       "    </text>\n",
       "    <path class=\"displacy-arrowhead\" d=\"M760.0,139.0 L768.0,127.0 752.0,127.0\" fill=\"currentColor\"/>\n",
       "</g>\n",
       "\n",
       "<g class=\"displacy-arrow\">\n",
       "    <path class=\"displacy-arc\" id=\"arrow-0-8\" stroke-width=\"2px\" d=\"M880,137.0 C880,92.0 940.0,92.0 940.0,137.0\" fill=\"none\" stroke=\"currentColor\"/>\n",
       "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
       "        <textPath xlink:href=\"#arrow-0-8\" class=\"displacy-label\" startOffset=\"50%\" fill=\"currentColor\" text-anchor=\"middle\">compound</textPath>\n",
       "    </text>\n",
       "    <path class=\"displacy-arrowhead\" d=\"M880,139.0 L872,127.0 888,127.0\" fill=\"currentColor\"/>\n",
       "</g>\n",
       "\n",
       "<g class=\"displacy-arrow\">\n",
       "    <path class=\"displacy-arc\" id=\"arrow-0-9\" stroke-width=\"2px\" d=\"M790,137.0 C790,47.0 945.0,47.0 945.0,137.0\" fill=\"none\" stroke=\"currentColor\"/>\n",
       "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
       "        <textPath xlink:href=\"#arrow-0-9\" class=\"displacy-label\" startOffset=\"50%\" fill=\"currentColor\" text-anchor=\"middle\">pobj</textPath>\n",
       "    </text>\n",
       "    <path class=\"displacy-arrowhead\" d=\"M945.0,139.0 L953.0,127.0 937.0,127.0\" fill=\"currentColor\"/>\n",
       "</g>\n",
       "</svg>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from spacy import displacy\n",
    " \n",
    "doc = nlp('Wall Street Journal just published an interesting piece on crypto currencies')\n",
    "displacy.render(doc, style='dep', jupyter=True, options={'distance': 90})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6.词向量使用\n",
    "##### \\[稀牛学院 x 网易云课程\\]《AI工程师(自然语言处理方向)》课程资料 by [@寒小阳]\n",
    "NLP中有一个非常强大的文本表示学习方法叫做word2vec，通过词的上下文学习到词语的稠密向量化表示，同时在这个表示形态下，语义相关的词在向量空间中会比较接近。也有类似`v(爷爷)-v(奶奶) ≈ v(男人)-v(女人)`的关系。\n",
    "\n",
    "如果大家要使用英文的词向量，需要先下载预先训练好的结果。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!python3 -m spacy download en_core_web_lg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp = spacy.load('en_core_web_lg')\n",
    "print(nlp.vocab['banana'].vector)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy import spatial\n",
    "\n",
    "# 余弦相似度计算\n",
    "cosine_similarity = lambda x, y: 1 - spatial.distance.cosine(x, y)\n",
    "\n",
    "# 男人、女人、国王、女王 的词向量\n",
    "man = nlp.vocab['man'].vector\n",
    "woman = nlp.vocab['woman'].vector\n",
    "queen = nlp.vocab['queen'].vector\n",
    "king = nlp.vocab['king'].vector\n",
    " \n",
    "# 我们对向量做一个简单的计算，\"man\" - \"woman\" + \"queen\"\n",
    "maybe_king = man - woman + queen\n",
    "computed_similarities = []\n",
    "\n",
    "# 扫描整个词库的词向量做比对，召回最接近的词向量\n",
    "for word in nlp.vocab:\n",
    "    if not word.has_vector:\n",
    "        continue\n",
    " \n",
    "    similarity = cosine_similarity(maybe_king, word.vector)\n",
    "    computed_similarities.append((word, similarity))\n",
    "\n",
    "# 排序与最接近结果展示\n",
    "computed_similarities = sorted(computed_similarities, key=lambda item: -item[1])\n",
    "print([w[0].text for w in computed_similarities[:10]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7.词汇与文本相似度\n",
    "##### \\[稀牛学院 x 网易云课程\\]《AI工程师(自然语言处理方向)》课程资料 by [@寒小阳]\n",
    "在词向量的基础上，spaCy提供了从词到文档的相似度计算的方法，下面的例子是它的使用方法。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 词汇语义相似度(关联性)\n",
    "banana = nlp.vocab['banana']\n",
    "dog = nlp.vocab['dog']\n",
    "fruit = nlp.vocab['fruit']\n",
    "animal = nlp.vocab['animal']\n",
    " \n",
    "print(dog.similarity(animal), dog.similarity(fruit)) # 0.6618534 0.23552845\n",
    "print(banana.similarity(fruit), banana.similarity(animal)) # 0.67148364 0.2427285"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 文本语义相似度(关联性)\n",
    "target = nlp(\"Cats are beautiful animals.\")\n",
    " \n",
    "doc1 = nlp(\"Dogs are awesome.\")\n",
    "doc2 = nlp(\"Some gorgeous creatures are felines.\")\n",
    "doc3 = nlp(\"Dolphins are swimming mammals.\")\n",
    " \n",
    "print(target.similarity(doc1))  # 0.8901765218466683\n",
    "print(target.similarity(doc2))  # 0.9115828449161616\n",
    "print(target.similarity(doc3))  # 0.7822956752876101"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](../img/xiniu_neteasy.png)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
