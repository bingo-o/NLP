{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](./img/NLP_banner.jpg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 逻辑回归/SVM与文本分类\n",
    "#### \\[稀牛学院 x 网易云课程\\]《文本分类机器学习模型与实战》课程资料整理 by [@龙心尘](https://blog.csdn.net/longxinchen_ml)\n",
    "\n",
    "* 参考文章：[《从原理到应用：简述Logistics回归算法》](https://www.jiqizhixin.com/articles/2018-05-13-3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 1 Logistic Regression\n",
    "#### \\[稀牛学院 x 网易云课程\\]《文本分类机器学习模型与实战》课程资料整理 by [@龙心尘](https://blog.csdn.net/longxinchen_ml)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.1 什么是 Logistic 回归？\n",
    "#### \\[稀牛学院 x 网易云课程\\]《文本分类机器学习模型与实战》课程资料整理 by [@龙心尘](https://blog.csdn.net/longxinchen_ml)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "和很多其他机器学习算法一样，逻辑回归也是从统计学中借鉴来的，尽管名字里有回归俩字儿，但它不是一个需要预测连续结果的回归算法。\n",
    "\n",
    "与之相反，Logistic 回归是二分类任务的首选方法。它输出一个 0 到 1 之间的离散二值结果。简单来说，它的结果不是 1 就是 0。\n",
    "\n",
    "癌症检测算法可看做是 Logistic 回归问题的一个简单例子，这种算法输入病理图片并且应该辨别患者是患有癌症（1）或没有癌症（0）。\n",
    "\n",
    "它是如何工作的?\n",
    "Logistic 回归通过使用其固有的 logistic 函数估计概率，来衡量因变量（我们想要预测的标签）与一个或多个自变量（特征）之间的关系。\n",
    "\n",
    "然后这些概率必须二值化才能真地进行预测。这就是 logistic 函数的任务，也称为 Sigmoid 函数。Sigmoid 函数是一个 S 形曲线，它可以将任意实数值映射到介于 0 和 1 之间的值，但并不能取到 0或1。然后使用阈值分类器将 0 和 1 之间的值转换为 0 或 1。\n",
    "\n",
    "下面的图片说明了 logistic 回归得出预测所需的所有步骤。\n",
    "\n",
    "\n",
    "![](https://image.jiqizhixin.com/uploads/editor/164bdc1d-46ae-4c67-868f-a36d8c129af0/1526191840515.png)\n",
    "\n",
    "\n",
    "下面是 logistic 函数（sigmoid 函数）的图形表示：\n",
    "\n",
    "![](https://image.jiqizhixin.com/uploads/editor/1fe28e0e-94a3-42aa-a1fa-1c46e21cc29d/1526191840278.png)\n",
    "\n",
    "我们希望随机数据点被正确分类的概率最大化，这就是最大似然估计。最大似然估计是统计模型中估计参数的通用方法。\n",
    "\n",
    "![](https://www.zhihu.com/equation?tex=C%28%5Ctheta%29+%3D+%5C%7B_%7B-log%281-h_%5Ctheta%28x%29%29%2C+y%3D0%7D%5E%7B-log%28h_%5Ctheta%28x%29%29%2Cy%3D1%7D%2C+where%3A+h_%5Ctheta%28x%29%3D%5Cfrac%7B1%7D%7B1%2Be%5E%7B-%5Ctheta%5ETx%7D%7D)\n",
    "\n",
    "你可以使用不同的方法（如优化算法）来最大化概率。牛顿法也是其中一种，可用于查找许多不同函数的最大值（或最小值），包括似然函数。也可以用梯度下降法代替牛顿法。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2 Logistic 回归 vs 线性回归\n",
    "#### \\[稀牛学院 x 网易云课程\\]《文本分类机器学习模型与实战》课程资料整理 by [@龙心尘](https://blog.csdn.net/longxinchen_ml)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "你可能会好奇：logistic 回归和线性回归之间的区别是什么。逻辑回归得到一个离散的结果，但线性回归得到一个连续的结果。预测房价的模型算是返回连续结果的一个好例子。该值根据房子大小或位置等参数的变化而变化。离散的结果总是一件事（你有癌症）或另一个（你没有癌症）。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.3 优缺点\n",
    "#### \\[稀牛学院 x 网易云课程\\]《文本分类机器学习模型与实战》课程资料整理 by [@龙心尘](https://blog.csdn.net/longxinchen_ml)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Logistic 回归是一种被人们广泛使用的算法，因为它非常高效，不需要太大的计算量，又通俗易懂，且很容易调整，并且输出校准好的预测概率。\n",
    "\n",
    "与线性回归一样，当你去掉与输出变量无关的属性以及相似度高的属性时，logistic 回归效果确实会更好。因此特征处理在 Logistic 和线性回归的性能方面起着重要的作用。\n",
    "\n",
    "Logistic 回归的另一个优点是它非常容易实现，且训练起来很高效。在研究中，我通常以 Logistic 回归模型作为基准，再尝试使用更复杂的算法。\n",
    "\n",
    "由于其简单且可快速实现的原因，Logistic 回归也是一个很好的基准，你可以用它来衡量其他更复杂的算法的性能。\n",
    "\n",
    "它的一个缺点就是我们不能用 logistic 回归来解决非线性问题，因为它的决策边界是线性的。我们来看看下面的例子，两个类各有俩实例。\n",
    "\n",
    "![](https://image.jiqizhixin.com/uploads/editor/439f4df7-544b-436d-9fc0-582aaa7c097f/1526191840966.png)\n",
    "\n",
    "显然，我们不可能在不出错的情况下划出一条直线来区分这两个类。使用简单的决策树是个更好的选择。\n",
    "\n",
    "![](https://image.jiqizhixin.com/uploads/editor/c473eda3-cb03-4433-ad89-bd81e88a5f33/1526191840677.png)\n",
    "\n",
    "Logistic 回归并非最强大的算法之一，它可以很容易地被更为复杂的算法所超越，另一个缺点是它高度依赖正确的数据表示。\n",
    "\n",
    "这意味着逻辑回归在你已经确定了所有重要的自变量之前还不会成为一个有用的工具。由于其结果是离散的，Logistic 回归只能预测分类结果。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.4 何时适用\n",
    "#### \\[稀牛学院 x 网易云课程\\]《文本分类机器学习模型与实战》课程资料整理 by [@龙心尘](https://blog.csdn.net/longxinchen_ml)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "就像我已经提到的那样，Logistic 回归通过线性边界将你的输入分成两个「区域」，每个类别划分一个区域。因此，你的数据应当是线性可分的，如下图所示的数据点：\n",
    "\n",
    "![](https://image.jiqizhixin.com/uploads/editor/cc1f0a41-fa18-4499-b12b-374da215ebe9/1526191840804.png)\n",
    "\n",
    "换句话说：当 Y 变量只有两个值时（例如，当你面临分类问题时），您应该考虑使用逻辑回归。注意，你也可以将 Logistic 回归用于多类别分类，下一节中将会讨论。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.5 多分类任务\n",
    "#### \\[稀牛学院 x 网易云课程\\]《文本分类机器学习模型与实战》课程资料整理 by [@龙心尘](https://blog.csdn.net/longxinchen_ml)\n",
    "\n",
    "优先推荐softmax\n",
    "\n",
    "![](https://wikimedia.org/api/rest_v1/media/math/render/svg/001ce4c2c74e78a66a4d7d04ab92cbd0d0fdec02)\n",
    "\n",
    "补充：\n",
    "1）一对多（OVA）\n",
    "按照这个策略，你可以训练 10 个二分类器，每个数字一个。这意味着训练一个分类器来检测 0，一个检测 1，一个检测 2，以此类推。当你想要对图像进行分类时，只需看看哪个分类器的预测分数最高\n",
    "\n",
    "2）一对一（OVO）\n",
    "按照这个策略，要为每一对数字训练一个二分类器。这意味着要训练一个可以区分 0s 和 1s 的分类器，一个可以区分 0s 和 2s 的分类器，一个可以区分 1s 和 2s 的分类器，等等。如果有 N 个类别，则需要训练 N×N（N-1）/ 2 个分类器，对于 MNIST 数据集，需要 45 个分类器。\n",
    "\n",
    "3) 其它分类算法\n",
    "其他常见的分类算法有朴素贝叶斯、决策树、随机森林、支持向量机、k-近邻等等。我们将在其他文章中讨论它们，但别被这些机器学习算法的数量吓到。请注意，最好能够真正了解 4 或 5 种算法，并将精力集中在特征处理上，这也是未来工作的主题。\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.6 小结\n",
    "#### \\[稀牛学院 x 网易云课程\\]《文本分类机器学习模型与实战》课程资料整理 by [@龙心尘](https://blog.csdn.net/longxinchen_ml)\n",
    "在这篇文章中，你已了解什么是 Logistic 回归，以及它是如何工作的。你现在对其优缺点也了有深刻的了解，并且知道何时用它。\n",
    "\n",
    "此外，你还探索了使用 Logistic 回归与 sklearn 进行多分类的方法，以及为什么前者是比其他机器学习算法更好的基准算法。\n",
    "\n",
    "\n",
    "原文链接：https://towardsdatascience.com/the-logistic-regression-algorithm-75fe48e21cfa\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 2 SVM\n",
    "#### \\[稀牛学院 x 网易云课程\\]《文本分类机器学习模型与实战》课程资料整理 by [@龙心尘](https://blog.csdn.net/longxinchen_ml)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  2.1 svm简介\n",
    "#### \\[稀牛学院 x 网易云课程\\]《文本分类机器学习模型与实战》课程资料整理 by [@龙心尘](https://blog.csdn.net/longxinchen_ml)\n",
    "* 参考文章：[《支持向量机(img/svm)是什么意思？》](https://www.zhihu.com/question/21094489/answer/86273196)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "在很久以前的情人节，大侠要去救他的爱人，但魔鬼和他玩了一个游戏。魔鬼在桌子上似乎有规律放了两种颜色的球，说：“你用一根棍分开它们？要求：尽量在放更多球之后，仍然适用。”\n",
    "\n",
    "![](img/svm1.png)\n",
    "于是大侠这样放，干的不错？\n",
    "![](img/svm2.png)\n",
    "然后魔鬼，又在桌上放了更多的球，似乎有一个球站错了阵营。\n",
    "![](img/svm3.png)\n",
    "SVM就是试图把棍放在最佳位置，好让在棍的两边有尽可能大的间隙。\n",
    "![](img/svm4.png)\n",
    "现在即使魔鬼放了更多的球，棍仍然是一个好的分界线。\n",
    "![](img/svm5.png)\n",
    "然后，在SVM 工具箱中有另一个更加重要的 trick。 魔鬼看到大侠已经学会了一个trick，于是魔鬼给了大侠一个新的挑战。\n",
    "![](img/svm6.png) 现在，大侠没有棍可以很好帮他分开两种球了，现在怎么办呢？当然像所有武侠片中一样大侠桌子一拍，球飞到空中。然后，凭借大侠的轻功，大侠抓起一张纸，插到了两种球的中间。\n",
    "![](img/svm7.png)\n",
    "\n",
    "现在，从魔鬼的角度看这些球，这些球看起来像是被一条曲线分开了。\n",
    "![](img/svm8.png)\n",
    "\n",
    "再之后，无聊的大人们，把这些球叫做 「data」，把棍子 叫做 「classifier」, 最大间隙trick 叫做「optimization」， 拍桌子叫做「kernelling」, 那张纸叫做「hyperplane」。\n",
    "\n",
    "图片来源：Support Vector Machines explained well\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2 如何计算svm最优超平面\n",
    "#### \\[稀牛学院 x 网易云课程\\]《文本分类机器学习模型与实战》课程资料整理 by [@龙心尘](https://blog.csdn.net/longxinchen_ml)\n",
    "\n",
    "* 参考文章：[《支持向量机 (SVM)分类器原理分析与基本应用》](https://www.cnblogs.com/muchen/p/6297027.html)\n",
    "\n",
    "1. 首先根据算法思想 - \"找到具有最小间隔的样本点，然后拟合出一个到这些样本点距离和最大的线段/平面。\" 写出目标函数：\n",
    "\n",
    "![](img/svm21.png)\n",
    "\n",
    "该式子的解就是待求的回归系数。\n",
    "\n",
    "然而，这是一个嵌套优化问题，非常难进行直接优化求解。为了解这个式子，还需要以下步骤。\n",
    "\n",
    "2. 不去计算内层的min优化，而是将距离值界定到一个范围 - 大于1，即最近的样本点，也即支持向量到超平面的距离为1。下图可以清楚表示这个意思：\n",
    "\n",
    "![](img/svm22.png)\n",
    "\n",
    "\n",
    "去掉min操作，代之以界定：label * (wTx + b) >= 1。\n",
    "\n",
    "3. 这样得到的式子就是一个带不等式的优化问题，可以采用拉格朗日乘子法(KKT条件)去求解。\n",
    "\n",
    "具体步骤推论本文不给出。推导结果为：\n",
    "\n",
    "![](img/svm23.png)\n",
    "\n",
    "\n",
    "另外，可加入松弛系数 C，用于控制 \"最大化间隔\" 和\"保证大部分点的函数间隔小于1.0\" 这两个目标的权重。\n",
    "\n",
    "将 α >= 0 条件改为 C >= α >= 0 即可。\n",
    "\n",
    "α 是用于求解过程中的一个向量，它和要求的结果回归系数是一一对应的关系。\n",
    "\n",
    "将其中的 α 解出后，便可依据如下两式子(均为推导过程中出现的式子)进行转换得到回归系数：\n",
    "\n",
    "![](img/svm24.png)\n",
    "![](img/svm25.png)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 版权归 © 稀牛学院 所有 保留所有权利\n",
    "![](./img/xiniu_neteasy.png)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
