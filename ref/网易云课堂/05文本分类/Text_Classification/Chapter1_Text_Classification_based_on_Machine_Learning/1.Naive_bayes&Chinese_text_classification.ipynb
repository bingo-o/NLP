{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](./img/NLP_banner.jpg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 朴素贝叶斯模型与中文文本分类\n",
    "#### \\[稀牛学院 x 网易云课程\\]《文本分类机器学习模型与实战》课程资料 by [@龙心尘](https://blog.csdn.net/longxinchen_ml)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* 参考文章：[《朴素贝叶斯模型与中文文本分类》](http://blog.csdn.net/han_xiaoyang/article/details/50616559)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. 引言\n",
    "#### \\[稀牛学院 x 网易云课程\\]《文本分类机器学习模型与实战》课程资料 by [@龙心尘](https://blog.csdn.net/longxinchen_ml)\n",
    "\n",
    "贝叶斯方法是一个历史悠久，有着坚实的理论基础的方法，同时处理很多问题时直接而又高效，很多高级自然语言处理模型也可以从它演化而来。因此，学习贝叶斯方法，是研究自然语言处理问题的一个非常好的切入口。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. 贝叶斯公式\n",
    "#### \\[稀牛学院 x 网易云课程\\]《文本分类机器学习模型与实战》课程资料 by [@龙心尘](https://blog.csdn.net/longxinchen_ml)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "贝叶斯公式就一行：\n",
    "\n",
    "> <center> $P(Y|X)=\\frac{P(X|Y)P(Y)}{P(X)} $ </center>\n",
    "\n",
    "而它其实是由以下的联合概率公式推导出来：\n",
    "\n",
    "> <center>$P(Y,X) = P(Y|X)P(X)=P(X|Y)P(Y) $</center>\n",
    "\n",
    "其中 $P(Y)$ 叫做先验概率， $P(Y|X)$ 叫做后验概率，$P(Y,X)$叫做联合概率。\n",
    "\n",
    "额，恩，没了，贝叶斯最核心的公式就这么些。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. 用机器学习的视角理解贝叶斯公式\n",
    "#### \\[稀牛学院 x 网易云课程\\]《文本分类机器学习模型与实战》课程资料 by [@龙心尘](https://blog.csdn.net/longxinchen_ml)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "在机器学习的视角下，我们把$X$理解成**“具有某特征”**，把$Y$理解成**“类别标签”**(一般机器学习问题中都是`X=>特征`, `Y=>结果`对吧)。在最简单的二分类问题(`是`与`否`判定)下，我们将$Y$理解成**“属于某类**”的标签。于是贝叶斯公式就变形成了下面的样子:\n",
    "\n",
    "> $P(“属于某类”|“具有某特征”)=\\frac{P(“具有某特征”|“属于某类”)P(“属于某类”)}{P(“具有某特征”)} $\n",
    "\n",
    " 我们尝试更口(shuo)语(ren)化(hua)的方式解释一下上述公式：\n",
    "\n",
    "> $P(“属于某类”|“具有某特征”)=$在已知某样本“具有某特征”的条件下，该样本“属于某类”的概率。所以叫做**『后验概率』**。\n",
    "\n",
    "> $P(“具有某特征”|“属于某类”)=$在已知某样本“属于某类”的条件下，该样本“具有某特征”的概率。\n",
    "\n",
    "> $P(“属于某类”) =$（在未知某样本具有该“具有某特征”的条件下，）该样本“属于某类”的概率。所以叫做**『先验概率』**。\n",
    "\n",
    "> $P(“具有某特征”) =$(在未知某样本“属于某类”的条件下，)该样本“具有某特征”的概率。\n",
    "\n",
    "而我们二分类问题的最终目的就是要**判断$P(“属于某类”|“具有某特征”)$是否大于1/2**就够了。贝叶斯方法把计算**“具有某特征的条件下属于某类”**的概率转换成需要计算**“属于某类的条件下具有某特征”**的概率，而后者获取方法就简单多了，我们只需要找到一些包含已知特征标签的样本，即可进行训练。而样本的类别标签都是明确的，所以贝叶斯方法在机器学习里属于有监督学习方法。\n",
    "\n",
    "这里再补充一下，一般**『先验概率』、『后验概率』是相对**出现的，比如$P(Y)$与$P(Y|X)$是关于$Y$的先验概率与后验概率，$P(X)$与$P(X|Y)$是关于$X$的先验概率与后验概率。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. 垃圾邮件识别\n",
    "#### \\[稀牛学院 x 网易云课程\\]《文本分类机器学习模型与实战》课程资料 by [@龙心尘](https://blog.csdn.net/longxinchen_ml)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "举个例子好啦，我们现在要对邮件进行分类，识别垃圾邮件和普通邮件，如果我们选择使用朴素贝叶斯分类器，那目标就是**判断$P(“垃圾邮件”|“具有某特征”)$是否大于1/2**。现在假设我们有垃圾邮件和正常邮件各1万封作为训练集。需要判断以下这个邮件是否属于垃圾邮件：\n",
    "\n",
    "> “我司可办理正规发票（保真）17%增值税发票点数优惠！”\n",
    "\n",
    "也就是**判断概率$P(“垃圾邮件”|“我司可办理正规发票（保真）17\\%增值税发票点数优惠！”)$是否大于1/2**。\n",
    "\n",
    "咳咳，有木有发现，转换成的这个概率，计算的方法：就是写个计数器，然后+1 +1 +1统计出所有垃圾邮件和正常邮件中出现这句话的次数啊！！！好，具体点说：\n",
    "\n",
    "> $P(“垃圾邮件”|“我司可办理正规发票（保真）17\\%增值税发票点数优惠！”)$\n",
    ">$ =\\frac{垃圾邮件中出现这句话的次数}{垃圾邮件中出现这句话的次数+正常邮件中出现这句话的次数}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. 分词\n",
    "#### \\[稀牛学院 x 网易云课程\\]《文本分类机器学习模型与实战》课程资料 by [@龙心尘](https://blog.csdn.net/longxinchen_ml)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "然后同学们开始朝我扔烂白菜和臭鸡蛋，“骗纸！！误人子弟！！你以为发垃圾邮件的人智商都停留在20世纪吗！！你以为它们发邮件像抄作业一样不改内容吗！！哪来那么多相同的句子！！\"。\n",
    "\n",
    "咳咳，表闹，确实，在我们这样的样本容量下，『完全击中』的句子很少甚至没有（无法满足大数定律，），算出来的概率会很失真。一方面找到庞大的训练集是一件非常困难的事情，另一方面其实对于任何的训练集，我们都可以构造出一个从未在训练集中出现的句子作为垃圾邮件（真心的，之前看过朴素贝叶斯分类分错的邮件，我觉得大中华同胞创(zao)新(jia)的能力简直令人惊(fa)呀(zhi)）。\n",
    "\n",
    "一个很悲哀但是很现实的结论：\n",
    "**训练集是有限的，而句子的可能性则是无限的。所以覆盖所有句子可能性的训练集是不存在的。**\n",
    "\n",
    "所以解决方法是？\n",
    "对啦！**句子的可能性无限，但是词语就那么些！！**汉语常用字2500个，常用词语也就56000个(你终于明白小学语文老师的用心良苦了)。按人们的经验理解，两句话意思相近并不强求非得每个字、词语都一样。比如**“我司可办理正规发票，17%增值税发票点数优惠！”**，这句话就比之前那句话少了**“（保真）”**这个词，但是意思基本一样。如果把这些情况也考虑进来，那样本数量就会增加，这就方便我们计算了。\n",
    "\n",
    "于是，我们可以不拿句子作为特征，而是拿句子里面的词语（组合）作为特征去考虑。比如**“正规发票”**可以作为一个单独的词语，**“增值税”**也可以作为一个单独的词语等等。\n",
    "\n",
    "> 句子**“我司可办理正规发票，17%增值税发票点数优惠！”就可以变成（“我”,“司”,“可”,“办理”,“正规发票”,“保真”,“增值税”,“发票”,“点数”,“优惠”)）**。\n",
    "\n",
    "于是你接触到了中文NLP中，最最最重要的技术之一：**分词**！！！也就是**把一整句话拆分成更细粒度的词语来进行表示**。咳咳，另外，分词之后**去除标点符号、数字甚至无关成分(停用词)是特征预处理中的一项技术**。\n",
    "\n",
    "**中文分词是一个专门的技术领域(我不会告诉你某搜索引擎厂码砖工有专门做分词的！！！)，我们将在下一篇文章探讨，这里先将其作为一个已知情况进行处理。具体细节请见下回分晓**\n",
    "\n",
    "我们观察（“我”,“司”,“可”,“办理”,“正规发票”,“保真”,“增值税”,“发票”,“点数”,“优惠”)，**这可以理解成一个向量：向量的每一维度都表示着该特征词在文本中的特定位置存在。这种将特征拆分成更小的单元，依据这些更灵活、更细粒度的特征进行判断的思维方式，在自然语言处理与机器学习中都是非常常见又有效的。**\n",
    "\n",
    "因此贝叶斯公式就变成了：\n",
    "\n",
    "> $P(“垃圾邮件”|（“我”,“司”,“可”,“办理”,“正规发票”,“保真”,“增值税”,“发票”,“点数”,“优惠”)）$\n",
    "> $=\\frac{P(（“我”,“司”,“可”,“办理”,“正规发票”,“保真”,“增值税”,“发票”,“点数”,“优惠”)|\"垃圾邮件\"）P(“垃圾邮件”)}{P(（“我”,“司”,“可”,“办理”,“正规发票”,“保真”,“增值税”,“发票”,“点数”,“优惠”)) }$\n",
    "\n",
    ">$P(“正常邮件”|（“我”,“司”,“可”,“办理”,“正规发票”,“保真”,“增值税”,“发票”,“点数”,“优惠”)）$\n",
    ">$=\\frac{P(（“我”,“司”,“可”,“办理”,“正规发票”,“保真”,“增值税”,“发票”,“点数”,“优惠”)|\"正常邮件\"）P(“正常邮件”)}{P(（“我”,“司”,“可”,“办理”,“正规发票”,“保真”,“增值税”,“发票”,“点数”,“优惠”)) }$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. 条件独立假设\n",
    "#### \\[稀牛学院 x 网易云课程\\]《文本分类机器学习模型与实战》课程资料 by [@龙心尘](https://blog.csdn.net/longxinchen_ml)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "有些同学说...好像...似乎...经过上面折腾，概率看起来更复杂了(-｡-;)\n",
    "那...那我们简化一下...\n",
    "\n",
    "概率$P(（“我”,“司”,“可”,“办理”,“正规发票”,“保真”,“增值税”,“发票”,“点数”,“优惠”)|\"垃圾邮件\"）$依旧不够好求，我们引进一个**很朴素的近似**。为了让公式显得更加紧凑，我们令字母S表示“垃圾邮件”,令字母H表示“正常邮件”。近似公式如下：\n",
    "\n",
    ">$P(（“我”,“司”,“可”,“办理”,“正规发票”,“保真”,“增值税”,“发票”,“点数”,“优惠”)|S）$\n",
    ">$=P(“我”|S）×P(“司”|S）×P(“可”|S）×P(“办理”|S）×P(“正规发票”|S）$\n",
    ">$×P(“保真”|S）×P(“增值税”|S）×P(“发票”|S）×P(“点数”|S）×P(“优惠”|S)$\n",
    "\n",
    "这就是传说中的**条件独立假设**。基于“正常邮件”的条件独立假设的式子与上式类似，此处省去。接着，将条件独立假设代入上面两个相反事件的贝叶斯公式。\n",
    "\n",
    "于是我们就只需要比较以下两个式子的大小：\n",
    "\n",
    ">$C = P(“我”|S)P(“司”|S)P(“可”|S)P(“办理”|S)P(“正规发票”|S)$\n",
    ">$×P(“保真”|S)P(“增值税”|S)P(“发票”|S)P(“点数”|S)P(“优惠”|S)P(“垃圾邮件”)$\n",
    ">$\\overline{C}=P(“我”|H)P(“司”|H)P(“可”|H)P(“办理”|H)P(“正规发票”|H)$\n",
    ">$×P(“保真”|H)P(“增值税”|H)P(“发票”|H)P(“点数”|H)P(“优惠”|H)P(“正常邮件”) $\n",
    "\n",
    "厉害！酱紫处理后**式子中的每一项都特别好求**！只需要**分别统计各类邮件中该关键词出现的概率**就可以了！！！比如：\n",
    "\n",
    ">$P(“发票”|S）=\\frac{垃圾邮件中所有“发票”的次数}{垃圾邮件中所有词语的次数}$\n",
    "\n",
    "统计次数非常方便，而且样本数量足够大，算出来的概率比较接近真实。于是垃圾邮件识别的问题就可解了。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. 朴素贝叶斯(Naive Bayes)，“Naive”在何处？\n",
    "#### \\[稀牛学院 x 网易云课程\\]《文本分类机器学习模型与实战》课程资料 by [@龙心尘](https://blog.csdn.net/longxinchen_ml)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**加上条件独立假设的贝叶斯方法就是朴素贝叶斯方法（Naive Bayes）。** Naive的发音是“乃一污”，意思是“朴素的”、“幼稚的”、**“蠢蠢的”**。咳咳，也就是说，大神们取名说该方法是一种比较萌蠢的方法，为啥？\n",
    "\n",
    "将句子（“我”,“司”,“可”,“办理”,“正规发票”) 中的 （“我”,“司”）与（“正规发票”）调换一下顺序，就变成了一个新的句子（“正规发票”,“可”,“办理”, “我”, “司”)。新句子与旧句子的意思完全不同。**但由于乘法交换律，朴素贝叶斯方法中算出来二者的条件概率完全一样！**计算过程如下：\n",
    "\n",
    ">$P(（“我”,“司”,“可”,“办理”,“正规发票”)|S)$\n",
    ">$=P(“我”|S)P(“司”|S)P(“可”|S)P(“办理”|S)P(“正规发票”|S) $\n",
    ">$=P(“正规发票”|S)P(“可”|S)P(“办理”|S)P(“我”|S)P(“司”|S）$\n",
    ">$=P(（“正规发票”,“可”,“办理”, “我”, “司”)|S)$\n",
    "\n",
    "**也就是说，在朴素贝叶斯眼里，“我司可办理正规发票”与“正规发票可办理我司”完全相同。朴素贝叶斯失去了词语之间的顺序信息。**这就相当于把所有的词汇扔进到一个袋子里随便搅和，贝叶斯都认为它们一样。因此这种情况也称作**词袋子模型(bag of words)**。\n",
    "\n",
    "![词袋子配图](http://ww1.sinaimg.cn/large/b57cc2efly1fxcjker3m9j205k07kq3u.jpg)\n",
    "\n",
    "词袋子模型与人们的日常经验完全不同。比如，在条件独立假设的情况下， **“武松打死了老虎”与“老虎打死了武松”被它认作一个意思了**。恩，朴素贝叶斯就是这么单纯和直接，对比于其他分类器，好像是显得有那么点萌蠢。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. 简单高效，吊丝逆袭\n",
    "#### \\[稀牛学院 x 网易云课程\\]《文本分类机器学习模型与实战》课程资料 by [@龙心尘](https://blog.csdn.net/longxinchen_ml)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "虽然说朴素贝叶斯方法萌蠢萌蠢的，但实践证明在垃圾邮件识别的应用还**令人诧异地好**。Paul Graham先生自己简单做了一个朴素贝叶斯分类器，**“1000封垃圾邮件能够被过滤掉995封，并且没有一个误判”。**（Paul Graham《黑客与画家》）\n",
    "\n",
    "那个...效果为啥好呢？\n",
    "\n",
    "“有人对此提出了一个理论解释，并且建立了什么时候朴素贝叶斯的效果能够等价于非朴素贝叶斯的充要条件，这个解释的核心就是：有些独立假设在各个分类之间的分布都是均匀的所以对于似然的相对大小不产生影响；即便不是如此，也有很大的可能性**各个独立假设所产生的消极影响或积极影响互相抵消，最终导致结果受到的影响不大**。具体的数学公式请参考[这篇 paper](http://www.cs.unb.ca/profs/hzhang/publications/FLAIRS04ZhangH.pdf)。”（刘未鹏《：平凡而又神奇的贝叶斯方法》）\n",
    "\n",
    "恩，这个分类器中最简单直接看似萌蠢的小盆友『朴素贝叶斯』，实际上却是**简单、实用、且强大**的。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 版权归 © 稀牛学院 所有 保留所有权利\n",
    "![](./img/xiniu_neteasy.png)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
