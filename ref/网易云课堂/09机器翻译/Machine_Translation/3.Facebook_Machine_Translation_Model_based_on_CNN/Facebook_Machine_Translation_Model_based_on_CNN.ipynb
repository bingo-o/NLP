{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "![](./img/NLP_banner.jpg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# facebook基于CNN的机器翻译模型\n",
    "#### \\[稀牛学院 x 网易云课程\\]《AI工程师(自然语言处理)》by 胡俊杰"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 本章概述\n",
    "- 基于CNN的翻译系统模型架构\n",
    "    - Pooling Encoder\n",
    "    - Convolution Encoder\n",
    "    - Convolution NMT\n",
    "    - 对比CNN与RNN去构建的 encoder-decoder模型，分析CNN的优缺点\n",
    "- 使用CNN完成神经翻译系统的Trick\n",
    "    - 对模型某些部分做缩放（scaling）\n",
    "    - 对模型参数的初始化\n",
    "    - 对超参数的选择\n",
    "- 【实战】facebook CNN机器翻译系统代码解析\n",
    "    - 举例训练，及测试 CNN翻译系统\n",
    "    - 分析 FconvModel, FconvEncoder, FconvDecoder\n",
    "    - 分析 main 函数中训练模型部分"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## 1.基于CNN的翻译系统模型结构\n",
    "#### \\[稀牛学院 x 网易云课程\\]《AI工程师(自然语言处理)》by 胡俊杰\n",
    "- 在自然语言处理中，大部分流行的seq2seq模型都是基于RNN结构去构建encoder和decoder，但是RNN对于下一个状态的预测需要依赖前面的所有历史状态，使得并行化操作难以充分进行，难以发挥完全发挥GPU并行的效率。相反CNN通过在固定窗口内的计算，使得计算的并行化变得更加简单，而且通过多层CNN网络可以构建层级结构(hierarchical structure)，可以达到利用更短的路径去覆盖更长范围内的信息。\n",
    "- Facebook提出了基于CNN的机器翻译模型，并开源了CNN的机器翻译工具[Fairseq](https://github.com/facebookresearch/fairseq)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.1 Pooling Encoder\n",
    "#### \\[稀牛学院 x 网易云课程\\]《AI工程师(自然语言处理)》by 胡俊杰\n",
    "\n",
    "- 最简单的non-recurrent encoder就是把k个连续的单词的词向量求平均值，通过在句子左右两边都做添加额外的空单词(paddings)，可以使得encoder输出跟原来句子同等长度的hidden embeddings。\n",
    "    - 假设原来的句子的词向量（word embedding）表示为 $w=[w_1,\\cdots,w_m],~\\forall~w_j\\in R^f$\n",
    "    - absolute position embeddings用于编码位置信息　$p=[p_1,\\cdots,p_m],~\\forall~p_j\\in R^f$\n",
    "    $$e_j = w_j + p_j,~~ z_j = {1\\over k} \\sum_{t=-k/2}^{k/2}e_{j+t} $$\n",
    "    - 传统的attention 机制\n",
    "    $$ c_i = \\sum_{j=1}^m a_{ij} e_j$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2 卷积编码器　Convolutional Encoder NMT [Gehring et. al 2016](https://arxiv.org/pdf/1611.02344.pdf)\n",
    "#### \\[稀牛学院 x 网易云课程\\]《AI工程师(自然语言处理)》by 胡俊杰\n",
    "\n",
    "- 卷积编码器在pooling encoder的基础上进行改进，使用一个CNN-a 卷积层来进一步编码源语言句子中的每个单词\n",
    "\n",
    "$$z_j = CNN-a(e)_j $$\n",
    "\n",
    "- 注意attention的时候，使用了另一个CNN-c　卷积层来编码源语言句子中的每个单词，并计算atttention weight，再进行加权求和\n",
    "$$c_i = \\sum_{j=1}^m a_{ij} CNN-c(e)_j$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2 卷积编码器　Convolutional Encoder NMT [Gehring et. al 2016](https://arxiv.org/pdf/1611.02344.pdf)\n",
    "#### \\[稀牛学院 x 网易云课程\\]《AI工程师(自然语言处理)》by 胡俊杰\n",
    "\n",
    "- 该模型的encoder 采用的是CNN，但其decoder还是采用了传统的RNN模型\n",
    "\n",
    "![](./img/cnn-encoder.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.３ 全卷积神经翻译模型　Convolutional NMT [Gehring et. al 2017](https://arxiv.org/pdf/1705.03122.pdf)\n",
    "#### \\[稀牛学院 x 网易云课程\\]《AI工程师(自然语言处理)》by 胡俊杰\n",
    "\n",
    "- 该模型的encoder和decoder都采用的是卷积核CNN，动图演示\n",
    "![](./img/fairseq.gif)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.３ 全卷积神经翻译模型　Convolutional NMT [Gehring et. al 2017](https://arxiv.org/pdf/1705.03122.pdf)\n",
    "#### \\[稀牛学院 x 网易云课程\\]《AI工程师(自然语言处理)》by 胡俊杰\n",
    "\n",
    "- 卷积核结构\n",
    "    - 假设有1D的卷积核的窗口大小是k(比如k=5)，每个卷积核都可以用一个权重矩阵$W\\in \\mathbb{R}^{2d\\times kd}$和 bias $b_w\\in \\mathbb{R}^{2d}$。对于窗口内的词向量　$X\\in \\mathbb{R}^{k\\times d}$把每个单词都拼接成一个长向量　$X'\\in \\mathbb{R}^{kd}$.\n",
    "    $$Y=WX'+b_w = [A B] \\in \\mathbb{R}^{2d} \\\\ A,B\\in \\mathbb{R}^{d} $$\n",
    "    \n",
    "    - 接下来我们采用Gated Linear Unites(GLU)的方式来进行编码, $\\sigma()$是一个非线性的激活函数，　$\\otimes$是element-wise mulitiplication，指的是对两个向量中的每个维度上的数值分别求乘积　\n",
    "    $$v([A B] = A \\otimes \\sigma(B) \\in \\mathbb{R}^d$$\n",
    "    \n",
    "    - 残差连接　Residual Connection:　把上一层的输入也累加到下一层的输出\n",
    "    $$h_i^l = v(W^l [h_{(i-k)/2}^{l-1},\\cdots,h_{(i+k)/2}^{l-1}]+b_w^l)+h_i^{l-1}　\\in \\mathbb{R}^d$$\n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.３ 全卷积神经翻译模型　Convolutional NMT [Gehring et. al 2017](https://arxiv.org/pdf/1705.03122.pdf)\n",
    "#### \\[稀牛学院 x 网易云课程\\]《AI工程师(自然语言处理)》by 胡俊杰\n",
    "\n",
    "- 编码器　Encoder:\n",
    "    - 假设原来的句子的词向量（word embedding）表示为 $w=[w_1,\\cdots,w_m],~\\forall~w_j\\in \\mathbb{R}^f$\n",
    "    - absolute position embeddings用于编码位置信息　$p=[p_1,\\cdots,p_m],~\\forall~p_j\\in \\mathbb{R}^f$\n",
    "    $$e_j = w_j + p_j \\\\ $$\n",
    "    \n",
    "    - encoder 先用一个线性函数$f:\\mathbb{R}^f\\rightarrow \\mathbb{R}^d$，把词向量映射到d维空间中  \n",
    "    - 接下来encoder会将词向量通过一层层卷积核，得到每一层的单词的隐式表达（hidden state）, 其中　$z_j^u$　代表的是第u层CNN中第j个单词的表达\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.３ 全卷积神经翻译模型　Convolutional NMT [Gehring et. al 2017](https://arxiv.org/pdf/1705.03122.pdf)\n",
    "#### \\[稀牛学院 x 网易云课程\\]《AI工程师(自然语言处理)》by 胡俊杰\n",
    "- Multi-step Attention机制\n",
    "    - 假设已经翻译的单词的词表达是 $g=[g_1,\\cdots, g_n]$，跟源语言的词表达一样，这里也是word embeddings加上positional embeddings\n",
    "    －假设decoder的卷积核的hidden state $h_i^l$, 可以进一步计算decoder已经生成的单词的每一层的单词表达\n",
    "    $$d_i^l = W_d^l h_i^l + b_d^l + g_i $$\n",
    "    \n",
    "    －假设encoder 最顶层(假设是第u层)中，每个单词的表达是　$z_j^u$。我们可以计算decoder第l层中第i个已经生成的单词　$h_i^l$与源语言句子中最顶层（也即是第u层）的第j个单词 $z_j^u$的权重:    \n",
    "    $$a_{ij}^l = {\\exp(d_i^l \\cdot z_j^u) \\over \\sum_{t=1}^m \\exp(d_i^l \\cdot z_t^u) } $$\n",
    "    \n",
    "    －我们可以进一步计算在decoder第l层，在第i个时刻的上下文向量（也即是context vector）如以下公式，其中我们将encoder最顶层(第u层)的词向量$z_j^u$与最底层的词向量$e_j$相加。\n",
    "    \n",
    "    $$c_i^l = \\sum_{j=1}^m a_{ij}^l (z_j^u + e_j) $$\n",
    "    －一旦我们计算好$c_i^l$,我们将　$c_i^l$加到$h_i^l$中，作为decoder 的下一层的输入"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.３ 全卷积神经翻译模型　Convolutional NMT [Gehring et. al 2017](https://arxiv.org/pdf/1705.03122.pdf)\n",
    "#### \\[稀牛学院 x 网易云课程\\]《AI工程师(自然语言处理)》by 胡俊杰\n",
    "- 解码器　decoder\n",
    "    - 把decoder最顶层的hidden state $h_i^L$　通过一个线性的函数映射到词表空间上$d\\rightarrow |V|$，之后在通过一个softmax函数　归一化成一个条件概率向量：\n",
    "    $$p(y_{i+1}|y_1,\\cdots, y_i, x)= softmax(W_o h_i^L + b_0) \\in \\mathbb{R}^{|V|} $$\n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.３ 全卷积神经翻译模型　Convolutional NMT [Gehring et. al 2017](https://arxiv.org/pdf/1705.03122.pdf)\n",
    "#### \\[稀牛学院 x 网易云课程\\]《AI工程师(自然语言处理)》by 胡俊杰\n",
    "- 模型的结构图\n",
    "\n",
    "<img src=\"./img/cnn-nmt.png\" alt=\"Drawing\" style=\"width: 500px;\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.4 全卷积神经翻译模型对比RNN神经翻译模型\n",
    "#### \\[稀牛学院 x 网易云课程\\]《AI工程师(自然语言处理)》by 胡俊杰\n",
    "- 全卷积神经网络使用层级结构，可以充分地并行化\n",
    "- 对于一个窗口大小为$k$的CNN，编码一个特征向量可以总结一个窗口为n个单词的信息，只需要做$O(n/k)$个卷积核操作。对比RNN，RNN编码一个窗口为n个单词的信息，需要做$O(n)$个操作，跟句子的长度成正比\n",
    "- 对于一个CNN的输入，我们都进行了相同数量的卷积操作及非线性操作。对比RNN，第一个输入的单词进行了n词非线性操作，而最后一个输入的单词只进行了一次非线性操作。对于每个输入都进行相同数量的操作会有利于训练。\n",
    "- 训练CNN NMT需要非常小心地设置参数及调整网络中某些层的缩放。\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2 使用CNN完成神经机器翻译系统的tricks\n",
    "#### \\[稀牛学院 x 网易云课程\\]《AI工程师(自然语言处理)》by 胡俊杰\n",
    "- 训练过程中，我们需要将网络中某些部分进行缩放(scaling)\n",
    "- 训练过程中，我们需要对权重初始化\n",
    "- 训练过程中，我们需要对超参数进行设置"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1 缩放操作（scaling）\n",
    "#### \\[稀牛学院 x 网易云课程\\]《AI工程师(自然语言处理)》by 胡俊杰\n",
    "- 我们将残差层的输出乘以　$\\sqrt{0.5}$，　这样会减小一半的偏差variance\n",
    "- 对于attention机制产生的上下文向量　$c_{ij}^l$　乘以一个系数　$m\\sqrt{1/m}$, 其中m为源语言句子中单词个个数，这样做的好处也是能减小偏差。\n",
    "- 对于CNN decoder有multiple atttention的情况，我们将encoder 每一层的gradient乘以一个系数，该系数是我们使用的attention的数量。注意的是，我们只对encoder中除了源语言单词的词向量矩阵以外的参数进行放大他们的gradient，源语言的词向量矩阵的gradient不进行放大。在实验中，我们发现这样的操作会使得训练能更加稳定。\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2 参数初始化\n",
    "#### \\[稀牛学院 x 网易云课程\\]《AI工程师(自然语言处理)》by 胡俊杰\n",
    "- 所有的词向量矩阵从一个以０为中心，标准差为0.1的高斯分布中随机初始化　$\\mathcal{N}(0, \\sqrt{n_l})$, 其中$n_l$为输入到这个神经元的输入个数，一般我们可以设置为0.1。这样能有助于保持一个正态分布的偏差。\n",
    "- 我们还需要对每一层的激活函数输出进行正则化(normalization)，　比如残差连接中，每一层层的输出向量需要先做正则化，再把这一层的输入加到输出的向量上。\n",
    "- 对于GLU，我们需要对其权重　$W$从一个正态分布$\\mathcal{N}(0, \\sqrt{4p\\over n_l})$中随机抽样，而其bias设置成０\n",
    "- 我们对每一层网络的输入向量都进行dropout处理\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.3 超参数设置\n",
    "#### \\[稀牛学院 x 网易云课程\\]《AI工程师(自然语言处理)》by 胡俊杰\n",
    "- encoder 和decoder都是用512维的hidden units，512维的word embeddings\n",
    "- 训练的时候使用Nesterov's accelerated gradient 的方法进行优化模型，momentum 设置成0.99\n",
    "- 如果gradient的norm超过0.1就把gradient 重新归一化到0.1以内。\n",
    "- 初始的learning rate设置成0.25，如果在每次进行valudation的时候dev数据集中的perplexity没有下降，我们就将learning rate乘以0.1,　一直持续到learning rate 降到$10^{-4}$以下我们停止训练\n",
    "- mini-batch　的大小设置成每次处理64句双语句子\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Facebook CNN 机器翻译系统代码解析\n",
    "#### \\[稀牛学院 x 网易云课程\\]《AI工程师(自然语言处理)》by 胡俊杰\n",
    "- 相应的代码可以在github上找到　[fairseq](https://github.com/pytorch/fairseq)\n",
    "- 安装\n",
    "```bash\n",
    "git clone https://github.com/pytorch/fairseq.git\n",
    "cd fairseq\n",
    "pip install -r requirements.txt\n",
    "python setup.py build develop\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1 实战例子\n",
    "#### \\[稀牛学院 x 网易云课程\\]《AI工程师(自然语言处理)》by 胡俊杰\n",
    "\n",
    "```bash\n",
    "# 预处理数据\n",
    "$ bash prepare-wmt14en2de.sh --icml17\n",
    "\n",
    "$ cd examples/translation/\n",
    "$ bash prepare-wmt14en2de.sh\n",
    "$ cd ../..\n",
    "\n",
    "# 将数据处理成二进制形式，加速读写\n",
    "$ TEXT=examples/translation/wmt14_en_de\n",
    "$ python preprocess.py --source-lang en --target-lang de \\\n",
    "  --trainpref $TEXT/train --validpref $TEXT/valid --testpref $TEXT/test \\\n",
    "  --destdir data-bin/wmt14_en_de --thresholdtgt 0 --thresholdsrc 0\n",
    "\n",
    "# 训练模型\n",
    "# 如果显存不足，可以将--max-tokens设置成1500\n",
    "$ mkdir -p checkpoints/fconv_wmt_en_de\n",
    "$ python train.py data-bin/wmt14_en_de \\\n",
    "  --lr 0.5 --clip-norm 0.1 --dropout 0.2 --max-tokens 4000 \\\n",
    "  --criterion label_smoothed_cross_entropy --label-smoothing 0.1 \\\n",
    "  --lr-scheduler fixed --force-anneal 50 \\\n",
    "  --arch fconv_wmt_en_de --save-dir checkpoints/fconv_wmt_en_de\n",
    "\n",
    "# 测试，生成\n",
    "$ python generate.py data-bin/wmt14_en_de \\\n",
    "  --path checkpoints/fconv_wmt_en_de/checkpoint_best.pt --beam 5 --remove-bpe\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2 使用预训练好的模型\n",
    "#### \\[稀牛学院 x 网易云课程\\]《AI工程师(自然语言处理)》by 胡俊杰\n",
    "\n",
    "```bash\n",
    "# 下载模型及数据\n",
    "$ mkdir -p data-bin\n",
    "$ curl https://dl.fbaipublicfiles.com/fairseq/models/wmt14.v2.en-fr.fconv-py.tar.bz2 | tar xvjf - -C data-bin\n",
    "$ curl https://dl.fbaipublicfiles.com/fairseq/data/wmt14.v2.en-fr.newstest2014.tar.bz2 | tar xvjf - -C data-bin\n",
    "\n",
    "# 进行翻译生成\n",
    "$ python generate.py data-bin/wmt14.en-fr.newstest2014  \\\n",
    "  --path data-bin/wmt14.en-fr.fconv-py/model.pt \\\n",
    "  --beam 5 --batch-size 128 --remove-bpe | tee /tmp/gen.out\n",
    "...\n",
    "| Translated 3003 sentences (96311 tokens) in 166.0s (580.04 tokens/s)\n",
    "| Generate test with beam=5: BLEU4 = 40.83, 67.5/46.9/34.4/25.5 (BP=1.000, ratio=1.006, syslen=83262, reflen=82787)\n",
    "\n",
    "# 对翻译结果打分\n",
    "$ grep ^H /tmp/gen.out | cut -f3- > /tmp/gen.out.sys\n",
    "$ grep ^T /tmp/gen.out | cut -f2- > /tmp/gen.out.ref\n",
    "$ python score.py --sys /tmp/gen.out.sys --ref /tmp/gen.out.ref\n",
    "BLEU4 = 40.83, 67.5/46.9/34.4/25.5 (BP=1.000, ratio=1.006, syslen=83262, reflen=82787)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.3 代码讲解\n",
    "#### \\[稀牛学院 x 网易云课程\\]《AI工程师(自然语言处理)》by 胡俊杰\n",
    "- CNN NMT类 FConvModel\n",
    "```python\n",
    "@register_model('fconv')\n",
    "class FConvModel(FairseqModel):\n",
    "    \"\"\"\n",
    "    Args:\n",
    "        encoder (FConvEncoder): the encoder\n",
    "        decoder (FConvDecoder): the decoder\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, encoder, decoder):\n",
    "        ...\n",
    "    @staticmethod\n",
    "    def add_args(parser):\n",
    "        parser.add_argument('--dropout', type=float, metavar='D',\n",
    "                            help='dropout probability')\n",
    "        parser.add_argument('--encoder-embed-dim', type=int, metavar='N',\n",
    "                            help='encoder embedding dimension')\n",
    "        parser.add_argument('--encoder-embed-path', type=str, metavar='STR',\n",
    "                            help='path to pre-trained encoder embedding')\n",
    "        parser.add_argument('--encoder-layers', type=str, metavar='EXPR',\n",
    "                            help='encoder layers [(dim, kernel_size), ...]')\n",
    "        parser.add_argument('--decoder-embed-dim', type=int, metavar='N',\n",
    "                            help='decoder embedding dimension')\n",
    "        parser.add_argument('--decoder-embed-path', type=str, metavar='STR',\n",
    "                            help='path to pre-trained decoder embedding')\n",
    "        parser.add_argument('--decoder-layers', type=str, metavar='EXPR',\n",
    "                            help='decoder layers [(dim, kernel_size), ...]')\n",
    "        parser.add_argument('--decoder-out-embed-dim', type=int, metavar='N',\n",
    "                            help='decoder output embedding dimension')\n",
    "    @classmethod\n",
    "    def build_model(cls, args, task):\n",
    "        base_architecture(args)\n",
    "        ...\n",
    "        encoder = FConvEncoder(\n",
    "            dictionary=task.source_dictionary,\n",
    "            embed_dim=args.encoder_embed_dim,\n",
    "            embed_dict=encoder_embed_dict,\n",
    "            convolutions=eval(args.encoder_layers),\n",
    "            dropout=args.dropout,\n",
    "            max_positions=args.max_source_positions,\n",
    "        )\n",
    "        decoder = FConvDecoder(\n",
    "            dictionary=task.target_dictionary,\n",
    "            embed_dim=args.decoder_embed_dim,\n",
    "            embed_dict=decoder_embed_dict,\n",
    "            convolutions=eval(args.decoder_layers),\n",
    "            out_embed_dim=args.decoder_out_embed_dim,\n",
    "            attention=eval(args.decoder_attention),\n",
    "            dropout=args.dropout,\n",
    "            max_positions=args.max_target_positions,\n",
    "            share_embed=args.share_input_output_embed,\n",
    "        )\n",
    "        return FConvModel(encoder, decoder)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.3 代码讲解\n",
    "#### \\[稀牛学院 x 网易云课程\\]《AI工程师(自然语言处理)》by 胡俊杰\n",
    "- CNN encoder类 \n",
    "```python\n",
    "class FConvEncoder(FairseqEncoder):\n",
    "    def __init__(\n",
    "            self, dictionary, embed_dim=512, embed_dict=None, max_positions=1024,\n",
    "            convolutions=((512, 3),) * 20, dropout=0.1, left_pad=True,\n",
    "    ):\n",
    "        ...\n",
    "        # 定义词向量矩阵及位置矩阵\n",
    "        self.embed_tokens = Embedding(num_embeddings, embed_dim, self.padding_idx)\n",
    "        self.embed_positions = PositionalEmbedding(\n",
    "            max_positions,\n",
    "            embed_dim,\n",
    "            self.padding_idx,\n",
    "            left_pad=self.left_pad,\n",
    "        )\n",
    "\n",
    "        convolutions = extend_conv_spec(convolutions)\n",
    "        in_channels = convolutions[0][0]\n",
    "        self.fc1 = Linear(embed_dim, in_channels, dropout=dropout)\n",
    "        self.projections = nn.ModuleList()\n",
    "        self.convolutions = nn.ModuleList()\n",
    "        self.residuals = []\n",
    "        \n",
    "        # 定义CNN层及残差层\n",
    "        layer_in_channels = [in_channels]\n",
    "        for _, (out_channels, kernel_size, residual) in enumerate(convolutions):\n",
    "            if residual == 0:\n",
    "                residual_dim = out_channels\n",
    "            else:\n",
    "                residual_dim = layer_in_channels[-residual]\n",
    "            self.projections.append(Linear(residual_dim, out_channels)\n",
    "                                    if residual_dim != out_channels else None)\n",
    "            if kernel_size % 2 == 1:\n",
    "                padding = kernel_size // 2\n",
    "            else:\n",
    "                padding = 0\n",
    "            self.convolutions.append(\n",
    "                ConvTBC(in_channels, out_channels * 2, kernel_size,\n",
    "                        dropout=dropout, padding=padding)\n",
    "            )\n",
    "            self.residuals.append(residual)\n",
    "            in_channels = out_channels\n",
    "            layer_in_channels.append(out_channels)\n",
    "        self.fc2 = Linear(in_channels, embed_dim)\n",
    "\n",
    "    def forward(self, src_tokens, src_lengths):\n",
    "        # 查找词向量及位置向量\n",
    "        x = self.embed_tokens(src_tokens) + self.embed_positions(src_tokens)\n",
    "        x = F.dropout(x, p=self.dropout, training=self.training)\n",
    "        input_embedding = x\n",
    "\n",
    "        # 将词的表达映射到CNN的输入空间 fc1: R^f ->R^d\n",
    "        x = self.fc1(x)\n",
    "\n",
    "        # 在句子左右两边添加padding\n",
    "        encoder_padding_mask = src_tokens.eq(self.padding_idx).t()  # -> T x B\n",
    "        if not encoder_padding_mask.any():\n",
    "            encoder_padding_mask = None\n",
    "\n",
    "        # 转置：B x T x C -> T x B x C\n",
    "        x = x.transpose(0, 1)\n",
    "\n",
    "        residuals = [x]\n",
    "        # 多层的CNN 层叠起来\n",
    "        for proj, conv, res_layer in zip(self.projections, self.convolutions, self.residuals):\n",
    "            if res_layer > 0:\n",
    "                residual = residuals[-res_layer]\n",
    "                residual = residual if proj is None else proj(residual)\n",
    "            else:\n",
    "                residual = None\n",
    "\n",
    "            if encoder_padding_mask is not None:\n",
    "                x = x.masked_fill(encoder_padding_mask.unsqueeze(-1), 0)\n",
    "\n",
    "            x = F.dropout(x, p=self.dropout, training=self.training)\n",
    "            if conv.kernel_size[0] % 2 == 1:\n",
    "                # padding is implicit in the conv\n",
    "                x = conv(x)\n",
    "            else:\n",
    "                padding_l = (conv.kernel_size[0] - 1) // 2\n",
    "                padding_r = conv.kernel_size[0] // 2\n",
    "                x = F.pad(x, (0, 0, 0, 0, padding_l, padding_r))\n",
    "                x = conv(x)\n",
    "            # GLU 层\n",
    "            x = F.glu(x, dim=2)\n",
    "            \n",
    "            # 残差层\n",
    "            if residual is not None:\n",
    "                x = (x + residual) * math.sqrt(0.5)\n",
    "            residuals.append(x)\n",
    "\n",
    "        # T x B x C -> B x T x C\n",
    "        x = x.transpose(1, 0)\n",
    "\n",
    "        # 将x映射回词向量空间 R^d -> R^f\n",
    "        x = self.fc2(x)\n",
    "\n",
    "        if encoder_padding_mask is not None:\n",
    "            encoder_padding_mask = encoder_padding_mask.t()  # -> B x T\n",
    "            x = x.masked_fill(encoder_padding_mask.unsqueeze(-1), 0)\n",
    "\n",
    "        # 将gradient放大\n",
    "        x = GradMultiply.apply(x, 1.0 / (2.0 * self.num_attention_layers))\n",
    "\n",
    "        # 把input embedding加到output中\n",
    "        y = (x + input_embedding) * math.sqrt(0.5)\n",
    "\n",
    "        return {\n",
    "            'encoder_out': (x, y),\n",
    "            'encoder_padding_mask': encoder_padding_mask,  # B x T\n",
    "        }\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.3 代码讲解\n",
    "#### \\[稀牛学院 x 网易云课程\\]《AI工程师(自然语言处理)》by 胡俊杰\n",
    "- 解码器decoder\n",
    "\n",
    "```python\n",
    "class FConvDecoder(FairseqIncrementalDecoder):\n",
    "    def __init__(self,...):\n",
    "\n",
    "        # 定义词向量矩阵及位置向量矩阵\n",
    "        self.embed_tokens = Embedding(num_embeddings, embed_dim, padding_idx)\n",
    "        self.embed_positions = PositionalEmbedding(\n",
    "            max_positions,\n",
    "            embed_dim,\n",
    "            padding_idx,\n",
    "            left_pad=self.left_pad,\n",
    "        ) if positional_embeddings else None\n",
    "        \n",
    "        convolutions = extend_conv_spec(convolutions)\n",
    "        in_channels = convolutions[0][0]\n",
    "        \n",
    "        self.fc1 = Linear(embed_dim, in_channels, dropout=dropout)\n",
    "        self.projections = nn.ModuleList()\n",
    "        self.convolutions = nn.ModuleList()\n",
    "        self.attention = nn.ModuleList()\n",
    "        self.residuals = []\n",
    "        \n",
    "        # 定义多层CNN\n",
    "        layer_in_channels = [in_channels]\n",
    "        for i, (out_channels, kernel_size, residual) in enumerate(convolutions):\n",
    "            if residual == 0:\n",
    "                residual_dim = out_channels\n",
    "            else:\n",
    "                residual_dim = layer_in_channels[-residual]\n",
    "            self.projections.append(Linear(residual_dim, out_channels)\n",
    "                                    if residual_dim != out_channels else None)\n",
    "            self.convolutions.append(\n",
    "                LinearizedConv1d(in_channels, out_channels * 2, kernel_size,\n",
    "                                 padding=(kernel_size - 1), dropout=dropout)\n",
    "            )\n",
    "            self.attention.append(AttentionLayer(out_channels, embed_dim)\n",
    "                                  if attention[i] else None)\n",
    "            self.residuals.append(residual)\n",
    "            in_channels = out_channels\n",
    "            layer_in_channels.append(out_channels)\n",
    "\n",
    "        self.adaptive_softmax = None\n",
    "        self.fc2 = self.fc3 = None\n",
    "\n",
    "    def forward(self, prev_output_tokens, encoder_out_dict=None, incremental_state=None):\n",
    "        ...\n",
    "        # 获得位置向量\n",
    "        if self.embed_positions is not None:\n",
    "            pos_embed = self.embed_positions(prev_output_tokens, incremental_state)\n",
    "        else:\n",
    "            pos_embed = 0\n",
    "\n",
    "        # 获得上一个生成的单词的词向量\n",
    "        x = self._embed_tokens(prev_output_tokens, incremental_state)\n",
    "\n",
    "        # 将词向量加上位置向量作为当前时刻的输入\n",
    "        x += pos_embed\n",
    "        x = F.dropout(x, p=self.dropout, training=self.training)\n",
    "        target_embedding = x\n",
    "\n",
    "        # 将输入从词向量空间映射到CNN输入空间\n",
    "        x = self.fc1(x)\n",
    "\n",
    "        # 转置：B x T x C -> T x B x C\n",
    "        x = self._transpose_if_training(x, incremental_state)\n",
    "\n",
    "        # 多层的CNN 堆叠\n",
    "        avg_attn_scores = None\n",
    "        num_attn_layers = len(self.attention)\n",
    "        residuals = [x]\n",
    "        for proj, conv, attention, res_layer in zip(self.projections, self.convolutions, self.attention,\n",
    "                                                    self.residuals):\n",
    "            if res_layer > 0:\n",
    "                residual = residuals[-res_layer]\n",
    "                residual = residual if proj is None else proj(residual)\n",
    "            else:\n",
    "                residual = None\n",
    "\n",
    "            x = F.dropout(x, p=self.dropout, training=self.training)\n",
    "            x = conv(x, incremental_state)\n",
    "            x = F.glu(x, dim=2)\n",
    "\n",
    "            # 注意力机制\n",
    "            if attention is not None:\n",
    "                x = self._transpose_if_training(x, incremental_state)\n",
    "\n",
    "                x, attn_scores = attention(x, target_embedding, (encoder_a, encoder_b), encoder_padding_mask)\n",
    "\n",
    "                if not self.training and self.need_attn:\n",
    "                    attn_scores = attn_scores / num_attn_layers\n",
    "                    if avg_attn_scores is None:\n",
    "                        avg_attn_scores = attn_scores\n",
    "                    else:\n",
    "                        avg_attn_scores.add_(attn_scores)\n",
    "\n",
    "                x = self._transpose_if_training(x, incremental_state)\n",
    "\n",
    "            # 残差连接\n",
    "            if residual is not None:\n",
    "                x = (x + residual) * math.sqrt(0.5)\n",
    "            residuals.append(x)\n",
    "\n",
    "        # 转置：T x B x C -> B x T x C\n",
    "        x = self._transpose_if_training(x, incremental_state)\n",
    "\n",
    "        # fc2:将输入映射到词表大小空间，可进行预测\n",
    "        if self.fc2 is not None and self.fc3 is not None:\n",
    "            x = self.fc2(x)\n",
    "            x = F.dropout(x, p=self.dropout, training=self.training)\n",
    "            x = self.fc3(x)\n",
    "\n",
    "        return x, avg_attn_scores\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.3 代码讲解\n",
    "#### \\[稀牛学院 x 网易云课程\\]《AI工程师(自然语言处理)》by 胡俊杰\n",
    "```python\n",
    "def main(args, init_distributed=False):\n",
    "    ...\n",
    "\n",
    "    # 载入数据\n",
    "    load_dataset_splits(task, ['train', 'valid'])\n",
    "\n",
    "    # 构建模型及优化函数\n",
    "    model = task.build_model(args)\n",
    "    criterion = task.build_criterion(args)\n",
    "\n",
    "    # 构建训练器 trainer\n",
    "    trainer = Trainer(args, task, model, criterion, dummy_batch, oom_batch)\n",
    "\n",
    "    # 初始化dataloader\n",
    "    epoch_itr = task.get_batch_iterator(...)\n",
    "\n",
    "    # 训练一直到learning rate太小就停止\n",
    "    max_epoch = args.max_epoch or math.inf\n",
    "    max_update = args.max_update or math.inf\n",
    "    lr = trainer.get_lr()\n",
    "    train_meter = StopwatchMeter()\n",
    "    train_meter.start()\n",
    "    while lr > args.min_lr and epoch_itr.epoch < max_epoch and trainer.get_num_updates() < max_update:\n",
    "        # 训练一个epoch\n",
    "        train(args, trainer, task, epoch_itr)\n",
    "\n",
    "        if epoch_itr.epoch % args.validate_interval == 0:\n",
    "            valid_losses = validate(args, trainer, task, epoch_itr, valid_subsets)\n",
    "\n",
    "        # 只用第一个validation loss去更新learning rate\n",
    "        lr = trainer.lr_step(epoch_itr.epoch, valid_losses[0])\n",
    "\n",
    "        # 保存模型\n",
    "        if epoch_itr.epoch % args.save_interval == 0:\n",
    "            save_checkpoint(args, trainer, epoch_itr, valid_losses[0])\n",
    "    train_meter.stop()\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 本节小结\n",
    "- 基于CNN的翻译系统模型架构\n",
    "    - Pooling Encoder\n",
    "    - Convolution Encoder\n",
    "    - Convolution NMT\n",
    "    - 对比CNN与RNN去构建的 encoder-decoder模型，分析CNN的优缺点\n",
    "- 使用CNN完成神经翻译系统的Trick\n",
    "    - 对模型某些部分做缩放（scaling）\n",
    "    - 对模型参数的初始化\n",
    "    - 对超参数的选择\n",
    "- 【实战】facebook CNN机器翻译系统代码解析\n",
    "    - 举例训练，及测试 CNN翻译系统\n",
    "    - 分析了 FconvModel, FconvEncoder, FconvDecoder\n",
    "    - 分析了 main 函数中训练模型部分"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 版权归 © 稀牛学院 所有 保留所有权利\n",
    "![](./img/xiniu_neteasy.png)"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "celltoolbar": "Slideshow",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
