{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](./img/NLP_banner.jpg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 基于DRMM的问答匹配\n",
    "#### [稀牛学院 x 网易云课程]《AI工程师(自然语言处理方向)》课程资料 by 褚则伟(zeweichu@gmail.com)\n",
    "\n",
    "DRRM模型我们参考[MatchZoo](https://github.com/NTMC-Community/MatchZoo)的实现"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "我们略过文本的预处理，训练和预测的代码，直接阅读模型的代码。模型使用Keras框架实现。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```\n",
    "\n",
    "\"\"\"An implementation of DRMM Model.\"\"\"\n",
    "import typing\n",
    "\n",
    "import keras\n",
    "import keras.backend as K\n",
    "\n",
    "from matchzoo import engine\n",
    "\n",
    "\n",
    "class DRMM(engine.BaseModel):\n",
    "    \"\"\"\n",
    "    DRMM Model.\n",
    "\n",
    "    Examples:\n",
    "        >>> model = DRMM()\n",
    "        >>> model.params['mlp_num_layers'] = 1\n",
    "        >>> model.params['mlp_num_units'] = 5\n",
    "        >>> model.params['mlp_num_fan_out'] = 1\n",
    "        >>> model.params['mlp_activation_func'] = 'tanh'\n",
    "        >>> model.guess_and_fill_missing_params(verbose=0)\n",
    "        >>> model.build()\n",
    "        >>> model.compile()\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    @classmethod\n",
    "    def get_default_params(cls) -> engine.ParamTable:\n",
    "        \"\"\":return: model default parameters.\"\"\"\n",
    "        params = super().get_default_params(with_embedding=True,\n",
    "                                            with_multi_layer_perceptron=True)\n",
    "        params.add(engine.Param(name='mask_value', value=-1,\n",
    "                                desc=\"The value to be masked from inputs.\"))\n",
    "        params['optimizer'] = 'adam'\n",
    "        params['input_shapes'] = [(5,), (5, 30,)]\n",
    "        return params\n",
    "\n",
    "    def build(self):\n",
    "        \"\"\"Build model structure.\"\"\"\n",
    "\n",
    "        # Scalar dimensions referenced here:\n",
    "        #   B = batch size (number of sequences)\n",
    "        #   D = embedding size\n",
    "        #   L = `input_left` sequence length\n",
    "        #   R = `input_right` sequence length\n",
    "        #   H = histogram size\n",
    "        #   K = size of top-k\n",
    "\n",
    "        # Left input and right input.\n",
    "        # query: shape = [B, L]\n",
    "        # doc: shape = [B, L, H]\n",
    "        # Note here, the doc is the matching histogram between original query\n",
    "        # and original document.\n",
    "        query = keras.layers.Input(\n",
    "            name='text_left',\n",
    "            shape=self._params['input_shapes'][0]\n",
    "        )\n",
    "        match_hist = keras.layers.Input(\n",
    "            name='match_histogram',\n",
    "            shape=self._params['input_shapes'][1]\n",
    "        )\n",
    "\n",
    "        embedding = self._make_embedding_layer()\n",
    "        # Process left input.\n",
    "        # shape = [B, L, D]\n",
    "        embed_query = embedding(query)\n",
    "        # shape = [B, L]\n",
    "        atten_mask = K.any(K.not_equal(query, self._params['mask_value']),\n",
    "                           axis=-1, keepdims=True)\n",
    "        atten_mask = K.cast(atten_mask, K.floatx())\n",
    "        atten_mask = K.expand_dims(atten_mask, axis=2)\n",
    "        # shape = [B, L, D]\n",
    "        attention_probs = self.attention_layer(embed_query, atten_mask)\n",
    "\n",
    "        # Process right input.\n",
    "        # shape = [B, L, 1]\n",
    "        dense_output = self._make_multi_layer_perceptron_layer()(match_hist)\n",
    "\n",
    "        # shape = [B, 1, 1]\n",
    "        dot_score = keras.layers.Dot(axes=[1, 1])(\n",
    "            [attention_probs, dense_output])\n",
    "\n",
    "        flatten_score = keras.layers.Flatten()(dot_score)\n",
    "\n",
    "        x_out = self._make_output_layer()(flatten_score)\n",
    "        self._backend = keras.Model(inputs=[query, match_hist], outputs=x_out)\n",
    "\n",
    "    @classmethod\n",
    "    def attention_layer(cls, attention_input: typing.Any,\n",
    "                        attention_mask: typing.Any = None\n",
    "                        ) -> keras.layers.Layer:\n",
    "        \"\"\"\n",
    "        Performs attention on the input.\n",
    "\n",
    "        :param attention_input: The input tensor for attention layer.\n",
    "        :param attention_mask: A tensor to mask the invalid values.\n",
    "        :return: The masked output tensor.\n",
    "        \"\"\"\n",
    "        # shape = [B, L, 1]\n",
    "        dense_input = keras.layers.Dense(1, use_bias=False)(attention_input)\n",
    "        if attention_mask is not None:\n",
    "            # Since attention_mask is 1.0 for positions we want to attend and\n",
    "            # 0.0 for masked positions, this operation will create a tensor\n",
    "            # which is 0.0 for positions we want to attend and -10000.0 for\n",
    "            # masked positions.\n",
    "\n",
    "            # shape = [B, L, 1]\n",
    "            dense_input = keras.layers.Lambda(\n",
    "                lambda x: x + (1.0 - attention_mask) * -10000.0,\n",
    "                name=\"attention_mask\"\n",
    "            )(dense_input)\n",
    "        # shape = [B, L, 1]\n",
    "        attention_probs = keras.layers.Lambda(\n",
    "            lambda x: keras.layers.activations.softmax(x, axis=1),\n",
    "            output_shape=lambda s: (s[0], s[1], s[2]),\n",
    "            name=\"attention_probs\"\n",
    "        )(dense_input)\n",
    "        return attention_probs\n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 版权归 © 稀牛学院 所有 保留所有权利\n",
    "![](./img/xiniu_neteasy.png)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
