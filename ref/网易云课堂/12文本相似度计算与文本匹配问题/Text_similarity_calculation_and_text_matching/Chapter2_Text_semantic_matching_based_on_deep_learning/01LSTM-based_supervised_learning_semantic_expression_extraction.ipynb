{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](./img/NLP_banner.jpg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 基于LSTM的监督学习语义表达抽取\n",
    "## InferSent\n",
    "\n",
    "#### [稀牛学院 x 网易云课程]《AI工程师(自然语言处理方向)》课程资料整理 by 褚则伟(zeweichu@gmail.com)¶"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[InferSent](https://github.com/facebookresearch/InferSent)的官方代码可以从GitHub上找到。\n",
    "\n",
    "我们这里省略数据预处理和训练的环节，只看模型的定义部分。模型利用PyTorch实现。\n",
    "\n",
    "<img src=\"./img/snli.png\" alt=\"drawing\" width=\"300\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```\n",
    "\n",
    "\"\"\"\n",
    "Main module for Natural Language Inference\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "class NLINet(nn.Module):\n",
    "    def __init__(self, config):\n",
    "        super(NLINet, self).__init__()\n",
    "\n",
    "        # classifier\n",
    "        self.nonlinear_fc = config['nonlinear_fc']\n",
    "        self.fc_dim = config['fc_dim']\n",
    "        self.n_classes = config['n_classes']\n",
    "        self.enc_lstm_dim = config['enc_lstm_dim']\n",
    "        self.encoder_type = config['encoder_type']\n",
    "        self.dpout_fc = config['dpout_fc']\n",
    "\n",
    "        self.encoder = eval(self.encoder_type)(config)\n",
    "        self.inputdim = 4*2*self.enc_lstm_dim\n",
    "        self.inputdim = 4*self.inputdim if self.encoder_type in \\\n",
    "                        [\"ConvNetEncoder\", \"InnerAttentionMILAEncoder\"] else self.inputdim\n",
    "        self.inputdim = self.inputdim/2 if self.encoder_type == \"LSTMEncoder\" \\\n",
    "                                        else self.inputdim\n",
    "        if self.nonlinear_fc: # 非线性的神经网络分类器\n",
    "            self.classifier = nn.Sequential(\n",
    "                nn.Dropout(p=self.dpout_fc),\n",
    "                nn.Linear(self.inputdim, self.fc_dim),\n",
    "                nn.Tanh(),\n",
    "                nn.Dropout(p=self.dpout_fc),\n",
    "                nn.Linear(self.fc_dim, self.fc_dim),\n",
    "                nn.Tanh(),\n",
    "                nn.Dropout(p=self.dpout_fc),\n",
    "                nn.Linear(self.fc_dim, self.n_classes),\n",
    "                )\n",
    "        else: # 线性神经网络分类器\n",
    "            self.classifier = nn.Sequential(\n",
    "                nn.Linear(self.inputdim, self.fc_dim),\n",
    "                nn.Linear(self.fc_dim, self.fc_dim),\n",
    "                nn.Linear(self.fc_dim, self.n_classes)\n",
    "                )\n",
    "\n",
    "    def forward(self, s1, s2):\n",
    "        # s1 : (s1, s1_len)\n",
    "        u = self.encoder(s1) # 编码句子1\n",
    "        v = self.encoder(s2) # 编码句子2\n",
    "\n",
    "        features = torch.cat((u, v, torch.abs(u-v), u*v), 1) # feature engineering\n",
    "        output = self.classifier(features) # 分类\n",
    "        return output\n",
    "\n",
    "    def encode(self, s1):\n",
    "        emb = self.encoder(s1)\n",
    "        return emb\n",
    "    \n",
    "    \n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```\n",
    "\n",
    "\"\"\"\n",
    "LSTM encoder\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "class LSTMEncoder(nn.Module):\n",
    "    def __init__(self, config):\n",
    "        super(LSTMEncoder, self).__init__()\n",
    "        self.bsize = config['bsize']\n",
    "        self.word_emb_dim = config['word_emb_dim']\n",
    "        self.enc_lstm_dim = config['enc_lstm_dim']\n",
    "        self.pool_type = config['pool_type']\n",
    "        self.dpout_model = config['dpout_model']\n",
    "\n",
    "        self.enc_lstm = nn.LSTM(self.word_emb_dim, self.enc_lstm_dim, 1,\n",
    "                                bidirectional=False, dropout=self.dpout_model)\n",
    "\n",
    "    def forward(self, sent_tuple):\n",
    "        # sent_len [max_len, ..., min_len] (batch)\n",
    "        # sent (seqlen x batch x worddim)\n",
    "\n",
    "        sent, sent_len = sent_tuple\n",
    "\n",
    "        # 按照句子的长短排序，并保留原始的idx顺序\n",
    "        sent_len, idx_sort = np.sort(sent_len)[::-1], np.argsort(-sent_len)\n",
    "        sent = sent.index_select(1, torch.cuda.LongTensor(idx_sort))\n",
    "\n",
    "        # 用pytorch自带的函数处理RNN的padding问题\n",
    "        sent_packed = nn.utils.rnn.pack_padded_sequence(sent, sent_len)\n",
    "        # LSTM编码序列\n",
    "        sent_output = self.enc_lstm(sent_packed)[1][0].squeeze(0)  # batch x 2*nhid\n",
    "\n",
    "        # 把句子返回原来的顺序\n",
    "        idx_unsort = np.argsort(idx_sort)\n",
    "        emb = sent_output.index_select(0, torch.cuda.LongTensor(idx_unsort))\n",
    "\n",
    "        return emb\n",
    "    \n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 版权归 © 稀牛学院 所有 保留所有权利\n",
    "![](./img/xiniu_neteasy.png)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
