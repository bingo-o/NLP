{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "AAqg8rR7DuBL"
   },
   "source": [
    "![](./img/NLP_banner.jpg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "B1UdQMBlDuBM"
   },
   "source": [
    "# seq2seq项目说明\n",
    "#### \\[稀牛学院 x 网易云课程\\]《AI工程师(自然语言处理方向)》课程资料 by [@寒小阳](https://blog.csdn.net/han_xiaoyang)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "C7Zpyq8_DuBO"
   },
   "source": [
    "### 1.seq2seq（序列到序列模型）应用\n",
    "在**聊天机器人，机器翻译，自动文摘，智能问答**等众多自然语言处理任务中都可能用到seq2seq模型，google在著名的[neural machine translation](https://arxiv.org/abs/1609.08144)中也使用过这个结构的模型(当然，现在因为效率等原因，可能不少应用项目迁移到transformer结构下了)，google在tensorflow的官方案例中给了一个[手把手训练seq2seq神经翻译系统](https://github.com/tensorflow/nmt)的github项目，下面我们就官方这个项目讲解一下如何应用tensorflow训练seq2seq的应用，并尝试用这个代码实现一个聊天机器人的智能AI应用。\n",
    "\n",
    "参考资料:\n",
    "* [neural machine translation](https://arxiv.org/abs/1609.08144)\n",
    "* [手把手训练seq2seq神经翻译系统](https://github.com/tensorflow/nmt)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "QYBHkefTDuBO"
   },
   "source": [
    "## 0.说明"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "SHMvr8yXDuBP"
   },
   "source": [
    "google的这个教程使用高版本tensorflow（TensorFlow 1.2+）的 seq2seq API完成，该API使seq2seq模型的构建过程干净、简单、易读，主要包括以下内容：\n",
    "\n",
    "- 使用 tf.data 中最新输入的管道对动态调整的输入序列进行预处理。\n",
    "- 使用批量填充和序列长度 bucketing，提高训练速度和推理速度。\n",
    "- 使用通用结构和训练时间表训练 seq2seq 模型，包括多种注意力机制和固定抽样。\n",
    "- 使用 in-graph 集束搜索在 seq2seq 模型中进行推理。\n",
    "- 优化 seq2seq 模型，以实现在多 GPU 设置中的模型训练。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "tgrHAauPDuBQ"
   },
   "source": [
    "## 1.引言\n",
    "\n",
    "序列到序列（seq2seq）模型（Sutskever et al., 2014, Cho et al., 2014）在机器翻译、语音识别和文本摘要等任务上取得了巨大的成功。这里的教程内容致力于帮助读者全面掌握 seq2seq 模型，并且展示了如何从头开始构建一个强大的 seq2seq 模型。以下的讲解会注重神经机器翻译（NMT）任务，神经机器翻译是 seq2seq 模型很好的试验台，并且已经获得了广泛的成功。我们使用的代码是极其轻量、高质量、可投入生产并且结合了最新研究思路的实现。我们通过以下方式实现这一目标：\n",
    "\n",
    "1. 使用最新的解码器/attention wrapper API、TensorFlow 高版本数据迭代器。\n",
    "2. 结合了我们在构建循环型和 seq2seq 型模型的专业知识。\n",
    "3. 提供了可构建最好 NMT 模型的技巧，同时还复现了谷歌的 NMT（GNMT）系统。\n",
    "\n",
    "我们相信提供所有人都很容易复制的基准是非常重要的。因此，我们基于以下公开的数据集提供了全部的试验结果和预训练模型：\n",
    "\n",
    "1. 小规模数据集：TED 演讲的英语-越南语平行语料库（133K 个句子对），该数据集由 IWSLT Evaluation Campaign 提供。\n",
    "2. 大规模数据集：德语-英语平行语料库（4.5M 个句子对），该数据集由 WMT Evaluation Campaign 提供。\n",
    "\n",
    "我们首先需要了解用于 NMT 任务的 seq2seq 模型的基本知识，并需要理解如何构建和训练一个 vanilla NMT 模型。第二部分将更进一步详细地解释如何构建带注意力机制的强大神经机器翻译模型。然后我们会讨论构建更好神经机器翻译模型（翻译速度和质量）可能的技巧，例如 TensorFlow 最好的实践方法（batching, bucketing）、双向循环神经网络和集束搜索(beam search)等。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "RZeL0NssDuBR"
   },
   "source": [
    "## 1.基础\n",
    "\n",
    "**关于神经机器翻译**\n",
    "\n",
    "以词组为基础的传统翻译系统将源语言句子拆分成多个词块，然后进行词对词的翻译。这使得翻译输出结果流畅性大打折扣，远远不如人类译文。我们会通读整个源语言句子、了解句子含义，然后输出翻译结果。神经机器翻译（NMT）竟然可以模仿人类的翻译过程！\n",
    "\n",
    "![img](https://github.com/tensorflow/nmt/raw/master/nmt/g3doc/img/encdec.jpg)\n",
    "\n",
    "<center>图1. 编码器-解码器结构——神经机器翻译的通用方法实例。</center>\n",
    "\n",
    "具体来说，神经机器翻译系统首先使用编码器读取源语言句子，构建一个「上下文」向量(context vector)，即代表句义的数字化向量；然后使用解码器处理该内容，并输出翻译结果，如图1所示。这就是我们通常所说的编码器-解码器结构。神经机器翻译用这种方法解决以词组为基础的传统翻译系统遇到的翻译问题：神经机器翻译能够捕捉语言中的长距离依赖结构，如词性一致、句法结构等，然后输出流利度更高的翻译结果，正如谷歌神经机器翻译系统已经做到的那样。\n",
    "\n",
    "NMT 模型在具体的结构中会发生变化。对于序列数据而言，最好的选择是循环神经网络（RNN），这也被大多数 NMT 模型采用。通常情况下，编码器和解码器都可使用循环神经网络。但是，循环神经网络模型有多种选择：\n",
    "* （a）方向性（directionality），单向或双向；\n",
    "* （b）深度，单层或多层；\n",
    "* （c）类型，通常是 vanilla RNN、长短期记忆（Long Short-term Memory，LSTM），或门控循环单元（gated recurrent unit，GRU）。\n",
    "\n",
    "感兴趣的同学可打开该网址(https://colah.github.io/posts/2015-08-Understanding-LSTMs/) ， 复习一下RNN 和 LSTM 的更多信息。\n",
    "\n",
    "这个教程中，我们将以单向的深度多层 RNN（deep multi-layer RNN）为例，它使用 LSTM 作为循环单元。模型实例如图 2 所示。我们在该实例中构建了一个模型，将源语言句子「I am a student」翻译成目标语言「Je suis étudiant」。该 NMT 模型包括两个循环神经网络：编码器 RNN，在不预测的情况下将输入的源语言单词进行编码；解码器，在预测下一个单词的条件下处理目标句子。\n",
    "\n",
    "若想参考更多信息，请查看论文 Luong（2016）（https://github.com/lmthang/thesis）。\n",
    "\n",
    "![img](https://github.com/tensorflow/nmt/raw/master/nmt/g3doc/img/seq2seq.jpg)\n",
    "\n",
    "<center>图2. 神经机器翻译——一个深度循环结构实例</center>\n",
    "\n",
    "上图将源语言句子「I am a student」翻译成目标语言句子「Je suis étudiant」。此处，`「<s>」`代表解码过程开始，`「</s>」`代表解码过程结束。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "RCuPESoMDuBS"
   },
   "source": [
    "## 2.代码准备\n",
    "\n",
    "为了安装该教程，我们需要先安装 TensorFlow。建议使用新版本的TensorFlow。为了安装 TensorFlow，请按照以下安装指导：https://www.tensorflow.org/install/。\n",
    "\n",
    "在安装 TensorFlow 之后，我们需要运行以下命令拉取这个教程的源代码：\n",
    "\n",
    "```\n",
    "git clone https://github.com/tensorflow/nmt/\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 102
    },
    "colab_type": "code",
    "id": "M-1M2HgvDuBS",
    "outputId": "2d0d456a-0219-49f2-f680-5c7f4d25c654"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cloning into 'nmt'...\n",
      "remote: Enumerating objects: 1247, done.\u001b[K\n",
      "Receiving objects:   0% (1/1247)   \r",
      "Receiving objects:   1% (13/1247)   \r",
      "Receiving objects:   2% (25/1247)   \r",
      "Receiving objects:   3% (38/1247)   \r",
      "Receiving objects:   4% (50/1247)   \r",
      "Receiving objects:   5% (63/1247)   \r",
      "Receiving objects:   6% (75/1247)   \r",
      "Receiving objects:   7% (88/1247)   \r",
      "Receiving objects:   8% (100/1247)   \r",
      "Receiving objects:   9% (113/1247)   \r",
      "Receiving objects:  10% (125/1247)   \r",
      "Receiving objects:  11% (138/1247)   \r",
      "Receiving objects:  12% (150/1247)   \r",
      "Receiving objects:  13% (163/1247)   \r",
      "Receiving objects:  14% (175/1247)   \r",
      "Receiving objects:  15% (188/1247)   \r",
      "Receiving objects:  16% (200/1247)   \r",
      "Receiving objects:  17% (212/1247)   \r",
      "Receiving objects:  18% (225/1247)   \r",
      "Receiving objects:  19% (237/1247)   \r",
      "Receiving objects:  20% (250/1247)   \r",
      "Receiving objects:  21% (262/1247)   \r",
      "Receiving objects:  22% (275/1247)   \r",
      "Receiving objects:  23% (287/1247)   \r",
      "Receiving objects:  24% (300/1247)   \r",
      "Receiving objects:  25% (312/1247)   \r",
      "Receiving objects:  26% (325/1247)   \r",
      "Receiving objects:  27% (337/1247)   \r",
      "Receiving objects:  28% (350/1247)   \r",
      "Receiving objects:  29% (362/1247)   \r",
      "Receiving objects:  30% (375/1247)   \r",
      "Receiving objects:  31% (387/1247)   \r",
      "Receiving objects:  32% (400/1247)   \r",
      "Receiving objects:  33% (412/1247)   \r",
      "Receiving objects:  34% (424/1247)   \r",
      "Receiving objects:  35% (437/1247)   \r",
      "Receiving objects:  36% (449/1247)   \r",
      "Receiving objects:  37% (462/1247)   \r",
      "Receiving objects:  38% (474/1247)   \r",
      "Receiving objects:  39% (487/1247)   \r",
      "Receiving objects:  40% (499/1247)   \r",
      "Receiving objects:  41% (512/1247)   \r",
      "Receiving objects:  42% (524/1247)   \r",
      "Receiving objects:  43% (537/1247)   \r",
      "Receiving objects:  44% (549/1247)   \r",
      "Receiving objects:  45% (562/1247)   \r",
      "Receiving objects:  46% (574/1247)   \r",
      "Receiving objects:  47% (587/1247)   \r",
      "Receiving objects:  48% (599/1247)   \r",
      "Receiving objects:  49% (612/1247)   \r",
      "Receiving objects:  50% (624/1247)   \r",
      "Receiving objects:  51% (636/1247)   \r",
      "Receiving objects:  52% (649/1247)   \r",
      "Receiving objects:  53% (661/1247)   \r",
      "Receiving objects:  54% (674/1247)   \r",
      "Receiving objects:  55% (686/1247)   \r",
      "Receiving objects:  56% (699/1247)   \r",
      "Receiving objects:  57% (711/1247)   \r",
      "Receiving objects:  58% (724/1247)   \r",
      "Receiving objects:  59% (736/1247)   \r",
      "Receiving objects:  60% (749/1247)   \r",
      "Receiving objects:  61% (761/1247)   \r",
      "Receiving objects:  62% (774/1247)   \r",
      "Receiving objects:  63% (786/1247)   \r",
      "Receiving objects:  64% (799/1247)   \r",
      "Receiving objects:  65% (811/1247)   \r",
      "Receiving objects:  66% (824/1247)   \r",
      "Receiving objects:  67% (836/1247)   \r",
      "Receiving objects:  68% (848/1247)   \r",
      "Receiving objects:  69% (861/1247)   \r",
      "Receiving objects:  70% (873/1247)   \r",
      "Receiving objects:  71% (886/1247)   \r",
      "Receiving objects:  72% (898/1247)   \r",
      "Receiving objects:  73% (911/1247)   \r",
      "Receiving objects:  74% (923/1247)   \r",
      "Receiving objects:  75% (936/1247)   \r",
      "Receiving objects:  76% (948/1247)   \r",
      "Receiving objects:  77% (961/1247)   \r",
      "Receiving objects:  78% (973/1247)   \r",
      "Receiving objects:  79% (986/1247)   \r",
      "Receiving objects:  80% (998/1247)   \r",
      "Receiving objects:  81% (1011/1247)   \r",
      "Receiving objects:  82% (1023/1247)   \r",
      "Receiving objects:  83% (1036/1247)   \r",
      "Receiving objects:  84% (1048/1247)   \r",
      "Receiving objects:  85% (1060/1247)   \r",
      "Receiving objects:  86% (1073/1247)   \r",
      "Receiving objects:  87% (1085/1247)   \r",
      "Receiving objects:  88% (1098/1247)   \r",
      "Receiving objects:  89% (1110/1247)   \r",
      "Receiving objects:  90% (1123/1247)   \r",
      "Receiving objects:  91% (1135/1247)   \r",
      "Receiving objects:  92% (1148/1247)   \r",
      "Receiving objects:  93% (1160/1247)   \r",
      "Receiving objects:  94% (1173/1247)   \r",
      "Receiving objects:  95% (1185/1247)   \r",
      "Receiving objects:  96% (1198/1247)   \r",
      "Receiving objects:  97% (1210/1247)   \r",
      "remote: Total 1247 (delta 0), reused 0 (delta 0), pack-reused 1247\u001b[K\n",
      "Receiving objects:  98% (1223/1247)   \r",
      "Receiving objects:  99% (1235/1247)   \r",
      "Receiving objects: 100% (1247/1247)   \r",
      "Receiving objects: 100% (1247/1247), 1.23 MiB | 15.70 MiB/s, done.\n",
      "Resolving deltas:   0% (0/890)   \r",
      "Resolving deltas:   2% (21/890)   \r",
      "Resolving deltas:   6% (60/890)   \r",
      "Resolving deltas:   9% (82/890)   \r",
      "Resolving deltas:  16% (143/890)   \r",
      "Resolving deltas:  17% (154/890)   \r",
      "Resolving deltas:  19% (175/890)   \r",
      "Resolving deltas:  20% (180/890)   \r",
      "Resolving deltas:  21% (187/890)   \r",
      "Resolving deltas:  23% (211/890)   \r",
      "Resolving deltas:  24% (215/890)   \r",
      "Resolving deltas:  25% (223/890)   \r",
      "Resolving deltas:  26% (235/890)   \r",
      "Resolving deltas:  29% (265/890)   \r",
      "Resolving deltas:  30% (268/890)   \r",
      "Resolving deltas:  31% (276/890)   \r",
      "Resolving deltas:  33% (301/890)   \r",
      "Resolving deltas:  37% (337/890)   \r",
      "Resolving deltas:  39% (355/890)   \r",
      "Resolving deltas:  40% (357/890)   \r",
      "Resolving deltas:  41% (371/890)   \r",
      "Resolving deltas:  43% (386/890)   \r",
      "Resolving deltas:  45% (405/890)   \r",
      "Resolving deltas:  53% (480/890)   \r",
      "Resolving deltas:  57% (516/890)   \r",
      "Resolving deltas:  61% (549/890)   \r",
      "Resolving deltas:  64% (573/890)   \r",
      "Resolving deltas:  67% (598/890)   \r",
      "Resolving deltas:  70% (624/890)   \r",
      "Resolving deltas:  71% (637/890)   \r",
      "Resolving deltas:  73% (653/890)   \r",
      "Resolving deltas:  74% (660/890)   \r",
      "Resolving deltas:  75% (675/890)   \r",
      "Resolving deltas:  76% (681/890)   \r",
      "Resolving deltas:  78% (695/890)   \r",
      "Resolving deltas:  82% (732/890)   \r",
      "Resolving deltas:  83% (742/890)   \r",
      "Resolving deltas:  84% (748/890)   \r",
      "Resolving deltas:  85% (761/890)   \r",
      "Resolving deltas:  86% (770/890)   \r",
      "Resolving deltas:  87% (775/890)   \r",
      "Resolving deltas:  88% (786/890)   \r",
      "Resolving deltas:  89% (797/890)   \r",
      "Resolving deltas:  90% (806/890)   \r",
      "Resolving deltas:  91% (810/890)   \r",
      "Resolving deltas:  92% (826/890)   \r",
      "Resolving deltas:  93% (832/890)   \r",
      "Resolving deltas:  95% (847/890)   \r",
      "Resolving deltas:  96% (856/890)   \r",
      "Resolving deltas:  97% (864/890)   \r",
      "Resolving deltas:  98% (875/890)   \r",
      "Resolving deltas:  99% (882/890)   \r",
      "Resolving deltas: 100% (890/890)   \r",
      "Resolving deltas: 100% (890/890), done.\n"
     ]
    }
   ],
   "source": [
    "!git clone https://github.com/tensorflow/nmt/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Jt-BxDEcDuBW"
   },
   "source": [
    "## 3.训练-构建我们第一个 NMT 系统\n",
    "\n",
    "我们首先需要了解构建一个 NMT 模型具体代码的核心，我们会在图 2 中更详细地讲解。我们后面会介绍数据准备和全部的代码，这一部分是指 model.py 文件。\n",
    "\n",
    "在网络的底层，编码器和解码器 RNN 接收到以下输入：首先是原句子，然后是从编码到解码模式的过渡边界符号`「<s>」`，最后是目标语句。对于训练来说，我们将为系统提供以下张量，它们是以时间为主（time-major）的格式，并包括了单词索引：\n",
    "\n",
    "- encoder_inputs [max_encoder_time, batch_size]：源输入词。\n",
    "- decoder_inputs [max_decoder_time, batch_size]：目标输入词。\n",
    "- decoder_outputs [max_decoder_time, batch_size]：目标输出词，这些是 decoder_inputs 按一个时间步向左移动，并且在右边有句子结束符。\n",
    "\n",
    "为了更高的效率，我们一次用多个句子（batch_size）进行训练。测试略有不同，我们会在后面讨论。\n",
    "\n",
    "### 3.1.嵌入(embedding)\n",
    "\n",
    "给定单词的分类属性，模型首先必须查找词来源和目标嵌入以检索相应的词表征。为了令该嵌入层能够运行，我们首先需要为每一种语言选定一个词汇表。通常，选定词汇表大小 V，那么频率最高的 V 个词将视为唯一的。而所有其他的词将转换并打上「unknown」标志，因此所有的词将有相同的嵌入。我们通常在训练期间嵌入权重，并且每种语言都有一套。\n",
    "\n",
    "```python\n",
    "# Embedding\n",
    "embedding_encoder = variable_scope.get_variable(\n",
    "    \"embedding_encoder\", [src_vocab_size, embedding_size], ...)# Look up embedding:#   encoder_inputs: [max_time, batch_size]#   encoder_emp_inp: [max_time, batch_size, embedding_size]\n",
    "encoder_emb_inp = embedding_ops.embedding_lookup(\n",
    "    embedding_encoder, encoder_inputs)\n",
    "```\n",
    "\n",
    "我们同样可以构建 embedding_decoder 和 decoder_emb_inp。注意我们可以选择预训练的词表征如 word2vec 或 Glove vectors 初始化嵌入权重。通常给定大量的训练数据，我们能从头学习这些嵌入权重。\n",
    "\n",
    "### 3.2.编码器(encoder)\n",
    "\n",
    "一旦可以检索到，词嵌入就能作为输入馈送到主神经网络中。该网络有两个多层循环神经网络组成，一个是原语言的编码器，另一个是目标语言的解码器。这两个 RNN 原则上可以共享相同的权重，然而在实践中，我们通常使用两组不同的循环神经网络参数（这些模型在拟合大型训练数据集上做得更好）。解码器 RNN 使用零向量作为它的初始状态，并且可以使用如下代码构建：\n",
    "\n",
    "```python\n",
    "# Build RNN cell\n",
    "encoder_cell = tf.nn.rnn_cell.BasicLSTMCell(num_units)\n",
    "# Run Dynamic RNN#   encoder_outpus: [max_time, batch_size, num_units]#   encoder_state: [batch_size, num_units]\n",
    "encoder_outputs, encoder_state = tf.nn.dynamic_rnn(\n",
    "    encoder_cell, encoder_emb_inp,\n",
    "    sequence_length=source_seqence_length, time_major=True)\n",
    "```\n",
    "\n",
    "注意语句有不同的长度以避免浪费计算力，因此我们会通过 source_seqence_length 告诉 dynamic_rnn 精确的句子长度。因为我们的输入是以时间为主（time major）的，我们需要设定 time_major=True。现在我们暂时只需要构建单层 LSTM、encoder_cell。我们后面会详细描述怎样构建多层 LSTM、添加 dropout 并使用注意力机制。\n",
    "\n",
    "### 3.3.解码器(decoder)\n",
    "\n",
    "decoder 也需要访问源信息，一种简单的方式是用编码器最后的隐藏态 encoder_state 对其进行初始化。在图 2 中，我们将源词「student」中的隐藏态传递到了解码器。\n",
    "\n",
    "```python\n",
    "# Build RNN cell\n",
    "decoder_cell = tf.nn.rnn_cell.BasicLSTMCell(num_units)\n",
    "```\n",
    "\n",
    "```python\n",
    "# Helper\n",
    "helper = tf.contrib.seq2seq.TrainingHelper(\n",
    "    decoder_emb_inp, decoder_lengths, time_major=True)# Decoder\n",
    "decoder = tf.contrib.seq2seq.BasicDecoder(\n",
    "    decoder_cell, helper, encoder_state,\n",
    "    output_layer=projection_layer)# Dynamic decoding\n",
    "outputs, _ = tf.contrib.seq2seq.dynamic_decode(decoder, ...)\n",
    "logits = outputs.rnn_output\n",
    "```\n",
    "\n",
    "此处代码的核心是 BasicDecoder、获取 decoder_cell(类似于 encoder_cell) 的 decoder、helper 以及之前作为输入的 encoder_state。\n",
    "\n",
    "通过分离 decoders 和 helpers，我们能重复使用不同的代码库，例如 TrainingHelper 可由 GreedyEmbeddingHelper 进行替换，来做贪婪解码。\n",
    "\n",
    "最后，我们从未提到过的 projection_layer 是一个密集矩阵，将顶部的隐藏态转变为维度 V 的逻辑向量。我们在图 2 的上部展示了此过程。\n",
    "\n",
    "```python\n",
    "projection_layer = layers_core.Dense(\n",
    "    tgt_vocab_size, use_bias=False)\n",
    "```\n",
    "\n",
    "### 3.4.损失构建\n",
    "\n",
    "给出以上的 logits，可计算训练损失：\n",
    "\n",
    "```python\n",
    "crossent = tf.nn.sparse_softmax_cross_entropy_with_logits(\n",
    "    labels=decoder_outputs, logits=logits)\n",
    "train_loss = (tf.reduce_sum(crossent * target_weights) /\n",
    "    batch_size)\n",
    "```\n",
    "\n",
    "以上代码中，target_weights 是一个与 decoder_outputs 大小一样的 0-1 矩阵。该矩阵将目标序列长度以外的其他位置填充为标量值 0。\n",
    "\n",
    "我们需要指出来的是，训练损失可以由 batch_size 分割，因此我们的超参数 batch_size 是「不变量」。也有些人将训练损失按照 batch_size * num_time_steps 分割，这样可以减少短句所造成的误差。更巧妙的，我们的超参数（应用于前面的方法）不能用于后面的方法。例如，如果两种方法都是用学习率为 1.0 的随机梯度下降，后面的方法将更有效地利用一个较小的学习率，即 1 / num_time_steps。\n",
    "\n",
    "### 3.5.梯度计算和优化器优化\n",
    "\n",
    "现在是时候定义我们的 NMT 模型的前向传播了。计算反向传播只需要写几行代码：\n",
    "\n",
    "```python\n",
    "# Calculate and clip gradients\n",
    "parameters = tf.trainable_variables()\n",
    "gradients = tf.gradients(train_loss, params)\n",
    "clipped_gradients, _ = tf.clip_by_global_norm(\n",
    "    gradients, max_gradient_norm)\n",
    "```\n",
    "\n",
    "训练 RNN 的一个重要步骤是梯度截断（gradient clipping）。这里，我们使用全局范数进行截断操作。最大值 max_gradient_norm 通常设置为 5 或 1。最后一步是选择优化器。Adam 优化器是最常见的选择。我们还要选择一个学习率，learning_rate 的值通常在 0.0001 和 0.001 之间，且可设置为随着训练进程逐渐减小。\n",
    "\n",
    "```python\n",
    "# Optimization\n",
    "optimizer = tf.train.AdamOptimizer(learning_rate)\n",
    "update_step = optimizer.apply_gradients(\n",
    "    zip(clipped_gradients, params))\n",
    "```\n",
    "\n",
    "在我们的实验中，我们使用标准的随机梯度下降（tf.train.GradientDescentOptimizer），并采用了递减的学习率方案，因此也就有更好的性能。\n",
    "\n",
    "### 3.6 开始训练 NMT 模型\n",
    "\n",
    "让我们开始训练第一个 NMT 模型，将越南语翻译为英语。代码的入口是nmt.py。\n",
    "\n",
    "我们将使用小规模的 Ted 演讲并行语料库（133k 的训练样本）进行训练。所有的数据都可从以下链接找到：https://nlp.stanford.edu/projects/nmt/。\n",
    "\n",
    "我们将使用 tst2012 作为开发数据集，tst 2013 作为测试数据集。运行以下命令行下载数据训练 NMT 模型：\n",
    "\n",
    "```shell\n",
    "nmt/scripts/download_iwslt15.sh /tmp/nmt_data\n",
    "```\n",
    "\n",
    "运行以下命令行开始训练：\n",
    "\n",
    "```python\n",
    "mkdir /tmp/nmt_model\n",
    "python -m nmt.nmt \\\n",
    "    --src=vi --tgt=en \\\n",
    "    --vocab_prefix=/tmp/nmt_data/vocab  \\\n",
    "    --train_prefix=/tmp/nmt_data/train \\\n",
    "    --dev_prefix=/tmp/nmt_data/tst2012  \\\n",
    "    --test_prefix=/tmp/nmt_data/tst2013 \\\n",
    "    --out_dir=/tmp/nmt_model \\\n",
    "    --num_train_steps=12000 \\\n",
    "    --steps_per_stats=100 \\\n",
    "    --num_layers=2 \\\n",
    "    --num_units=128 \\\n",
    "    --dropout=0.2 \\\n",
    "    --metrics=bleu\n",
    "```\n",
    "\n",
    "以上命令行训练一个 2 层的 LSTM seq2seq 模型，带有 128-dim 的隐藏单元和 12 个 epochs 的嵌入。我们使用 0.2（或然率为 0.8）的 dropout 值。如果没误差，在我们训练中随着降低混淆度，我们应该能看到类似于以下的 logs。\n",
    "\n",
    "```python\n",
    "# First evaluation, global step 0\n",
    "  eval dev: perplexity 17193.66\n",
    "  eval test: perplexity 17193.27\n",
    "# Start epoch 0, step 0, lr 1, Tue Apr 25 23:17:41 2017\n",
    "  sample train data:\n",
    "    src_reverse: </s> </s> Điều đó , dĩ nhiên , là câu chuyện trích ra từ học thuyết của Karl Marx .\n",
    "    ref: That , of course , was the <unk> distilled from the theories of Karl Marx . </s> </s> </s>\n",
    "  epoch 0 step 100 lr 1 step-time 0.89s wps 5.78K ppl 1568.62 bleu 0.00\n",
    "  epoch 0 step 200 lr 1 step-time 0.94s wps 5.91K ppl 524.11 bleu 0.00\n",
    "  epoch 0 step 300 lr 1 step-time 0.96s wps 5.80K ppl 340.05 bleu 0.00\n",
    "  epoch 0 step 400 lr 1 step-time 1.02s wps 6.06K ppl 277.61 bleu 0.00\n",
    "  epoch 0 step 500 lr 1 step-time 0.95s wps 5.89K ppl 205.85 bleu 0.00\n",
    "```\n",
    "\n",
    "更多细节，请查看：train.py。\n",
    "\n",
    "我们可以使用 Tensorboard 在训练过程中查看模型的summary：\n",
    "\n",
    "```shell\n",
    "tensorboard --port 22222 --logdir /tmp/nmt_model/\n",
    "```\n",
    "\n",
    "通过以下简单的变化，就能逆向完成英语到越南语的翻译。\n",
    "\n",
    "```shell\n",
    "--src=en --tgt=vi\n",
    "```\n",
    "\n",
    "### 3.7 预测(推理)与生成翻译结果\n",
    "\n",
    "当你训练你的 NMT 模型时（并且一旦你已经训练了模型），可以在给定之前不可见的源语句的情况下获得翻译。这一过程被称作推理。训练与推理之间有一个明确的区分（测试）：在推理时，我们只访问源语句，即 encoder_inputs。解码的方式有很多种，包括 greedy 解码、采样解码和束搜索解码（beam-search）。下面我们讨论一下 greedy 解码策略。\n",
    "\n",
    "其想法简单，我们将在图 3 中作说明：\n",
    "\n",
    "1. 在训练获取 encoder_state 的过程中，我们依然以相同方式编码源语句，并且 encoder_state 用于初始化解码器。\n",
    "2. 一旦解码器接收到开始符`<s>`（在我们的代码中指 tgt_sos_id），就开始解码处理（翻译）。\n",
    "3. 最大的单词，其 id 与最大的 logit 值相关联，正如被发出的词（这是 greedy 行为）。例如在图 3 中，单词 moi 在第一个解码步中具有最高的翻译概率。接着我们把这一单词作为输入馈送至下一个时间步。\n",
    "4. 这一过程会持续到这句话的终止符`「</s>」`，然后输出（在我们的代码中是 tgt_eos_id）。\n",
    "\n",
    "![img](https://github.com/tensorflow/nmt/raw/master/nmt/g3doc/img/greedy_dec.jpg)\n",
    "\n",
    "<center>图 3. Greedy 解码实例</center>\n",
    "\n",
    "推理与训练的区别在于步骤 3。推理不总是馈送作为输入的正确目标词，而是使用被模型预测的单词。下面是实现 greedy 解码的代码。它与训练解码器非常相似。\n",
    "\n",
    "```python\n",
    "# Helper\n",
    "helper = tf.contrib.seq2seq.GreedyEmbeddingHelper(\n",
    "    embedding_decoder,\n",
    "    tf.fill([batch_size], tgt_sos_id), tgt_eos_id)\n",
    "# Decoder\n",
    "decoder = tf.contrib.seq2seq.BasicDecoder(\n",
    "    decoder_cell, helper, encoder_state,\n",
    "    output_layer=projection_layer)# Dynamic decoding\n",
    "outputs, _ = tf.contrib.seq2seq.dynamic_decode(\n",
    "    decoder, maximum_iterations=maximum_iterations)\n",
    "translations = outputs.sample_id\n",
    "```\n",
    "\n",
    "我们在本文中使用了 GreedyEmbeddingHelper 而不是 TrainingHelper。由于无法提前知道目标语句的长度，我们使用 maximum_iterations 限制翻译的长度。一个启发是解码最多两倍的源语句长度。\n",
    "\n",
    "```python\n",
    "maximum_iterations = tf.round(tf.reduce_max(source_sequence_length) * 2)\n",
    "```\n",
    "\n",
    "我们已经训练了一个模型，现在可以创建一个推理文件并翻译一些语句：\n",
    "\n",
    "```shell\n",
    "cat > /tmp/my_infer_file.vi# (copy and paste some sentences from /tmp/nmt_data/tst2013.vi)\n",
    "\n",
    "python -m nmt.nmt \\\n",
    "    --model_dir=/tmp/nmt_model \\\n",
    "    --inference_input_file=/tmp/my_infer_file.vi \\\n",
    "    --inference_output_file=/tmp/nmt_model/output_infer\n",
    "\n",
    "cat /tmp/nmt_model/output_infer # To view the inference as output\n",
    "```\n",
    "\n",
    "注意上述指令也可在模型被训练时运行，只要存在一个训练检查点。详见 inference.py。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "h40rRHfmDuBX"
   },
   "source": [
    "## 4.提升\n",
    "\n",
    "在训练了一些最基本的序列到序列模型之后，我们现在更进一步。为了打造当前最优的神经机器翻译系统，我们需要更多的秘诀：注意力机制。该机制由 Bahdanau 等人在 2015 年首次提出（https://arxiv.org/abs/1409.0473 ），稍后 Luong 等人和其他人完善了它，其核心思想是当我们翻译时通过「注意」相关的源内容，建立直接的短连接。注意力机制的一个很好副产品是源语句和目标语句之间的一个易于可视化的对齐矩阵（如图 4 所示）。\n",
    "\n",
    "![img](https://github.com/tensorflow/nmt/raw/master/nmt/g3doc/img/attention_vis.jpg)\n",
    "\n",
    "<center>图 4. 注意力可视化——源语句与目标语句之间对齐的实例。图片来自 2015 年 Bahdanau 等人的论文。</center>\n",
    "\n",
    "请记住在 vanilla 序列到序列模型中，当开始编码处理时，我们把最后的源状态从编码器传递到解码器。这对短、中长度的语句效果很好；对于长句子，单一固定大小的隐状态成为了信息瓶颈。注意力机制没有摈弃源 RNN 中计算的所有隐状态，而是提出了允许解码器窥探它们的方法（把它们看作是源信息的动态存储）。如此，注意力机制提升了长句的翻译质量。现在，注意力机制实至名归，已成功应用于其他诸多任务（比如语音识别）。\n",
    "\n",
    "### 4.1 注意力机制背景\n",
    "\n",
    "我们现在描述一下注意力机制的实例（Luong et al., 2015），它已经被应用到几个最新型的系统当中了，包括开源工具，比如 OpenNMT（http://opennmt.net/about/ ）和此教程中的 TF seq2seq API。我们还将会提供注意力机制相关变体的内容。\n",
    "\n",
    "![img](https://github.com/tensorflow/nmt/raw/master/nmt/g3doc/img/attention_mechanism.jpg)\n",
    "\n",
    "图 5. 注意力机制——基于注意力的 NMT 系统（Luong et al., 2015 中有具体描述）。\n",
    "\n",
    "我们重点详解注意力计算过程中的第一步。为了更加清晰，我们没有展示图（2）中的嵌入层和投影层。\n",
    "\n",
    "如图 5 所示，注意力计算发生在解码步骤中的每一步。它包含下列步骤：\n",
    "\n",
    "1. 当前目标隐蔽状态和所有源状态（source state）进行比较，以导出权重（weight），见图 4。\n",
    "2. 基于注意力权重，我们计算了一个背景向量（context vector），作为源状态的平均权值。\n",
    "3. 将背景向量与当前目标隐蔽态进行结合以生成最终的注意力向量。\n",
    "4. 此注意力向量将作为下一时序步骤的输入。前三个步骤可以由下列公式总结：\n",
    "\n",
    "![img](https://github.com/tensorflow/nmt/raw/master/nmt/g3doc/img/attention_equation_0.jpg)\n",
    "\n",
    "这里，函数 score 用于将目标隐蔽状态 ht 和每一个源状态 hs 进行比较，结果会被标准化成生成式注意力权重（一个源位置的分布）。其实有很多种关于评分函数（scoring function）的选择；比较流行的评分函数包括公式（4）中给出的乘法与加法形式。一旦被计算，注意力向量 at 就会用于推导 softmax logit 和损失。这与 vanilla seq2seq 模型顶层的目标隐蔽态相似。函数 f 也可以利用其它形式。\n",
    "\n",
    "![img](https://github.com/tensorflow/nmt/raw/master/nmt/g3doc/img/attention_equation_1.jpg)\n",
    "\n",
    "注意力机制的多种实现方法可由以下链接获得：https://github.com/tensorflow/tensorflow/blob/master/tensorflow/contrib/seq2seq/python/ops/attention_wrapper.py 。\n",
    "\n",
    "注意力机制中有什么相关注意事项呢？\n",
    "\n",
    "上述公式表明注意力机制有很多种变体。这些变体依赖于评分函数（scoring function）和注意力函数（attention function）的形式，也依赖于前一状态 ht-1，而不依赖于开始建议的评分函数 ht（Bahdanau et al., 2015）。实际上我们发现的只有一些选择上的注意事项。\n",
    "* 一，注意力的基本形式，例如，目标和源之间的直接联系需要被呈现。\n",
    "* 二，把注意力向量输入给下一时间步骤，以把之前的注意力决策告知给网络（Luong et al., 2015）。\n",
    "\n",
    "评分函数的选择经常可以造成不同的性能表现。\n",
    "\n",
    "### 4.2 Attention Wrapper API\n",
    "\n",
    "在我们的 Attention Wrapper API 的实现中，借鉴了 Weston et al., 2015 在 onmemory network 工作中的术语。相比于拥有可读、可写的记忆，此教程中的 attention 机制仅是可读的记忆。特别是对隐藏态（或者隐藏态的变体，例如 $W\\overline{h}_s$ in Luong's scoring style or $W_2\\overline{h}_s$ ) 的设定，被认为是「记忆」。在每个时间步下，我们使用现有的目标隐藏态作为「query」决定读取哪一部分记忆。通常情况下，query 需要与单个记忆条相对应的 keys 进行对比。在上面对注意机制的演示中，我们偶然使用一套源隐藏态（或者其变体，例如$W_1h_t$）作为「key」。你们可以从这种记忆网络术语中获取灵感，找到其他形式的 attention。\n",
    "\n",
    "由于 attention wrapper，就不再需要扩展我们带有 attention 的 vanilla seq2seq 代码。这部分文件为 attention_model.py。\n",
    "\n",
    "首先，我们需要定义一种注意机制，例如采用 Luong et al., 2015 的研究。\n",
    "\n",
    "```python\n",
    "# attention_states: [batch_size, max_time, num_units]\n",
    "attention_states = tf.transpose(encoder_outputs, [1, 0, 2])\n",
    "# Create an attention mechanism\n",
    "attention_mechanism = tf.contrib.seq2seq.LuongAttention(\n",
    "    num_units, attention_states,\n",
    "    memory_sequence_length=source_sequence_length)\n",
    "```\n",
    "\n",
    "在之前的 Encoder 部分，encoder_outputs 是一套顶层的掩藏态源，形式为 [max_time, batch_size, num_units]（因为我们使用 dynamic_rnn with time_major 设定）。在注意机制上，我们需要保证通过的「memory」是批次为主的，所以需要调换 attention_states。我们通过 source_sequence_length 保证注意机制的权重有适当的规范化（只在 non-padding 的位置）。定义完注意机制之后，我们使用 AttentionWrapper 来包裹解码单元。\n",
    "\n",
    "```python\n",
    "decoder_cell = tf.contrib.seq2seq.AttentionWrapper(\n",
    "    decoder_cell, attention_mechanism,\n",
    "    attention_layer_size=num_units)\n",
    "```\n",
    "\n",
    "剩下的代码基本和编码器一转样 (https://github.com/tensorflow/nmt#decoder)!\n",
    "\n",
    "### 4.3 上手—打造一个基于注意力的 NMT 模型\n",
    "\n",
    "为了使注意力发挥作用，我们需要用到 luong、scaled_luong、bahdanau 或 normed_bahdanau 其中的一个作为训练期间注意力标志（attention flag）的值。该标志指定了我们将要使用的注意力机制。除此之外，我们需要为注意力模型创建一个新目录，因此无需重新使用之前训练的基本 NMT 模型。\n",
    "\n",
    "运行以下指令开始训练：\n",
    "\n",
    "```shell\n",
    "mkdir /tmp/nmt_attention_model\n",
    "\n",
    "python -m nmt.nmt \\\n",
    "    --attention=scaled_luong \\\n",
    "    --src=vi --tgt=en \\\n",
    "    --vocab_prefix=/tmp/nmt_data/vocab  \\\n",
    "    --train_prefix=/tmp/nmt_data/train \\\n",
    "    --dev_prefix=/tmp/nmt_data/tst2012  \\\n",
    "    --test_prefix=/tmp/nmt_data/tst2013 \\\n",
    "    --out_dir=/tmp/nmt_attention_model \\\n",
    "    --num_train_steps=12000 \\\n",
    "    --steps_per_stats=100 \\\n",
    "    --num_layers=2 \\\n",
    "    --num_units=128 \\\n",
    "    --dropout=0.2 \\\n",
    "    --metrics=bleu\n",
    "```\n",
    "\n",
    "训练之后，我们可以使用带有新 model_dir 的相同推理指令进行推理：\n",
    "\n",
    "```shell\n",
    "python -m nmt.nmt \\\n",
    "    --model_dir=/tmp/nmt_attention_model \\\n",
    "    --inference_input_file=/tmp/my_infer_file.vi \\\n",
    "    --inference_output_file=/tmp/nmt_attention_model/output_infer\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "hkOBiN7BDuBX"
   },
   "source": [
    "## 5.技巧和注意点\n",
    "\n",
    "### 5.1 构建训练、验证和测试图\n",
    "\n",
    "当我们使用 TensorFlow 单间一个机器学习模型的时候，最好构建三个分开的 graph：\n",
    "\n",
    "- 训练图，包括：\n",
    "  - Batches, buckets, 以及来自文件或外部输入的数据集的部分采样数据；\n",
    "  - 前向和反向传播的 ops；\n",
    "  - 创建 optimizer，以及添加训练 op；\n",
    "- 验证图，包括：\n",
    "  - Batches, buckets, 以及来自文件或外部输入的数据集输入数据\n",
    "  - 训练时的前向传播 op，以及要添加的 evaluation ops\n",
    "- 预测图，包括：\n",
    "  - 不需要批处理的输入数据\n",
    "  - 不需要 subsample 和 bucket 输入数据\n",
    "  - 从 placeholders 读取输入数据（数据可以通过 feed_dict 被图读取，或者使用 C++ TensorFlow serving binary）\n",
    "  - 模型前向传播的部分 ops，以及一些可能 [session.run](http://session.run/) 函数调用的所需要的额外的为存储状态（state）所需要的 inputs/outputs。\n",
    "\n",
    "现在比较棘手的一点是 ，如何在一个机器上让三个图共享这些 variables，这可以通过为每个图创建不同的 session 来解决。训练过程的session 周期性的保存 checkpoints，然后 eval session 和 inference session 就可以读取checkpoints。下面的例子展示了这两种方法的不同。\n",
    "\n",
    "#### 前一种方法：三个模型都在一个图里，并且共享一个 session。\n",
    "\n",
    "```python\n",
    "with tf.variable_scope('root'):\n",
    "  train_inputs = tf.placeholder()\n",
    "  train_op, loss = BuildTrainModel(train_inputs)\n",
    "  initializer = tf.global_variables_initializer()\n",
    "\n",
    "with tf.variable_scope('root', reuse=True):\n",
    "  eval_inputs = tf.placeholder()\n",
    "  eval_loss = BuildEvalModel(eval_inputs)\n",
    "\n",
    "with tf.variable_scope('root', reuse=True):\n",
    "  infer_inputs = tf.placeholder()\n",
    "  inference_output = BuildInferenceModel(infer_inputs)\n",
    "\n",
    "sess = tf.Session()\n",
    "\n",
    "sess.run(initializer)\n",
    "\n",
    "for i in itertools.count():\n",
    "  train_input_data = ...\n",
    "  sess.run([loss, train_op], feed_dict={train_inputs: train_input_data})\n",
    "\n",
    "  if i % EVAL_STEPS == 0:\n",
    "    while data_to_eval:\n",
    "      eval_input_data = ...\n",
    "      sess.run([eval_loss], feed_dict={eval_inputs: eval_input_data})\n",
    "\n",
    "  if i % INFER_STEPS == 0:\n",
    "    sess.run(inference_output, feed_dict={infer_inputs: infer_input_data})\n",
    "```\n",
    "\n",
    "#### 后一种方法：三个模型在三个图里，三个 sessions 共享同样的 variables。\n",
    "\n",
    "```python\n",
    "train_graph = tf.Graph()\n",
    "eval_graph = tf.Graph()\n",
    "infer_graph = tf.Graph()\n",
    "\n",
    "with train_graph.as_default():\n",
    "  train_iterator = ...\n",
    "  train_model = BuildTrainModel(train_iterator)\n",
    "  initializer = tf.global_variables_initializer()\n",
    "\n",
    "with eval_graph.as_default():\n",
    "  eval_iterator = ...\n",
    "  eval_model = BuildEvalModel(eval_iterator)\n",
    "\n",
    "with infer_graph.as_default():\n",
    "  infer_iterator, infer_inputs = ...\n",
    "  infer_model = BuildInferenceModel(infer_iterator)\n",
    "\n",
    "checkpoints_path = \"/tmp/model/checkpoints\"\n",
    "\n",
    "train_sess = tf.Session(graph=train_graph)\n",
    "eval_sess = tf.Session(graph=eval_graph)\n",
    "infer_sess = tf.Session(graph=infer_graph)\n",
    "\n",
    "train_sess.run(initializer)\n",
    "train_sess.run(train_iterator.initializer)\n",
    "\n",
    "for i in itertools.count():\n",
    "\n",
    "  train_model.train(train_sess)\n",
    "\n",
    "  if i % EVAL_STEPS == 0:\n",
    "    checkpoint_path = train_model.saver.save(train_sess, checkpoints_path, global_step=i)\n",
    "    eval_model.saver.restore(eval_sess, checkpoint_path)\n",
    "    eval_sess.run(eval_iterator.initializer)\n",
    "    while data_to_eval:\n",
    "      eval_model.eval(eval_sess)\n",
    "\n",
    "  if i % INFER_STEPS == 0:\n",
    "    checkpoint_path = train_model.saver.save(train_sess, checkpoints_path, global_step=i)\n",
    "    infer_model.saver.restore(infer_sess, checkpoint_path)\n",
    "    infer_sess.run(infer_iterator.initializer, feed_dict={infer_inputs: infer_input_data})\n",
    "    while data_to_infer:\n",
    "      infer_model.infer(infer_sess)\n",
    "```\n",
    "\n",
    "注意后一种方法是如何被转换为分布式版本的。\n",
    "\n",
    "后种方法与前种方法的另一个不同在于，后者不用在 *session.sun* 调用时通过 *feed_dicts* 喂给数据，而是使用自带状态的 *iterator* 对象（stateful iterator objects）。不论在单机还是分布式集群上，这些 *iterators* 可以让输入管道（input pipeline）变得更容易。在下一小节，我们将使用新的数据输入管道（input data pipeline）。\n",
    "\n",
    "### 5.2 数据输入管道(Data Input Pipeline)\n",
    "\n",
    "在 TensorFlow 1.2版本之前，用户有两种把数据喂给 TensorFlow training 和 eval pipelines的方法：\n",
    "\n",
    "- 1. 在每次训练调用 *session.run* 时，通过 *feed_dict* 直接喂给数据；\n",
    "- 2. 使用 tf.train（例如 tf.train.batch）和 tf.contrib.train 中的队列机制（queueing machanisms）；\n",
    "- 3. 使用来自 helper 层级框架比如 tf.contrib.learn 或 tf.contrib.slim 的 helpers （这种方法是使用更高效的方法利用第二种方法）。\n",
    "\n",
    "第一种方法对不熟悉 TensorFlow 或需要做一些外部的数据修改（比如他们自己的 minibatch queueing）的用户来说更简单，这种方法只需用简单的 Python 语法就可实现。第二种和第三种方法更标准但也不那么灵活，他们需要开启多个 Python 线程（queue runners）。更重要的是，如果操作不当会导致死锁或难以查明的错误。尽管如此，队列的方法仍要比 *feed_dict* 的方法高效很多，并且也是单机和分布式训练的标准。\n",
    "\n",
    "从TensorFlow 1.2开始，有一种新的数据读取的方法可以使用： dataset iterators，其在 **tf.data**模块。Data iterators 非常灵活，易于使用和操作，并且利用 TensorFlow C++ runtime 实现了高效和多线程。\n",
    "\n",
    "我们可以使用一个 batch data Tensor，一个文件名，或者包含多个文件名的 Tensor 来创建一个 **dataset**。下面是一些例子：\n",
    "\n",
    "```python\n",
    "# Training dataset consists of multiple files.\n",
    "train_dataset = tf.data.TextLineDataset(train_files)\n",
    "\n",
    "# Evaluation dataset uses a single file, but we may\n",
    "# point to a different file for each evaluation round.\n",
    "eval_file = tf.placeholder(tf.string, shape=())\n",
    "eval_dataset = tf.data.TextLineDataset(eval_file)\n",
    "\n",
    "# For inference, feed input data to the dataset directly via feed_dict.\n",
    "infer_batch = tf.placeholder(tf.string, shape=(num_infer_examples,))\n",
    "infer_dataset = tf.data.Dataset.from_tensor_slices(infer_batch)\n",
    "```\n",
    "\n",
    "所有的数据都可以完成像数据预处理一样的处理方式，包括数据的 reading 和 cleaning，bucketing（在 training 和 eval 的时候），filtering 以及 batching。\n",
    "\n",
    "把每个句子转换为单词串的向量（vectors of word strings），那我们可以使用 dataset 的 map transformation:\n",
    "\n",
    "```python\n",
    "dataset = dataset.map(lambda string: tf.string_split([string]).values)\n",
    "```\n",
    "\n",
    "我们也可以把每个句子向量转换为包含向量与其动态长度的元组：\n",
    "\n",
    "```python\n",
    "dataset = dataset.map(lambda words: (words, tf.size(words))\n",
    "```\n",
    "\n",
    "最后，我们可以对每个句子应用 vocabulary lookup。给定一个 lookup 的 table，此 map 函数可以把元组的第一个元素从串向量转换为数字向量。（译者注：不好翻译，原文是：Finally, we can perform a vocabulary lookup on each sentence. Given a lookup table object table, this map converts the first tuple elements from a vector of strings to a vector of integers.）\n",
    "\n",
    "```python\n",
    "dataset = dataset.map(lambda words, size: (table.lookup(words), size))\n",
    "```\n",
    "\n",
    "合并两个 datasets 也非常简单，如果两个文件有行行对应的翻译，并且两个文件分别被不同的 dataset 读取，那么可以通过下面这种方式生成一个新的 dataset，这个新的 dataset 的内容是两种语言的翻译一一对应的元组。\n",
    "\n",
    "```python\n",
    "source_target_dataset = tf.data.Dataset.zip((source_dataset, target_dataset))\n",
    "```\n",
    "\n",
    "Batching 变长的句子实现起来也很直观。下边的代码从 *source_target_dataset* 中 batch 了 *batch_size* 个元素，并且分别为每个 batch 的源向量和目标向量 padding 到最长的源向量和目标向量的长度。\n",
    "\n",
    "```python\n",
    "batched_dataset = source_target_dataset.padded_batch(\n",
    "        batch_size,\n",
    "        padded_shapes=((tf.TensorShape([None]),  # source vectors of unknown size\n",
    "                        tf.TensorShape([])),     # size(source)\n",
    "                       (tf.TensorShape([None]),  # target vectors of unknown size\n",
    "                        tf.TensorShape([]))),    # size(target)\n",
    "        padding_values=((src_eos_id,  # source vectors padded on the right with src_eos_id\n",
    "                         0),          # size(source) -- unused\n",
    "                        (tgt_eos_id,  # target vectors padded on the right with tgt_eos_id\n",
    "                         0)))         # size(target) -- unused\n",
    "```\n",
    "\n",
    "从 dataset 拿到的数据会嵌套为元组，其 tensors 的最左边的维度是 batch_size. 其结构如下：\n",
    "\n",
    "- iterator[0][0] has the batched and padded source sentence matrices.\n",
    "- iterator[0][1] has the batched source size vectors.\n",
    "- iterator[1][0] has the batched and padded target sentence matrices.\n",
    "- iterator[1][1] has the batched target size vectors.\n",
    "\n",
    "最后，bucketing 多个 batch 的大小差不多的源句子也是可以的。更多的代码实现详见文件[utils/iterator_utils.py](https://github.com/tensorflow/nmt/tree/master/nmt/utils/iterator_utils.py)。\n",
    "\n",
    "从 dataset 中读取数据需要三行的代码：创建 iterator，取其值，初始化。\n",
    "\n",
    "```python\n",
    "batched_iterator = batched_dataset.make_initializable_iterator()\n",
    "\n",
    "((source, source_lengths), (target, target_lengths)) = batched_iterator.get_next()\n",
    "\n",
    "# At initialization time.\n",
    "session.run(batched_iterator.initializer, feed_dict={...})\n",
    "```\n",
    "\n",
    "一旦 iterator 被初始化，那么 [session.run](http://session.run) 每一次调用 source 和 target ，都会从dataset中自动提取下一个 minibatch 的数据。\n",
    "\n",
    "### 5.3 让 NMT 模型更完美的其他技巧\n",
    "\n",
    "#### 双向RNN(Bidirectional RNN)\n",
    "\n",
    "一般来讲，encoder 的双向 RNNs 可以让模型表现更好（训练速度会下降，因为有更多的层需要计算）。这里，我们给出了构建一个单层双向层的 encoder 的简单代码：\n",
    "\n",
    "```python\n",
    "# Construct forward and backward cells\n",
    "forward_cell = tf.nn.rnn_cell.BasicLSTMCell(num_units)\n",
    "backward_cell = tf.nn.rnn_cell.BasicLSTMCell(num_units)\n",
    "\n",
    "bi_outputs, encoder_state = tf.nn.bidirectional_dynamic_rnn(\n",
    "    forward_cell, backward_cell, encoder_emb_inp,\n",
    "    sequence_length=source_sequence_length, time_major=True)\n",
    "encoder_outputs = tf.concat(bi_outputs, -1)\n",
    "```\n",
    "\n",
    "*encoder_outputs* 和 *encoder_state* 也可以使用 Encoder 小节的方法获取到。需要注意的是，如果要创建多层双向层，你需要修改一下 encoder_state，见 [model.py](https://github.com/tensorflow/nmt/tree/master/nmt/model.py) 的*_build_bidirectional_rnn()*方法。\n",
    "\n",
    "#### 集束搜索(Beam Search)\n",
    "\n",
    "Greedy decoding 可以给我们非常合理的翻译结果，但是 beam search decoding 可以让翻译结果更好。Beam search 的思想是，考虑我们可以选择的所有翻译结果的排名最靠前的几个候选的集合，我们探索其所有的可能翻译结果（大家也可以参考[知乎的beam search讨论](https://www.zhihu.com/question/54356960/answer/138990060)）。Beam 的这个 size 我们称为 *beam width*，一个较小的 beam width 比如说 10，就已经足够大了。我们推荐读者阅读 [Neubig, (2017)](https://arxiv.org/abs/1703.01619) 的 7.2.3 小节。这是 beam search 的一个例子：\n",
    "\n",
    "```python\n",
    "# Replicate encoder infos beam_width times\n",
    "decoder_initial_state = tf.contrib.seq2seq.tile_batch(\n",
    "    encoder_state, multiplier=hparams.beam_width)\n",
    "\n",
    "# Define a beam-search decoder\n",
    "decoder = tf.contrib.seq2seq.BeamSearchDecoder(\n",
    "        cell=decoder_cell,\n",
    "        embedding=embedding_decoder,\n",
    "        start_tokens=start_tokens,\n",
    "        end_token=end_token,\n",
    "        initial_state=decoder_initial_state,\n",
    "        beam_width=beam_width,\n",
    "        output_layer=projection_layer,\n",
    "        length_penalty_weight=0.0)\n",
    "\n",
    "# Dynamic decoding\n",
    "outputs, _ = tf.contrib.seq2seq.dynamic_decode(decoder, ...)\n",
    "```\n",
    "\n",
    "在 Decoder 小节，*dynamic_decode()* API 也被使用过。解码结束，我们就可以使用下面的代码得到翻译结果：\n",
    "\n",
    "```python\n",
    "translations = outputs.predicted_ids\n",
    "# Make sure translations shape is [batch_size, beam_width, time]\n",
    "if self.time_major:\n",
    "   translations = tf.transpose(translations, perm=[1, 2, 0])\n",
    "```\n",
    "\n",
    "更多细节，可查看 [model.py](https://github.com/tensorflow/nmt/tree/master/nmt/model.py), *_build_decoder()* 函数。\n",
    "\n",
    "#### 超参数(Hyperparameters)\n",
    "\n",
    "有一些超参数也可以供我们调节。这里，根据我们的实验，我们列举了几个超参数【你可以表示不认同，保留自己的看法】。\n",
    "\n",
    "- **optimizer**：对于“不太常见”的网络结构，Adam 可能可以给出一个较合理的结果，如果你用 SGD 进行训练，那么 SGD 往往可以取得更好的结果。\n",
    "- **Attention**：Bahdanau 类型的 attention，encoder 需要双向结构才能表现很好；同时 Luong 类型的 attention 需要其他的一些设置才能表现很好。在本教程中，我们推荐使用被改进的这两个类型的 attention：**scaled_luong** 和 **normed_bahdanau**。\n",
    "\n",
    "**多 GPU 训练 | Multi-GPU training**\n",
    "\n",
    "训练一个 NMT 模型可能需要几天的时间，我们可以把不同的 RNN layers 放在不同的 GPUs 进行训练可以加快训练速度。这里是使用多 GPUs 创建 RNN layers 的例子：\n",
    "\n",
    "```python\n",
    "cells = []\n",
    "for i in range(num_layers):\n",
    "  cells.append(tf.contrib.rnn.DeviceWrapper(\n",
    "      tf.contrib.rnn.LSTMCell(num_units),\n",
    "      \"/gpu:%d\" % (num_layers % num_gpus)))\n",
    "cell = tf.contrib.rnn.MultiRNNCell(cells)\n",
    "```\n",
    "\n",
    "另外，我们还需要 tf.gradients 的 colocate_gradients_with_ops 参数来同步梯度的计算。\n",
    "\n",
    "你会发现，尽管我们使用了多个 GPUs，但是 attention-based NMT 模型的训练速度提升不大。问题的关键在于，在标准的 attention 模型中，在每个时间步，我们都需要用最后一层的输出去“查询”attention，这就意味着，每一个解码的时间步都需要等前面的时间步完全完成。因此，我们不能简单的通过在多 GPUs 上部署 RNN layers 来同步解码过程。\n",
    "\n",
    "[GNMT attention architecture](https://arxiv.org/pdf/1609.08144.pdf) 可以通过使用第一层的输出来查询 attention 的方法来同步 decoder 的计算。这样，解码器的每一步就可以在前一步的第一层和 attention 计算完成之后就可以进行解码了。我们的 API 实现了这个结构 [GNMTAttentionMultiCell](https://github.com/tensorflow/nmt/tree/master/nmt/gnmt_model.py)，其是*tf.contrib.rnn.MultiRNNCell*的子类。这里是使用 *GNMTAttentionMultiCell* 创建一个 decoder 的例子：\n",
    "\n",
    "```python\n",
    "cells = []\n",
    "for i in range(num_layers):\n",
    "  cells.append(tf.contrib.rnn.DeviceWrapper(\n",
    "      tf.contrib.rnn.LSTMCell(num_units),\n",
    "      \"/gpu:%d\" % (num_layers % num_gpus)))\n",
    "attention_cell = cells.pop(0)\n",
    "attention_cell = tf.contrib.seq2seq.AttentionWrapper(\n",
    "    attention_cell,\n",
    "    attention_mechanism,\n",
    "    attention_layer_size=None,  # don't add an additional dense layer.\n",
    "    output_attention=False,)\n",
    "cell = GNMTAttentionMultiCell(attention_cell, cells)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "zfpQD0nvDuBZ"
   },
   "source": [
    "### 结果与预测质量\n",
    "\n",
    "IWSLT 英语-越南语\n",
    "\n",
    "训练：133k 的样本，dev=tst2012，test=tst2013\n",
    "\n",
    "Systems | tst2012 (dev) | test2013 (test)\n",
    "--- | :---: | :---:\n",
    "NMT (greedy) | 23.2 | 25.5\n",
    "NMT (beam=10) | 23.8 | **26.1**\n",
    "[(Luong & Manning, 2015)](https://nlp.stanford.edu/pubs/luong-manning-iwslt15.pdf) | - | 23.3\n",
    "\n",
    "*训练速度：在英伟达 K40m 上是 0.37s 的时间步、15.3k 的 wps，在 Titan X 上是 0.17 s 的时间步，32.2k 的 wps。*\n",
    "\n",
    "WMT 德语-英语\n",
    "\n",
    "训练：4.5M 的样本量，dev=newstest2013，test=newtest2015\n",
    "\n",
    "Systems | newstest2013 (dev) | newstest2015\n",
    "--- | :---: | :---:\n",
    "NMT (greedy) | 27.1 | 27.6\n",
    "NMT (beam=10) | 28.0 | 28.9\n",
    "NMT + GNMT attention (beam=10) | 29.0 | **29.9**\n",
    "[WMT SOTA](http://matrix.statmt.org/) | - | 29.3\n",
    "\n",
    "*训练速度：在英伟达 K40m 上是 2.1s 的时间步，3.4k 的 wps，在英伟达 Titan X 上是 0.7s 的时间步，8.7k 的 wps。*\n",
    "\n",
    "为了查看 GNMT 注意的加速度，我们只在 K40m 上做了基准测试：\n",
    "\n",
    "Systems | 1 gpu | 4 gpus | 8 gpus\n",
    "--- | :---: | :---: | :---:\n",
    "NMT (4 layers) | 2.2s, 3.4K | 1.9s, 3.9K | -\n",
    "NMT (8 layers) | 3.5s, 2.0K | - | 2.9s, 2.4K\n",
    "NMT + GNMT attention (4 layers) | 2.6s, 2.8K | 1.7s, 4.3K | -\n",
    "NMT + GNMT attention (8 layers) | 4.2s, 1.7K | - | 1.9s, 3.8K\n",
    "\n",
    "*WMT 英语-德语 全对比*\n",
    "\n",
    "第二行是我们 GNMT 注意模型：模型 1（4 层），模型 2（8 层）。\n",
    "\n",
    "Systems | newstest2014 | newstest2015\n",
    "--- | :---: | :---:\n",
    "*Ours* &mdash; NMT + GNMT attention (4 layers) | 23.7 | 26.5\n",
    "*Ours* &mdash; NMT + GNMT attention (8 layers) | 24.4 | **27.6**\n",
    "[WMT SOTA](http://matrix.statmt.org/) | 20.6 | 24.9\n",
    "OpenNMT [(Klein et al., 2017)](https://arxiv.org/abs/1701.02810) | 19.3 | -\n",
    "tf-seq2seq [(Britz et al., 2017)](https://arxiv.org/abs/1703.03906) | 22.2 | 25.2\n",
    "GNMT [(Wu et al., 2016)](https://research.google.com/pubs/pub45610.html) | **24.6** | -\n",
    "\n",
    "**其他资源**\n",
    "\n",
    "若想深入了解神经机器翻译和序列-序列模型，我们非常推荐以下资源：\n",
    "\n",
    "- Neural Machine Translation and Sequence-to-sequence Models: A Tutorial：https://arxiv.org/abs/1703.01619\n",
    "- Neural Machine Translation - Tutorial ACL 2016：https://sites.google.com/site/acl16nmt/\n",
    "- Thang Luong's Thesis on Neural Machine Translation：https://github.com/lmthang/thesis\n",
    "\n",
    "用于构建 seq2seq 模型的工具很多：\n",
    "\n",
    "- Stanford NMT https://nlp.stanford.edu/projects/nmt/ [Matlab] \n",
    "- tf-seq2seq https://github.com/google/seq2seq [TensorFlow] \n",
    "- Nemantus https://github.com/rsennrich/nematus [Theano] \n",
    "- OpenNMT http://opennmt.net/ [Torch]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "KgRicJtbDuBa"
   },
   "source": [
    "## 6.seq2seq构建的聊天机器人应用\n",
    "我们来使用seq2seq框架完成一个聊天机器人构建的任务，我给大家准备了一些对话语料，我们使用这份数据来构建聊天机器人的AI应用。在此之前，我们先了解一下原有的翻译系统需要准备的语料格式，我们把中文数据处理成格式一致的形态。\n",
    "\n",
    "我们先拉取一份样例数据。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 510
    },
    "colab_type": "code",
    "id": "D6WFcziSDuBb",
    "outputId": "3ea4f8d0-2a2a-4912-81b4-941ce80b49f5"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/content/nmt\n",
      "Download training dataset train.en and train.vi.\n",
      "  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n",
      "                                 Dload  Upload   Total   Spent    Left  Speed\n",
      "100 12.9M  100 12.9M    0     0  4415k      0  0:00:03  0:00:03 --:--:-- 4415k\n",
      "  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n",
      "                                 Dload  Upload   Total   Spent    Left  Speed\n",
      "100 17.2M  100 17.2M    0     0  5386k      0  0:00:03  0:00:03 --:--:-- 5386k\n",
      "Download dev dataset tst2012.en and tst2012.vi.\n",
      "  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n",
      "                                 Dload  Upload   Total   Spent    Left  Speed\n",
      "100  136k  100  136k    0     0   188k      0 --:--:-- --:--:-- --:--:--  188k\n",
      "  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n",
      "                                 Dload  Upload   Total   Spent    Left  Speed\n",
      "100  183k  100  183k    0     0   253k      0 --:--:-- --:--:-- --:--:--  253k\n",
      "Download test dataset tst2013.en and tst2013.vi.\n",
      "  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n",
      "                                 Dload  Upload   Total   Spent    Left  Speed\n",
      "100  129k  100  129k    0     0   196k      0 --:--:-- --:--:-- --:--:--  196k\n",
      "  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n",
      "                                 Dload  Upload   Total   Spent    Left  Speed\n",
      "100  179k  100  179k    0     0   246k      0 --:--:-- --:--:-- --:--:--  246k\n",
      "Download vocab file vocab.en and vocab.vi.\n",
      "  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n",
      "                                 Dload  Upload   Total   Spent    Left  Speed\n",
      "100  136k  100  136k    0     0   207k      0 --:--:-- --:--:-- --:--:--  208k\n",
      "  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n",
      "                                 Dload  Upload   Total   Spent    Left  Speed\n",
      "100 46767  100 46767    0     0   102k      0 --:--:-- --:--:-- --:--:--  102k\n"
     ]
    }
   ],
   "source": [
    "%cd nmt\n",
    "!bash nmt/scripts/download_iwslt15.sh /tmp/nmt_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "2Cp3DTKmDuBe"
   },
   "source": [
    "**查看一下包含的文件**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 51
    },
    "colab_type": "code",
    "id": "_KeeIZNaDuBg",
    "outputId": "055d8dfa-c074-476c-de5a-d7da129873e7"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train.en  tst2012.en  tst2013.en  vocab.en\n",
      "train.vi  tst2012.vi  tst2013.vi  vocab.vi\n"
     ]
    }
   ],
   "source": [
    "!ls /tmp/nmt_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "3CSnj8lPDuBj"
   },
   "source": [
    "**看一下源语言与目标语言的格式，以及对应的数据量**\n",
    "\n",
    "可以看到都是做过tokenization之后的数据。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 207
    },
    "colab_type": "code",
    "id": "GXhLcDo7DuBk",
    "outputId": "caef6cfe-054e-49f9-cd43-55521aa42b40"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Rachel Pike : The science behind a climate headline\n",
      "In 4 minutes , atmospheric chemist Rachel Pike provides a glimpse of the massive scientific effort behind the bold headlines on climate change , with her team -- one of thousands who contributed -- taking a risky flight over the rainforest in pursuit of data on a key molecule .\n",
      "I &apos;d like to talk to you today about the scale of the scientific effort that goes into making the headlines you see in the paper .\n",
      "Headlines that look like this when they have to do with climate change , and headlines that look like this when they have to do with air quality or smog .\n",
      "They are both two branches of the same field of atmospheric science .\n",
      "Recently the headlines looked like this when the Intergovernmental Panel on Climate Change , or IPCC , put out their report on the state of understanding of the atmospheric system .\n",
      "That report was written by 620 scientists from 40 countries .\n",
      "They wrote almost a thousand pages on the topic .\n",
      "And all of those pages were reviewed by another 400-plus scientists and reviewers , from 113 countries .\n",
      "It &apos;s a big community . It &apos;s such a big community , in fact , that our annual gathering is the largest scientific meeting in the world .\n"
     ]
    }
   ],
   "source": [
    "!head -10 /tmp/nmt_data/train.en"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "-vne-jIUDuBm",
    "outputId": "cef0d6f5-bf20-49b4-b6e5-f6b61a26db63"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "133317 /tmp/nmt_data/train.en\n"
     ]
    }
   ],
   "source": [
    "!wc -l /tmp/nmt_data/train.en"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Y7ySPsk0DuBp"
   },
   "source": [
    "**还需要准备好vocabulary词表**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 187
    },
    "colab_type": "code",
    "id": "P1-RpoqSDuBq",
    "outputId": "d7eaa38e-3a76-4277-b8fd-31960922a80d"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<unk>\n",
      "<s>\n",
      "</s>\n",
      "Rachel\n",
      ":\n",
      "The\n",
      "science\n",
      "behind\n",
      "a\n",
      "climate\n"
     ]
    }
   ],
   "source": [
    "!head -10 /tmp/nmt_data/vocab.en"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "W-_HmH1vDuBt",
    "outputId": "d1271a6b-05d4-4631-9531-769f98ad68e0"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "17191 /tmp/nmt_data/vocab.en\n"
     ]
    }
   ],
   "source": [
    "!wc -l /tmp/nmt_data/vocab.en"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "gbue9sZYDuBw",
    "outputId": "38e898d3-0d66-4a5d-ad49-f4d73cd5bd44"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7709 /tmp/nmt_data/vocab.vi\n"
     ]
    }
   ],
   "source": [
    "!wc -l /tmp/nmt_data/vocab.vi"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "C7W7aj-iEr7k"
   },
   "source": [
    "## 聊天机器人语料\n",
    "这里列举一些从网络中找到的用于训练中文（英文）聊天机器人的对话语料。\n",
    "\n",
    "### 公开语料\n",
    "搜集到的一些数据集如下，点击链接可以进入原始地址\n",
    "\n",
    "1. [dgk_shooter_min.conv.zip](https://github.com/rustch3n/dgk_lost_conv)\n",
    "<br>中文电影对白语料，噪音比较大，许多对白问答关系没有对应好\n",
    "\n",
    "2. [The NUS SMS Corpus](https://github.com/kite1988/nus-sms-corpus)\n",
    "<br>包含中文和英文短信息语料，据说是世界最大公开的短消息语料\n",
    "\n",
    "3. [ChatterBot中文基本聊天语料](https://github.com/gunthercox/chatterbot-corpus/tree/master/chatterbot_corpus/data)\n",
    "<br>ChatterBot聊天引擎提供的一点基本中文聊天语料，量很少，但质量比较高\n",
    "\n",
    "4. [Datasets for Natural Language Processing](https://github.com/karthikncode/nlp-datasets)\n",
    "<br>这是他人收集的自然语言处理相关数据集，主要包含Question Answering，Dialogue Systems， Goal-Oriented Dialogue Systems三部分，都是英文文本。可以使用机器翻译为中文，供中文对话使用\n",
    "\n",
    "5. [小黄鸡](https://github.com/rustch3n/dgk_lost_conv/tree/master/results)\n",
    "<br>据传这就是小黄鸡的语料：xiaohuangji50w_fenciA.conv.zip （已分词） 和 xiaohuangji50w_nofenci.conv.zip （未分词）\n",
    "\n",
    "6. [白鹭时代中文问答语料](https://github.com/Samurais/egret-wenda-corpus)\n",
    "<br>由白鹭时代官方论坛问答板块10,000+ 问题中，选择被标注了“最佳答案”的纪录汇总而成。人工review raw data，给每一个问题，一个可以接受的答案。目前，语料库只包含2907个问答。([备份](./egret-wenda-corpus.zip))\n",
    "\n",
    "7. [Chat corpus repository](https://github.com/Marsan-Ma/chat_corpus)\n",
    "<br>chat corpus collection from various open sources\n",
    "<br>包括：开放字幕、英文电影字幕、中文歌词、英文推文\n",
    "\n",
    "8. [保险行业QA语料库](https://github.com/Samurais/insuranceqa-corpus-zh)\n",
    "<br>通过翻译 [insuranceQA](https://github.com/shuzi/insuranceQA)产生的数据集。train_data含有问题12,889条，数据 141779条，正例：负例 = 1:10； test_data含有问题2,000条，数据 22000条，正例：负例 = 1:10；valid_data含有问题2,000条，数据 22000条，正例：负例 = 1:10"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "oxWshRykDuBz"
   },
   "source": [
    "**我们下载小黄鸡语料，并对它做一个处理，使得它符合seq2seq模型的输入格式**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 323
    },
    "colab_type": "code",
    "id": "aZiHEojSFE3h",
    "outputId": "28655b79-ed94-47b3-8d9b-7aa9d62c7ac3"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--2019-01-27 07:52:17--  https://github.com/candlewill/Dialog_Corpus/raw/master/xiaohuangji50w_nofenci.conv.zip\n",
      "Resolving github.com (github.com)... 192.30.253.112, 192.30.253.113\n",
      "Connecting to github.com (github.com)|192.30.253.112|:443... connected.\n",
      "HTTP request sent, awaiting response... 302 Found\n",
      "Location: https://raw.githubusercontent.com/candlewill/Dialog_Corpus/master/xiaohuangji50w_nofenci.conv.zip [following]\n",
      "--2019-01-27 07:52:17--  https://raw.githubusercontent.com/candlewill/Dialog_Corpus/master/xiaohuangji50w_nofenci.conv.zip\n",
      "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 151.101.0.133, 151.101.64.133, 151.101.128.133, ...\n",
      "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|151.101.0.133|:443... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 10232473 (9.8M) [application/zip]\n",
      "Saving to: ‘xiaohuangji50w_nofenci.conv.zip’\n",
      "\n",
      "\r",
      "          xiaohuang   0%[                    ]       0  --.-KB/s               \r",
      "         xiaohuangj  17%[==>                 ]   1.74M  8.70MB/s               \r",
      "        xiaohuangji  59%[==========>         ]   5.84M  14.5MB/s               \r",
      "xiaohuangji50w_nofe 100%[===================>]   9.76M  19.1MB/s    in 0.5s    \n",
      "\n",
      "2019-01-27 07:52:18 (19.1 MB/s) - ‘xiaohuangji50w_nofenci.conv.zip’ saved [10232473/10232473]\n",
      "\n",
      "Archive:  xiaohuangji50w_nofenci.conv.zip\n",
      "  inflating: xiaohuangji50w_nofenci.conv  \n"
     ]
    }
   ],
   "source": [
    "!wget https://github.com/candlewill/Dialog_Corpus/raw/master/xiaohuangji50w_nofenci.conv.zip\n",
    "!unzip xiaohuangji50w_nofenci.conv.zip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 527
    },
    "colab_type": "code",
    "id": "pZBDdmhVFQrd",
    "outputId": "6ff94090-282a-4cee-9ded-56607ef9d76d"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "E\n",
      "Q 呵呵\n",
      "A 是王若猫的。\n",
      "E\n",
      "Q 不是\n",
      "A 那是什么？\n",
      "E\n",
      "Q 怎么了\n",
      "A 我很难过，安慰我~\n",
      "E\n",
      "Q 开心点哈,一切都会好起来\n",
      "A 嗯 会的\n",
      "E\n",
      "Q 我还喜欢她,怎么办\n",
      "A 我帮你告诉她？发短信还是打电话？\n",
      "E\n",
      "Q 短信\n",
      "A 嗯嗯。我也相信\n",
      "E\n",
      "Q 你知道谁么\n",
      "A 肯定不是我，是阮德培\n",
      "E\n",
      "Q 许兵是谁\n",
      "A 吴院四班小帅哥\n",
      "E\n",
      "Q 这么假\n",
      "A 三鹿奶粉也假，不一样的卖啊\n",
      "E\n",
      "Q 许兵是傻逼\n",
      "A 被你发现了。\n"
     ]
    }
   ],
   "source": [
    "!perl -pi.bak -e 's/(E\\s)/\\1Q /g' xiaohuangji50w_nofenci.conv\n",
    "!perl -pi.bak -e 's/(Q M)/Q/g' xiaohuangji50w_nofenci.conv\n",
    "!perl -pi.bak -e 's/(M )/A /g' xiaohuangji50w_nofenci.conv\n",
    "!head -30 xiaohuangji50w_nofenci.conv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "NDF6ipgHN_n-"
   },
   "outputs": [],
   "source": [
    "text = open(\"xiaohuangji50w_nofenci.conv\").read().split(\"E\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "5PMg5KV8OGOj",
    "outputId": "576f2e61-c9a7-4fae-a9b2-57c85f5c7717"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Q 呵呵\\nA 是王若猫的。\\n'"
      ]
     },
     "execution_count": 32,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "BFgSeknoF0IO"
   },
   "outputs": [],
   "source": [
    "import jieba\n",
    "\n",
    "def split_conv(in_f, out_q, out_a):\n",
    "  out_question = open(out_q, 'w')\n",
    "  out_answer = open(out_a, 'w')\n",
    "  text = open(in_f).read().split(\"E\\n\")\n",
    "  for pair in text:\n",
    "    # 句子长度太短的问题对话，跳过\n",
    "    if len(pair)<=4:\n",
    "      continue\n",
    "    # 切分问题和回答\n",
    "    contents = pair.split(\"\\n\")\n",
    "    out_question.write(\" \".join(jieba.lcut(contents[0].strip(\"Q \")))+\"\\n\")\n",
    "    out_answer.write(\" \".join(jieba.lcut(contents[1].strip(\"A \")))+\"\\n\")\n",
    "  out_question.close()\n",
    "  out_answer.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "4WMjrPRkGvFZ"
   },
   "outputs": [],
   "source": [
    "in_f = \"xiaohuangji50w_nofenci.conv\"\n",
    "out_q = 'question.file'\n",
    "out_a = 'answer.file'\n",
    "split_conv(in_f, out_q, out_a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 187
    },
    "colab_type": "code",
    "id": "RDFtDEZhJkmq",
    "outputId": "90544356-fbe8-48ac-ade2-224ade638dba"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "呵呵\n",
      "不是\n",
      "怎么 了\n",
      "开心 点哈 , 一切 都 会 好 起来\n",
      "我 还 喜欢 她 , 怎么办\n",
      "短信\n",
      "你 知道 谁 么\n",
      "许兵 是 谁\n",
      "这么 假\n",
      "许兵 是 傻 逼\n"
     ]
    }
   ],
   "source": [
    "!head -10 question.file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 187
    },
    "colab_type": "code",
    "id": "Gc2KjA7KKFr4",
    "outputId": "0f319e2c-519f-467f-a961-9572f76376ee"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "是 王若 猫 的 。\n",
      "那 是 什么 ？\n",
      "我 很 难过 ， 安慰 我 ~\n",
      "嗯   会 的\n",
      "我 帮 你 告诉 她 ？ 发短信 还是 打电话 ？\n",
      "嗯 嗯 。 我 也 相信\n",
      "肯定 不是 我 ， 是 阮德培\n",
      "吴院 四班 小帅哥\n",
      "三鹿 奶粉 也 假 ， 不 一样 的 卖 啊\n",
      "被 你 发现 了 。\n"
     ]
    }
   ],
   "source": [
    "!head -10 answer.file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "eIejL95sHNdk",
    "outputId": "1efeed65-3fa7-47ed-b3b7-9675ac45b0d9"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "454131 question.file\n"
     ]
    }
   ],
   "source": [
    "!wc -l question.file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "Aa29ly54HQok",
    "outputId": "ace052ec-bbb5-4c32-f159-7651a38dcfe5"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "454131 answer.file\n"
     ]
    }
   ],
   "source": [
    "!wc -l answer.file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "f4XWRgjyDuB0"
   },
   "outputs": [],
   "source": [
    "import re\n",
    "def get_vocab(in_f, out_f):\n",
    "    vocab_dic = {}\n",
    "    for line in open(in_f, encoding='utf-8'):\n",
    "        words = line.strip().split(\" \")\n",
    "        for word in words:\n",
    "            # 保留汉字内容\n",
    "            if not re.match(r\"[\\u4e00-\\u9fa5]+\", word):\n",
    "                continue\n",
    "            try:\n",
    "                vocab_dic[word] += 1\n",
    "            except:\n",
    "                vocab_dic[word] = 1\n",
    "    out = open(out_f, 'w', encoding='utf-8')\n",
    "    out.write(\"<unk>\\n<s>\\n</s>\\n\")\n",
    "    vocab = sorted(vocab_dic.items(),key = lambda x:x[1],reverse = True)\n",
    "    for word in [x[0] for x in vocab[:80000]]:\n",
    "        out.write(word)\n",
    "        out.write(\"\\n\")\n",
    "    out.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "3oxjX4evHje0"
   },
   "source": [
    "#### 切分训练，验证，测试集"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "I-8TqvFkHqbB"
   },
   "outputs": [],
   "source": [
    "!mkdir data\n",
    "!head -300000 question.file > data/train.input\n",
    "!head -300000 answer.file > data/train.output\n",
    "!head -380000 question.file | tail -80000 > data/val.input\n",
    "!head -380000 answer.file | tail -80000 > data/val.output\n",
    "!tail -75000 question.file > data/test.input\n",
    "!tail -75000 answer.file > data/test.output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "QQ2ooLx9DuB3"
   },
   "source": [
    "**构建词表**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "gNy2gLOADuB4"
   },
   "outputs": [],
   "source": [
    "in_file = \"question.file\"\n",
    "out_file = \"./data/vocab.input\"\n",
    "get_vocab(in_file, out_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "H1LlbosEDuB5"
   },
   "outputs": [],
   "source": [
    "in_file = \"answer.file\"\n",
    "out_file = \"./data/vocab.output\"\n",
    "get_vocab(in_file, out_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "ALzBFu4LDuB8"
   },
   "source": [
    "**新建文件夹**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "ORzsXuC3DuB9"
   },
   "outputs": [],
   "source": [
    "!mkdir /tmp/nmt_attention_model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "rgdXPZpfDuB-"
   },
   "source": [
    "**训练摘要生成模型**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 4232
    },
    "colab_type": "code",
    "id": "uSQnnDoeDuB_",
    "outputId": "d30685a3-e3ec-4ca2-c21f-a766f0262e4c"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# Job id 0\n",
      "# Devices visible to TensorFlow: [_DeviceAttributes(/job:localhost/replica:0/task:0/device:CPU:0, CPU, 268435456, 9021483762835584285), _DeviceAttributes(/job:localhost/replica:0/task:0/device:XLA_CPU:0, XLA_CPU, 17179869184, 656894764299535912)]\n",
      "# Loading hparams from /tmp/nmt_attention_model/hparams\n",
      "# Vocab file ./data/vocab.input exists\n",
      "# Vocab file ./data/vocab.output exists\n",
      "  saving hparams to /tmp/nmt_attention_model/hparams\n",
      "  saving hparams to /tmp/nmt_attention_model/best_bleu/hparams\n",
      "  attention=scaled_luong\n",
      "  attention_architecture=standard\n",
      "  avg_ckpts=False\n",
      "  batch_size=128\n",
      "  beam_width=0\n",
      "  best_bleu=0\n",
      "  best_bleu_dir=/tmp/nmt_attention_model/best_bleu\n",
      "  check_special_token=True\n",
      "  colocate_gradients_with_ops=True\n",
      "  decay_scheme=\n",
      "  dev_prefix=./data/val\n",
      "  dropout=0.2\n",
      "  embed_prefix=None\n",
      "  encoder_type=uni\n",
      "  eos=</s>\n",
      "  epoch_step=0\n",
      "  forget_bias=1.0\n",
      "  infer_batch_size=32\n",
      "  infer_mode=greedy\n",
      "  init_op=uniform\n",
      "  init_weight=0.1\n",
      "  language_model=False\n",
      "  learning_rate=1.0\n",
      "  length_penalty_weight=0.0\n",
      "  log_device_placement=False\n",
      "  max_gradient_norm=5.0\n",
      "  max_train=0\n",
      "  metrics=['bleu']\n",
      "  num_buckets=5\n",
      "  num_dec_emb_partitions=0\n",
      "  num_decoder_layers=2\n",
      "  num_decoder_residual_layers=0\n",
      "  num_embeddings_partitions=0\n",
      "  num_enc_emb_partitions=0\n",
      "  num_encoder_layers=2\n",
      "  num_encoder_residual_layers=0\n",
      "  num_gpus=1\n",
      "  num_inter_threads=0\n",
      "  num_intra_threads=0\n",
      "  num_keep_ckpts=5\n",
      "  num_sampled_softmax=0\n",
      "  num_train_steps=12000\n",
      "  num_translations_per_input=1\n",
      "  num_units=128\n",
      "  optimizer=sgd\n",
      "  out_dir=/tmp/nmt_attention_model\n",
      "  output_attention=True\n",
      "  override_loaded_hparams=False\n",
      "  pass_hidden_state=True\n",
      "  random_seed=None\n",
      "  residual=False\n",
      "  sampling_temperature=0.0\n",
      "  share_vocab=False\n",
      "  sos=<s>\n",
      "  src=input\n",
      "  src_embed_file=\n",
      "  src_max_len=50\n",
      "  src_max_len_infer=None\n",
      "  src_vocab_file=./data/vocab.input\n",
      "  src_vocab_size=56491\n",
      "  steps_per_external_eval=None\n",
      "  steps_per_stats=100\n",
      "  subword_option=\n",
      "  test_prefix=./data/test\n",
      "  tgt=output\n",
      "  tgt_embed_file=\n",
      "  tgt_max_len=50\n",
      "  tgt_max_len_infer=None\n",
      "  tgt_vocab_file=./data/vocab.output\n",
      "  tgt_vocab_size=50041\n",
      "  time_major=True\n",
      "  train_prefix=./data/train\n",
      "  unit_type=lstm\n",
      "  use_char_encode=False\n",
      "  vocab_prefix=./data/vocab\n",
      "  warmup_scheme=t2t\n",
      "  warmup_steps=0\n",
      "WARNING:tensorflow:From /content/nmt/nmt/utils/iterator_utils.py:235: group_by_window (from tensorflow.contrib.data.python.ops.grouping) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use `tf.data.experimental.group_by_window(...)`.\n",
      "# Creating train graph ...\n",
      "# Build a basic encoder\n",
      "  num_layers = 2, num_residual_layers=0\n",
      "  cell 0  LSTM, forget_bias=1WARNING:tensorflow:From /content/nmt/nmt/model_helper.py:402: BasicLSTMCell.__init__ (from tensorflow.python.ops.rnn_cell_impl) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "This class is deprecated, please use tf.nn.rnn_cell.LSTMCell, which supports all the feature this cell currently has. Please replace the existing code with tf.nn.rnn_cell.LSTMCell(name='basic_lstm_cell').\n",
      "  DropoutWrapper, dropout=0.2   DeviceWrapper, device=/gpu:0\n",
      "  cell 1  LSTM, forget_bias=1  DropoutWrapper, dropout=0.2   DeviceWrapper, device=/gpu:0\n",
      "  cell 0  LSTM, forget_bias=1  DropoutWrapper, dropout=0.2   DeviceWrapper, device=/gpu:0\n",
      "  cell 1  LSTM, forget_bias=1  DropoutWrapper, dropout=0.2   DeviceWrapper, device=/gpu:0\n",
      "  learning_rate=1, warmup_steps=0, warmup_scheme=t2t\n",
      "  decay_scheme=, start_decay_step=12000, decay_steps 0, decay_factor 1\n",
      "# Trainable variables\n",
      "Format: <name>, <shape>, <(soft) device placement>\n",
      "  embeddings/encoder/embedding_encoder:0, (56491, 128), /device:CPU:0\n",
      "  embeddings/decoder/embedding_decoder:0, (50041, 128), /device:CPU:0\n",
      "  dynamic_seq2seq/encoder/rnn/multi_rnn_cell/cell_0/basic_lstm_cell/kernel:0, (256, 512), /device:GPU:0\n",
      "  dynamic_seq2seq/encoder/rnn/multi_rnn_cell/cell_0/basic_lstm_cell/bias:0, (512,), /device:GPU:0\n",
      "  dynamic_seq2seq/encoder/rnn/multi_rnn_cell/cell_1/basic_lstm_cell/kernel:0, (256, 512), /device:GPU:0\n",
      "  dynamic_seq2seq/encoder/rnn/multi_rnn_cell/cell_1/basic_lstm_cell/bias:0, (512,), /device:GPU:0\n",
      "  dynamic_seq2seq/decoder/memory_layer/kernel:0, (128, 128), \n",
      "  dynamic_seq2seq/decoder/attention/multi_rnn_cell/cell_0/basic_lstm_cell/kernel:0, (384, 512), /device:GPU:0\n",
      "  dynamic_seq2seq/decoder/attention/multi_rnn_cell/cell_0/basic_lstm_cell/bias:0, (512,), /device:GPU:0\n",
      "  dynamic_seq2seq/decoder/attention/multi_rnn_cell/cell_1/basic_lstm_cell/kernel:0, (256, 512), /device:GPU:0\n",
      "  dynamic_seq2seq/decoder/attention/multi_rnn_cell/cell_1/basic_lstm_cell/bias:0, (512,), /device:GPU:0\n",
      "  dynamic_seq2seq/decoder/attention/luong_attention/attention_g:0, (), /device:GPU:0\n",
      "  dynamic_seq2seq/decoder/attention/attention_layer/kernel:0, (256, 128), /device:GPU:0\n",
      "  dynamic_seq2seq/decoder/output_projection/kernel:0, (128, 50041), /device:GPU:0\n",
      "# Creating eval graph ...\n",
      "# Build a basic encoder\n",
      "  num_layers = 2, num_residual_layers=0\n",
      "  cell 0  LSTM, forget_bias=1  DeviceWrapper, device=/gpu:0\n",
      "  cell 1  LSTM, forget_bias=1  DeviceWrapper, device=/gpu:0\n",
      "  cell 0  LSTM, forget_bias=1  DeviceWrapper, device=/gpu:0\n",
      "  cell 1  LSTM, forget_bias=1  DeviceWrapper, device=/gpu:0\n",
      "# Trainable variables\n",
      "Format: <name>, <shape>, <(soft) device placement>\n",
      "  embeddings/encoder/embedding_encoder:0, (56491, 128), /device:CPU:0\n",
      "  embeddings/decoder/embedding_decoder:0, (50041, 128), /device:CPU:0\n",
      "  dynamic_seq2seq/encoder/rnn/multi_rnn_cell/cell_0/basic_lstm_cell/kernel:0, (256, 512), /device:GPU:0\n",
      "  dynamic_seq2seq/encoder/rnn/multi_rnn_cell/cell_0/basic_lstm_cell/bias:0, (512,), /device:GPU:0\n",
      "  dynamic_seq2seq/encoder/rnn/multi_rnn_cell/cell_1/basic_lstm_cell/kernel:0, (256, 512), /device:GPU:0\n",
      "  dynamic_seq2seq/encoder/rnn/multi_rnn_cell/cell_1/basic_lstm_cell/bias:0, (512,), /device:GPU:0\n",
      "  dynamic_seq2seq/decoder/memory_layer/kernel:0, (128, 128), \n",
      "  dynamic_seq2seq/decoder/attention/multi_rnn_cell/cell_0/basic_lstm_cell/kernel:0, (384, 512), /device:GPU:0\n",
      "  dynamic_seq2seq/decoder/attention/multi_rnn_cell/cell_0/basic_lstm_cell/bias:0, (512,), /device:GPU:0\n",
      "  dynamic_seq2seq/decoder/attention/multi_rnn_cell/cell_1/basic_lstm_cell/kernel:0, (256, 512), /device:GPU:0\n",
      "  dynamic_seq2seq/decoder/attention/multi_rnn_cell/cell_1/basic_lstm_cell/bias:0, (512,), /device:GPU:0\n",
      "  dynamic_seq2seq/decoder/attention/luong_attention/attention_g:0, (), /device:GPU:0\n",
      "  dynamic_seq2seq/decoder/attention/attention_layer/kernel:0, (256, 128), /device:GPU:0\n",
      "  dynamic_seq2seq/decoder/output_projection/kernel:0, (128, 50041), /device:GPU:0\n",
      "# Creating infer graph ...\n",
      "# Build a basic encoder\n",
      "  num_layers = 2, num_residual_layers=0\n",
      "  cell 0  LSTM, forget_bias=1  DeviceWrapper, device=/gpu:0\n",
      "  cell 1  LSTM, forget_bias=1  DeviceWrapper, device=/gpu:0\n",
      "  cell 0  LSTM, forget_bias=1  DeviceWrapper, device=/gpu:0\n",
      "  cell 1  LSTM, forget_bias=1  DeviceWrapper, device=/gpu:0\n",
      "  decoder: infer_mode=greedybeam_width=0, length_penalty=0.000000\n",
      "# Trainable variables\n",
      "Format: <name>, <shape>, <(soft) device placement>\n",
      "  embeddings/encoder/embedding_encoder:0, (56491, 128), /device:CPU:0\n",
      "  embeddings/decoder/embedding_decoder:0, (50041, 128), /device:CPU:0\n",
      "  dynamic_seq2seq/encoder/rnn/multi_rnn_cell/cell_0/basic_lstm_cell/kernel:0, (256, 512), /device:GPU:0\n",
      "  dynamic_seq2seq/encoder/rnn/multi_rnn_cell/cell_0/basic_lstm_cell/bias:0, (512,), /device:GPU:0\n",
      "  dynamic_seq2seq/encoder/rnn/multi_rnn_cell/cell_1/basic_lstm_cell/kernel:0, (256, 512), /device:GPU:0\n",
      "  dynamic_seq2seq/encoder/rnn/multi_rnn_cell/cell_1/basic_lstm_cell/bias:0, (512,), /device:GPU:0\n",
      "  dynamic_seq2seq/decoder/memory_layer/kernel:0, (128, 128), \n",
      "  dynamic_seq2seq/decoder/attention/multi_rnn_cell/cell_0/basic_lstm_cell/kernel:0, (384, 512), /device:GPU:0\n",
      "  dynamic_seq2seq/decoder/attention/multi_rnn_cell/cell_0/basic_lstm_cell/bias:0, (512,), /device:GPU:0\n",
      "  dynamic_seq2seq/decoder/attention/multi_rnn_cell/cell_1/basic_lstm_cell/kernel:0, (256, 512), /device:GPU:0\n",
      "  dynamic_seq2seq/decoder/attention/multi_rnn_cell/cell_1/basic_lstm_cell/bias:0, (512,), /device:GPU:0\n",
      "  dynamic_seq2seq/decoder/attention/luong_attention/attention_g:0, (), /device:GPU:0\n",
      "  dynamic_seq2seq/decoder/attention/attention_layer/kernel:0, (256, 128), /device:GPU:0\n",
      "  dynamic_seq2seq/decoder/output_projection/kernel:0, (128, 50041), \n",
      "# log_file=/tmp/nmt_attention_model/log_1548576134\n",
      "  created train model with fresh parameters, time 0.34s\n",
      "  created infer model with fresh parameters, time 0.26s\n",
      "  # 8630\n",
      "    src: 老子 不 搞\n",
      "    ref: 你 有 啥 不 高兴 的 事 ， 说 出来 让 大家 开心 一下\n",
      "    nmt: 熄灯 幹 卡则 幹 陈虹 陈虹\n",
      "  created eval model with fresh parameters, time 0.29s\n",
      "2019-01-27 08:03:58.745213: W tensorflow/core/framework/allocator.cc:122] Allocation of 4919230464 exceeds 10% of system memory.\n",
      "tcmalloc: large alloc 4919230464 bytes == 0x2bcb4000 @  0x7fbf7e9e0b6b 0x7fbf7ea00379 0x7fbf6c706487 0x7fbf6c3fe51f 0x7fbf6c4a799a 0x7fbf6c478761 0x7fbf6c478c4f 0x7fbf6c478cf8 0x7fbf6f9423d2 0x7fbf6c667aca 0x7fbf6c65b6b5 0x7fbf6c6d7822 0x7fbf6c6d55f7 0x7fbf7d2e057f 0x7fbf7e3c26db 0x7fbf7e6fb88f\n",
      "2019-01-27 08:05:12.886265: W tensorflow/core/framework/allocator.cc:122] Allocation of 999218688 exceeds 10% of system memory.\n",
      "2019-01-27 08:06:03.966427: W tensorflow/core/framework/allocator.cc:122] Allocation of 3996874752 exceeds 10% of system memory.\n",
      "tcmalloc: large alloc 3996876800 bytes == 0x2bcb4000 @  0x7fbf7e9e0b6b 0x7fbf7ea00379 0x7fbf6c706487 0x7fbf6c3fe51f 0x7fbf6c4a799a 0x7fbf6c478761 0x7fbf6c478c4f 0x7fbf6c478cf8 0x7fbf6f9423d2 0x7fbf6c667aca 0x7fbf6c65b6b5 0x7fbf6c6d7822 0x7fbf6c6d55f7 0x7fbf7d2e057f 0x7fbf7e3c26db 0x7fbf7e6fb88f\n",
      "2019-01-27 08:07:30.739635: W tensorflow/core/framework/allocator.cc:122] Allocation of 1255428608 exceeds 10% of system memory.\n",
      "2019-01-27 08:07:43.000768: W tensorflow/core/framework/allocator.cc:122] Allocation of 3791906816 exceeds 10% of system memory.\n",
      "tcmalloc: large alloc 3791912960 bytes == 0x2bcb4000 @  0x7fbf7e9e0b6b 0x7fbf7ea00379 0x7fbf6c706487 0x7fbf6c3fe51f 0x7fbf6c4a799a 0x7fbf6c478761 0x7fbf6c478c4f 0x7fbf6c478cf8 0x7fbf6f9423d2 0x7fbf6c667aca 0x7fbf6c65b6b5 0x7fbf6c6d7822 0x7fbf6c6d55f7 0x7fbf7d2e057f 0x7fbf7e3c26db 0x7fbf7e6fb88f\n",
      "tcmalloc: large alloc 3971260416 bytes == 0x2bcb4000 @  0x7fbf7e9e0b6b 0x7fbf7ea00379 0x7fbf6c706487 0x7fbf6c3fe51f 0x7fbf6c4a799a 0x7fbf6c478761 0x7fbf6c478c4f 0x7fbf6c478cf8 0x7fbf6f9423d2 0x7fbf6c667aca 0x7fbf6c65b6b5 0x7fbf6c6d7822 0x7fbf6c6d55f7 0x7fbf7d2e057f 0x7fbf7e3c26db 0x7fbf7e6fb88f\n",
      "tcmalloc: large alloc 3791912960 bytes == 0x2bcb4000 @  0x7fbf7e9e0b6b 0x7fbf7ea00379 0x7fbf6c706487 0x7fbf6c3fe51f 0x7fbf6c4a799a 0x7fbf6c478761 0x7fbf6c478c4f 0x7fbf6c478cf8 0x7fbf6f9423d2 0x7fbf6c667aca 0x7fbf6c65b6b5 0x7fbf6c6d7822 0x7fbf6c6d55f7 0x7fbf7d2e057f 0x7fbf7e3c26db 0x7fbf7e6fb88f\n",
      "tcmalloc: large alloc 3791912960 bytes == 0x2bcb4000 @  0x7fbf7e9e0b6b 0x7fbf7ea00379 0x7fbf6c706487 0x7fbf6c3fe51f 0x7fbf6c4a799a 0x7fbf6c478761 0x7fbf6c478c4f 0x7fbf6c478cf8 0x7fbf6f9423d2 0x7fbf6c667aca 0x7fbf6c65b6b5 0x7fbf6c6d7822 0x7fbf6c6d55f7 0x7fbf7d2e057f 0x7fbf7e3c26db 0x7fbf7e6fb88f\n",
      "tcmalloc: large alloc 4201848832 bytes == 0x2bcb4000 @  0x7fbf7e9e0b6b 0x7fbf7ea00379 0x7fbf6c706487 0x7fbf6c3fe51f 0x7fbf6c4a799a 0x7fbf6c478761 0x7fbf6c478c4f 0x7fbf6c478cf8 0x7fbf6f9423d2 0x7fbf6c667aca 0x7fbf6c65b6b5 0x7fbf6c6d7822 0x7fbf6c6d55f7 0x7fbf7d2e057f 0x7fbf7e3c26db 0x7fbf7e6fb88f\n",
      "tcmalloc: large alloc 3996876800 bytes == 0x2bcb4000 @  0x7fbf7e9e0b6b 0x7fbf7ea00379 0x7fbf6c706487 0x7fbf6c3fe51f 0x7fbf6c4a799a 0x7fbf6c478761 0x7fbf6c478c4f 0x7fbf6c478cf8 0x7fbf6f9423d2 0x7fbf6c667aca 0x7fbf6c65b6b5 0x7fbf6c6d7822 0x7fbf6c6d55f7 0x7fbf7d2e057f 0x7fbf7e3c26db 0x7fbf7e6fb88f\n",
      "  eval dev: perplexity 50033.89, time 906s, Sun Jan 27 08:17:23 2019.\n",
      "tcmalloc: large alloc 4432437248 bytes == 0x2bcb4000 @  0x7fbf7e9e0b6b 0x7fbf7ea00379 0x7fbf6c706487 0x7fbf6c3fe51f 0x7fbf6c4a799a 0x7fbf6c478761 0x7fbf6c478c4f 0x7fbf6c478cf8 0x7fbf6f9423d2 0x7fbf6c667aca 0x7fbf6c65b6b5 0x7fbf6c6d7822 0x7fbf6c6d55f7 0x7fbf7d2e057f 0x7fbf7e3c26db 0x7fbf7e6fb88f\n",
      "tcmalloc: large alloc 4432437248 bytes == 0x2bcb4000 @  0x7fbf7e9e0b6b 0x7fbf7ea00379 0x7fbf6c706487 0x7fbf6c3fe51f 0x7fbf6c4a799a 0x7fbf6c478761 0x7fbf6c478c4f 0x7fbf6c478cf8 0x7fbf6f9423d2 0x7fbf6c667aca 0x7fbf6c65b6b5 0x7fbf6c6d7822 0x7fbf6c6d55f7 0x7fbf7d2e057f 0x7fbf7e3c26db 0x7fbf7e6fb88f\n",
      "tcmalloc: large alloc 4432437248 bytes == 0x2bcb4000 @  0x7fbf7e9e0b6b 0x7fbf7ea00379 0x7fbf6c706487 0x7fbf6c3fe51f 0x7fbf6c4a799a 0x7fbf6c478761 0x7fbf6c478c4f 0x7fbf6c478cf8 0x7fbf6f9423d2 0x7fbf6c667aca 0x7fbf6c65b6b5 0x7fbf6c6d7822 0x7fbf6c6d55f7 0x7fbf7d2e057f 0x7fbf7e3c26db 0x7fbf7e6fb88f\n",
      "tcmalloc: large alloc 4432437248 bytes == 0x2bcb4000 @  0x7fbf7e9e0b6b 0x7fbf7ea00379 0x7fbf6c706487 0x7fbf6c3fe51f 0x7fbf6c4a799a 0x7fbf6c478761 0x7fbf6c478c4f 0x7fbf6c478cf8 0x7fbf6f9423d2 0x7fbf6c667aca 0x7fbf6c65b6b5 0x7fbf6c6d7822 0x7fbf6c6d55f7 0x7fbf7d2e057f 0x7fbf7e3c26db 0x7fbf7e6fb88f\n",
      "  eval test: perplexity 50037.16, time 815s, Sun Jan 27 08:30:59 2019.\n",
      "2019-01-27 08:30:59.371143: I tensorflow/core/kernels/lookup_util.cc:376] Table trying to initialize from file ./data/vocab.output is already initialized.\n",
      "2019-01-27 08:30:59.371143: I tensorflow/core/kernels/lookup_util.cc:376] Table trying to initialize from file ./data/vocab.output is already initialized.\n",
      "2019-01-27 08:30:59.371386: I tensorflow/core/kernels/lookup_util.cc:376] Table trying to initialize from file ./data/vocab.input is already initialized.\n",
      "  created infer model with fresh parameters, time 0.22s\n",
      "# Start step 0, lr 1, Sun Jan 27 08:30:59 2019\n",
      "# Init train iterator, skipping 0 elements\n",
      "  step 100 lr 1 step-time 2.60s wps 0.56K ppl 17827.47 gN 21.16 bleu 0.00, Sun Jan 27 08:35:19 2019\n",
      "  step 200 lr 1 step-time 2.21s wps 0.65K ppl 976.06 gN 5.97 bleu 0.00, Sun Jan 27 08:39:00 2019\n",
      "  step 300 lr 1 step-time 2.17s wps 0.67K ppl 552.16 gN 3.62 bleu 0.00, Sun Jan 27 08:42:38 2019\n",
      "  step 400 lr 1 step-time 2.38s wps 0.66K ppl 590.66 gN 4.01 bleu 0.00, Sun Jan 27 08:46:36 2019\n",
      "  step 500 lr 1 step-time 2.30s wps 0.66K ppl 433.11 gN 3.06 bleu 0.00, Sun Jan 27 08:50:26 2019\n",
      "  step 600 lr 1 step-time 2.30s wps 0.66K ppl 347.75 gN 2.77 bleu 0.00, Sun Jan 27 08:54:15 2019\n",
      "  step 700 lr 1 step-time 2.32s wps 0.66K ppl 314.29 gN 2.56 bleu 0.00, Sun Jan 27 08:58:07 2019\n",
      "  step 800 lr 1 step-time 2.21s wps 0.66K ppl 246.35 gN 2.13 bleu 0.00, Sun Jan 27 09:01:48 2019\n",
      "  step 900 lr 1 step-time 2.21s wps 0.65K ppl 224.77 gN 2.48 bleu 0.00, Sun Jan 27 09:05:29 2019\n",
      "  step 1000 lr 1 step-time 2.33s wps 0.66K ppl 223.08 gN 2.10 bleu 0.00, Sun Jan 27 09:09:22 2019\n",
      "# Save eval, global step 1000\n",
      "2019-01-27 09:09:23.307011: I tensorflow/core/kernels/lookup_util.cc:376] Table trying to initialize from file ./data/vocab.output is already initialized.\n",
      "2019-01-27 09:09:23.307011: I tensorflow/core/kernels/lookup_util.cc:376] Table trying to initialize from file ./data/vocab.output is already initialized.\n",
      "2019-01-27 09:09:23.307277: I tensorflow/core/kernels/lookup_util.cc:376] Table trying to initialize from file ./data/vocab.input is already initialized.\n",
      "  loaded infer model parameters from /tmp/nmt_attention_model/translate.ckpt-1000, time 0.11s\n",
      "  # 41510\n",
      "    src: 说个 傅里叶 变换\n",
      "    ref: =   =\n",
      "    nmt: <unk> <unk> <unk> <unk> <unk> <unk>\n",
      "2019-01-27 09:09:23.459672: I tensorflow/core/kernels/lookup_util.cc:376] Table trying to initialize from file ./data/vocab.output is already initialized.\n",
      "2019-01-27 09:09:23.459672: I tensorflow/core/kernels/lookup_util.cc:376] Table trying to initialize from file ./data/vocab.output is already initialized.\n",
      "2019-01-27 09:09:23.460115: I tensorflow/core/kernels/lookup_util.cc:376] Table trying to initialize from file ./data/vocab.input is already initialized.\n",
      "  loaded eval model parameters from /tmp/nmt_attention_model/translate.ckpt-1000, time 0.12s\n",
      "tcmalloc: large alloc 4919230464 bytes == 0x5241a000 @  0x7fbf7e9e0b6b 0x7fbf7ea00379 0x7fbf6c706487 0x7fbf6c3fe51f 0x7fbf6c4a799a 0x7fbf6c478761 0x7fbf6c478c4f 0x7fbf6c478cf8 0x7fbf6f9423d2 0x7fbf6c667aca 0x7fbf6c65b6b5 0x7fbf6c6d7822 0x7fbf6c6d55f7 0x7fbf7d2e057f 0x7fbf7e3c26db 0x7fbf7e6fb88f\n",
      "  eval dev: perplexity 328.47, time 897s, Sun Jan 27 09:24:21 2019.\n",
      "  eval test: perplexity 436.29, time 812s, Sun Jan 27 09:37:54 2019.\n",
      "  step 1100 lr 1 step-time 2.27s wps 0.65K ppl 208.46 gN 2.07 bleu 0.00, Sun Jan 27 09:41:41 2019\n",
      "  step 1200 lr 1 step-time 2.26s wps 0.65K ppl 187.18 gN 2.13 bleu 0.00, Sun Jan 27 09:45:27 2019\n",
      "  step 1300 lr 1 step-time 2.36s wps 0.66K ppl 208.60 gN 2.36 bleu 0.00, Sun Jan 27 09:49:23 2019\n",
      "  step 1400 lr 1 step-time 2.31s wps 0.65K ppl 179.46 gN 2.10 bleu 0.00, Sun Jan 27 09:53:14 2019\n",
      "  step 1500 lr 1 step-time 2.24s wps 0.65K ppl 162.42 gN 2.01 bleu 0.00, Sun Jan 27 09:56:58 2019\n",
      "  step 1600 lr 1 step-time 2.45s wps 0.66K ppl 203.35 gN 2.22 bleu 0.00, Sun Jan 27 10:01:03 2019\n",
      "  step 1700 lr 1 step-time 2.32s wps 0.65K ppl 157.10 gN 1.97 bleu 0.00, Sun Jan 27 10:04:55 2019\n",
      "  step 1800 lr 1 step-time 2.31s wps 0.66K ppl 165.35 gN 2.15 bleu 0.00, Sun Jan 27 10:08:46 2019\n",
      "  step 1900 lr 1 step-time 2.31s wps 0.66K ppl 151.73 gN 2.09 bleu 0.00, Sun Jan 27 10:12:37 2019\n",
      "  step 2000 lr 1 step-time 2.26s wps 0.66K ppl 133.99 gN 1.79 bleu 0.00, Sun Jan 27 10:16:23 2019\n",
      "# Save eval, global step 2000\n",
      "2019-01-27 10:16:23.894014: I tensorflow/core/kernels/lookup_util.cc:376] Table trying to initialize from file ./data/vocab.output is already initialized.\n",
      "2019-01-27 10:16:23.894014: I tensorflow/core/kernels/lookup_util.cc:376] Table trying to initialize from file ./data/vocab.output is already initialized.\n",
      "2019-01-27 10:16:23.894265: I tensorflow/core/kernels/lookup_util.cc:376] Table trying to initialize from file ./data/vocab.input is already initialized.\n",
      "  loaded infer model parameters from /tmp/nmt_attention_model/translate.ckpt-2000, time 0.10s\n",
      "  # 51767\n",
      "    src: 讲个 黄色笑话 呗\n",
      "    ref: 二\n",
      "    nmt: <unk> <unk> <unk>\n",
      "2019-01-27 10:16:24.020441: I tensorflow/core/kernels/lookup_util.cc:376] Table trying to initialize from file ./data/vocab.output is already initialized.\n",
      "2019-01-27 10:16:24.020441: I tensorflow/core/kernels/lookup_util.cc:376] Table trying to initialize from file ./data/vocab.output is already initialized.\n",
      "2019-01-27 10:16:24.020711: I tensorflow/core/kernels/lookup_util.cc:376] Table trying to initialize from file ./data/vocab.input is already initialized.\n",
      "  loaded eval model parameters from /tmp/nmt_attention_model/translate.ckpt-2000, time 0.11s\n",
      "  eval dev: perplexity 133.70, time 901s, Sun Jan 27 10:31:25 2019.\n"
     ]
    }
   ],
   "source": [
    "!python3 -m nmt.nmt \\\n",
    "    --attention=scaled_luong \\\n",
    "    --src=input --tgt=output \\\n",
    "    --vocab_prefix=./data/vocab  \\\n",
    "    --train_prefix=./data/train \\\n",
    "    --dev_prefix=./data/val  \\\n",
    "    --test_prefix=./data/test \\\n",
    "    --out_dir=/tmp/nmt_attention_model \\\n",
    "    --num_train_steps=12000 \\\n",
    "    --steps_per_stats=1 \\\n",
    "    --num_layers=2 \\\n",
    "    --num_units=128 \\\n",
    "    --dropout=0.2 \\\n",
    "    --metrics=bleu"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "4DTr0yNnDuCC"
   },
   "source": [
    "![](./img/xiniu_neteasy.png)"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "3.seq2seq_application_step_by_step.ipynb",
   "provenance": [],
   "version": "0.3.2"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
