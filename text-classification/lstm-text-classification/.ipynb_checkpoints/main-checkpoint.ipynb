{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import optim, nn\n",
    "from torch.autograd import Variable\n",
    "import argparse\n",
    "\n",
    "parser = argparse.ArgumentParser(description=\"LSTM text classification\")\n",
    "parser.add_argument(\"--lr\", type=float, default=1e-3, help=\"initial learning rate [default: 1e-3]\")\n",
    "parser.add_argument(\"--num-epochs\", type=int, default=20, help=\"number of epochs for train\")\n",
    "parser.add_argument(\"--batch-size\", type=int, default=16, help=\"batch size for training [default: 16]\")\n",
    "parser.add_argument(\"--seed\", type=int, default=1, help=\"random seed\")\n",
    "parser.add_argument(\"--cuda-able\", action=\"store_true\", default=False, help=\"enables cuda\")\n",
    "parser.add_argument(\"--save-model\", type=str, default=\"./LSTM_Text.pt\", help=\"path to save the final model\")\n",
    "parser.add_argument(\"--data\", type=str, default=\"./data/corpus.pt\", help=\"location of the data corpus\")\n",
    "parser.add_argument(\"--dropout\", type=float, default=0.5, help=\"the probability for dropout (0 = no dropout) [default: 0.5]\")\n",
    "parser.add_argument(\"--embed-size\", type=int, default=64, help=\"number of embedding size [default: 64]\")\n",
    "parser.add_argument(\"--hidden-size\", type=int, default=128, help=\"number of lstm hidden size [default: 128]\")\n",
    "parser.add_argument(\"--num-layers\", type=int, default=3, help=\"biLSTM layer numbers\")\n",
    "parser.add_argument(\"--bidirectional\", action=\"store_true\", help=\"if True, becomes a bidirectional LSTM [default: False]\")\n",
    "\n",
    "args = parser.parse_args()\n",
    "torch.manual_seed(args.seed)\n",
    "use_cuda = torch.cuda.is_available() and args.cuda_able\n",
    "num_devices = torch.cuda.device_count()\n",
    "device = torch.device(\"cuda:\" + str(num_devices - 1) if use_cuda else \"cpu\")\n",
    "\n",
    "# Load data\n",
    "from data_loader import DataLoader\n",
    "data = torch.load(args.data)\n",
    "args.vocab_size = data[\"dict\"][\"vocab_size\"]\n",
    "args.label_size = data[\"dict\"][\"label_size\"]\n",
    "\n",
    "train_loader = DataLoader(data[\"train\"][\"sents\"], data[\"train\"][\"labels\"], batch_size=args.batch_size)\n",
    "valid_loader = DataLoader(data[\"valid\"][\"sents\"], data[\"valid\"][\"labels\"], batch_size=args.batch_size, shuffle=False)\n",
    "\n",
    "# Build model\n",
    "from model import LSTM_Text\n",
    "model = LSTM_Text(args).to(device)\n",
    "optimizer = optim.Adam(model.parameters(), lr=args.lr)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "from tqdm import tqdm\n",
    "# Train\n",
    "\n",
    "def train():\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    hidden = model.init_hidden()\n",
    "    for data, labels in tqdm(train_loader, mininterval=1):\n",
    "        data = data.to(device)\n",
    "        labels = labels.to(device)\n",
    "        hidden = (Variable(h.detach()) for h in hidden)\n",
    "        out, hidden = model(data, hidden)\n",
    "        loss = criterion(out, labels)\n",
    "        total_loss += loss\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "    return total_loss.item() / train_loader.sents_size\n",
    "    \n",
    "def valid():\n",
    "    model.eval()\n",
    "    total_correct = 0\n",
    "    hidden = model.init_hidden()\n",
    "    for data, labels in tqdm(valid_loader, mininterval=1):\n",
    "        data = data.to(device)\n",
    "        labels = labels.to(device)\n",
    "        hidden = (Variable(each.detach()) for each in hidden)\n",
    "        out, hidden = model(data, hidden)\n",
    "        total_correct += (torch.argmax(out, dim=1) == labels).sum()\n",
    "    return total_correct / valid_loader.sents_size\n",
    "\n",
    "best_acc = None\n",
    "for epoch in range(self.num_epochs):\n",
    "    loss = train()\n",
    "    acc = valid()\n",
    "    print('-' * 90)\n",
    "    print(\"epoch[{} / {}], loss: {:.9f}, acc: {:.4f}\".format(epoch +\n",
    "                                                             1, args.num_epochs, loss, acc))\n",
    "    print('-' * 90)\n",
    "    \n",
    "    if not best_acc or best_acc < acc:\n",
    "        best_acc = acc\n",
    "        model_params = model.state_dict()\n",
    "        \n",
    "print(\"best_acc: \" + str(best_acc))\n",
    "torch.save(model_params, args.save_model)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
